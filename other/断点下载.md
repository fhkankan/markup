# 断点下载

## 依赖

````
python环境
requests模块
对应文件的下载链接(注意：下载的文件必须支持断点续传)
````

## 实现原理

### tqdm

tqdm是一个快速、扩展性强的进度条工具库，用户只需要封装任意的迭代器 tqdm(iterator)，[tqdm官方文档](https://github.com/tqdm/tqdm#documentation)。

对于爬虫进度的监控，这是个不错的工具。

### requests

对于requests的网络请求返回结果中，当需要获取文本的时候我们会使用response.text获取文本信息,使用response.content获取字节流,比如下载图片保存到一个文件,而对于大个的文件我们就要采取分块读取的方式了。

第一步，我们需要设置requests.get的stream参数为True。 默认情况下是stream的值为false，**它会立即开始下载文件并存放到内存当中，倘若文件过大就会导致内存不足的情况**．当把get函数的**stream参数设置成True**时，**它不会立即开始下载**，当你使用iter_content或iter_lines遍历内容或访问内容属性时才开始下载。需要注意一点：文件没有下载之前，它也需要保持连接。

### 断点续传

所谓断点续传，也就是要从文件已经下载的地方开始继续下载。在以前版本的 HTTP 协议是不支持断点的，HTTP/1.1 开始就支持了。一般断点下载时会用到 header请求头的Range字段，这也是现在众多号称多线程下载工具（如 FlashGet、迅雷等）实现多线程下载的核心所在。

### HTTP请求头Range

range是请求资源的部分内容（不包括响应头的大小），单位是byte，即字节，从0开始. 如果服务器能够正常响应的话，服务器会返回 206 Partial Content 的状态码及说明. 如果不能处理这种Range的话，就会返回整个资源以及响应状态码为 200 OK .（这个要注意，要分段下载时，要先判断这个）。

**Range请求头格式**

```
Range: bytes=start-end
```

**Range头域**

```
Range头域可以请求实体的一个或者多个子范围。例如，  
表示头500个字节：bytes=0-499  
表示第二个500字节：bytes=500-999  
表示最后500个字节：bytes=-500  
表示500字节以后的范围：bytes=500-  
第一个和最后一个字节：bytes=0-0,-1  
同时指定几个范围：bytes=500-600,601-999 

Range: bytes=10- ：第10个字节及最后个字节的数据
Range: bytes=40-100 ：第40个字节到第100个字节之间的数据.
```

注意，这个表示[start,end]，即是包含请求头的start及end字节的，所以，下一个请求，应该是上一个请求的[end+1, nextEnd]

## 客户端

### requests

[参考](https://blog.csdn.net/qq_35203425/article/details/80987880)

```python
import sys
import requests
import os

# 屏蔽warning信息
requests.packages.urllib3.disable_warnings()

def download(url, file_path):
    # 第一次请求是为了得到文件总大小
    r1 = requests.get(url, stream=True, verify=False)
    total_size = int(r1.headers['Content-Length'])

    # 这重要了，先看看本地文件下载了多少
    if os.path.exists(file_path):
        temp_size = os.path.getsize(file_path)  # 本地已经下载的文件大小
    else:
        temp_size = 0
    # 显示一下下载了多少   
    print(temp_size)
    print(total_size)
    # 核心部分，这个是请求下载时，从本地文件已经下载过的后面下载
    headers = {'Range': 'bytes=%d-' % temp_size}  
    # 重新请求网址，加入新的请求头的
    r = requests.get(url, stream=True, verify=False, headers=headers)

    # 下面写入文件也要注意，看到"ab"了吗？
    # "ab"表示追加形式写入文件
    with open(file_path, "ab") as f:
        for chunk in r.iter_content(chunk_size=1024):
            if chunk:
                temp_size += len(chunk)
                f.write(chunk)
                f.flush()

                ###这是下载实现进度显示####
                done = int(50 * temp_size / total_size)
                sys.stdout.write("\r[%s%s] %d%%" % ('█' * done, ' ' * (50 - done), 100 * temp_size / total_size))
                sys.stdout.flush()
    print()  # 避免上面\r 回车符


if __name__ == '__main__':
    link = r'https://api.gdc.cancer.gov/data/'
    UUID = r'2a4a3044-0b1a-4722-83ed-43ba5d6d25b0'
    path = r'F:\SYY\temp\a.txt'
    url = os.path.join(link, UUID)
    # 调用一下函数试试
    download(url, path)

```

### tqdm

[参考](https://my.oschina.net/jiansin/blog/3016552)

```python
# -*- coding: utf-8 -*-
import requests
from tqdm import tqdm
import os
import time

# 用法一
def tqdm_demo():
    text = ""
    for char in tqdm(["a", "b", "c", "d"]):
        text = text + char
        time.sleep(0.5)
# 用法二

def tqdm_demo2():
    pbar = tqdm(["a", "b", "c", "d"])
    for char in pbar:
        time.sleep(0.5)
        pbar.set_description("Processing %s" % char)
# 手动控制运行
# tqdm.update()方法用于手动更新进度条，对读取文件之类的流操作非常有用。
def tqdm_demo3():
    with tqdm(total=100) as pbar:
        for i in range(10):
            pbar.update(10)
            time.sleep(0.5)


def download_from_url(url, dst):
    '''
    :param url:  下载地址
    :param dst:  文件名称
    :return:
    '''
    #发起网络请求
    response = requests.get(url, stream=True)
    #获取返回的文件的大小
    file_size = int(response.headers['content-length'])

    #判断当前目录中是否有该文件，如果有获取文件的大小，从而实现断点续传
    if os.path.exists(dst):
        first_byte = os.path.getsize(dst)
    else:
        first_byte = 0
    #如果文件大小已经超过了服务器返回的文件的大小，返回文件长度
    if first_byte >= file_size: #(4)
        return file_size
    #设置断点续传的位置
    header = {"Range": f"bytes=%s-%s"%(first_byte,file_size)}
    # desc :进度条的前缀
    # unit 定义每个迭代的单元。默认为"it"，即每个迭代，在下载或解压时，设为"B"，代表每个“块”。
    # unit_scale 默认为False，如果设置为1或者True，会自动根据国际单位制进行转换 (kilo, mega, etc.) 。比如，在下载进度条的例子中，如果为False，数据大小是按照字节显示，设为True之后转换为Kb、Mb。
    #total：总的迭代次数，不设置则只显示统计信息，没有图形化的进度条。设置为len(iterable)，会显示黑色方块的图形化进度条。
    pbar = tqdm(total=file_size, initial=first_byte,unit='B', unit_scale=True, desc=dst)
    #发送网络请求
    req = requests.get(url, headers=header, stream=True) #(5)
    #这里的二进制需要采用追加的方式写入文件，不然无法实现断点续传
    with(open(dst, 'ab')) as f:
        for chunk in req.iter_content(chunk_size=1024): #(6)
            if chunk:
                #用于方便观察进度条，在下载大视频的时候去掉也能观察出来
                time.sleep(0.01)
                f.write(chunk)
                f.flush()
                pbar.update(1024)
    pbar.close()
    return file_size

if __name__ == '__main__':
    url = "https://ss0.bdstatic.com/94oJfD_bAAcT8t7mm9GUKT-xh_/timg?image&quality=100&size=b4000_4000&sec=1551406646&di=a385cb186c0f1c2c45e5c49b4015e848&src=http://img18.3lian.com/d/file/201709/21/f498e01633b5b704ebfe0385f52bad20.jpg"
    download_from_url(url, "百度美女图片.jpg")
    #以下是tqdm实例
    tqdm_demo()
    tqdm_demo2()
    tqdm_demo3()
```

### 多线程

[多进程分片下载](https://www.cnblogs.com/bergus/p/4903715.html)

```python
from __future__ import unicode_literals

from multiprocessing.dummy import Pool as ThreadPool
import threading

import os
import sys
import cPickle
from collections import namedtuple
import urllib2
from urlparse import urlsplit

import time


# global lock
lock = threading.Lock()


# default parameters
defaults = dict(
    thread_count=10,
    buffer_size=500 * 1024,
    block_size=1000 * 1024)


def progress(percent, width=50):
    print "%s %d%%\r" % (('%%-%ds' % width) % (width * percent / 100 * '='), percent),
    if percent >= 100:
        print
        sys.stdout.flush()


def write_data(filepath, data):
    with open(filepath, 'wb') as output:
        cPickle.dump(data, output)


def read_data(filepath):
    with open(filepath, 'rb') as output:
        return cPickle.load(output)


FileInfo = namedtuple('FileInfo', 'url name size lastmodified')


def get_file_info(url):
    class HeadRequest(urllib2.Request):

        def get_method(self):
            return "HEAD"
    res = urllib2.urlopen(HeadRequest(url))
    res.read()
    headers = dict(res.headers)
    size = int(headers.get('content-length', 0))
    lastmodified = headers.get('last-modified', '')
    name = None
    if headers.has_key('content-disposition'):
        name = headers['content-disposition'].split('filename=')[1]
        if name[0] == '"' or name[0] == "'":
            name = name[1:-1]
    else:
        name = os.path.basename(urlsplit(url)[2])

    return FileInfo(url, name, size, lastmodified)


def download(url, output,
             thread_count=defaults['thread_count'],
             buffer_size=defaults['buffer_size'],
             block_size=defaults['block_size']):
    # get latest file info
    file_info = get_file_info(url)

    # init path
    if output is None:
        output = file_info.name
    workpath = '%s.ing' % output
    infopath = '%s.inf' % output

    # split file to blocks. every block is a array [start, offset, end],
    # then each greenlet download filepart according to a block, and
    # update the block' offset.
    blocks = []

    if os.path.exists(infopath):
        # load blocks
        _x, blocks = read_data(infopath)
        if (_x.url != url or
                _x.name != file_info.name or
                _x.lastmodified != file_info.lastmodified):
            blocks = []

    if len(blocks) == 0:
        # set blocks
        if block_size > file_info.size:
            blocks = [[0, 0, file_info.size]]
        else:
            block_count, remain = divmod(file_info.size, block_size)
            blocks = [[i * block_size, i * block_size,
                       (i + 1) * block_size - 1] for i in range(block_count)]
            blocks[-1][-1] += remain
        # create new blank workpath
        with open(workpath, 'wb') as fobj:
            fobj.write('')

    print 'Downloading %s' % url
    # start monitor
    threading.Thread(target=_monitor, args=(
        infopath, file_info, blocks)).start()

    # start downloading
    with open(workpath, 'rb+') as fobj:
        args = [(url, blocks[i], fobj, buffer_size)
                for i in range(len(blocks)) if blocks[i][1] < blocks[i][2]]

        if thread_count > len(args):
            thread_count = len(args)

        pool = ThreadPool(thread_count)
        pool.map(_worker, args)
        pool.close()
        pool.join()

    # rename workpath to output
    if os.path.exists(output):
        os.remove(output)
    os.rename(workpath, output)

    # delete infopath
    if os.path.exists(infopath):
        os.remove(infopath)

    assert all([block[1] >= block[2] for block in blocks]) is True


def _worker((url, block, fobj, buffer_size)):
    req = urllib2.Request(url)
    req.headers['Range'] = 'bytes=%s-%s' % (block[1], block[2])
    res = urllib2.urlopen(req)

    while 1:
        chunk = res.read(buffer_size)
        if not chunk:
            break
        with lock:
            fobj.seek(block[1])
            fobj.write(chunk)
            block[1] += len(chunk)


def _monitor(infopath, file_info, blocks):
    while 1:
        with lock:
            percent = sum([block[1] - block[0]
                           for block in blocks]) * 100 / file_info.size
            progress(percent)
            if percent >= 100:
                break
            write_data(infopath, (file_info, blocks))
        time.sleep(2)


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='多线程文件下载器.')
    parser.add_argument('url', type=str, help='下载连接')
    parser.add_argument('-o', type=str, default=None,
                        dest="output", help='输出文件')
    parser.add_argument(
        '-t', type=int, default=defaults['thread_count'], dest="thread_count", help='下载的线程数量')
    parser.add_argument(
        '-b', type=int, default=defaults['buffer_size'], dest="buffer_size", help='缓存大小')
    parser.add_argument(
        '-s', type=int, default=defaults['block_size'], dest="block_size", help='字区大小')

    argv = sys.argv[1:]

    if len(argv) == 0:
        argv = ['https://eyes.nasa.gov/eyesproduct/EYES/os/win']

    args = parser.parse_args(argv)

    start_time = time.time()
    download(args.url, args.output, args.thread_count,
             args.buffer_size, args.block_size)
    print '下载时间: %ds' % int(time.time() - start_time)
```

示例2

```python
# 在python3下测试
 
import sys
import requests
import threading
import datetime
 
# 传入的命令行参数，要下载文件的url
url = sys.argv[1]
 
 
def Handler(start, end, url, filename):
    
    headers = {'Range': 'bytes=%d-%d' % (start, end)}
    r = requests.get(url, headers=headers, stream=True)
    
    # 写入文件对应位置
    with open(filename, "r+b") as fp:
        fp.seek(start)
        var = fp.tell()
        fp.write(r.content)
 
 
def download_file(url, num_thread = 5):
    
    r = requests.head(url)
    try:
        file_name = url.split('/')[-1]
        file_size = int(r.headers['content-length'])   # Content-Length获得文件主体的大小，当http服务器使用Connection:keep-alive时，不支持Content-Length
    except:
        print("检查URL，或不支持对线程下载")
        return
 
    #  创建一个和要下载文件一样大小的文件
    fp = open(file_name, "wb")
    fp.truncate(file_size)
    fp.close()
 
    # 启动多线程写文件
    part = file_size // num_thread  # 如果不能整除，最后一块应该多几个字节
    for i in range(num_thread):
        start = part * i
        if i == num_thread - 1:   # 最后一块
            end = file_size
        else:
            end = start + part
 
        t = threading.Thread(target=Handler, kwargs={'start': start, 'end': end, 'url': url, 'filename': file_name})
        t.setDaemon(True)
        t.start()
 
    # 等待所有线程下载完成
    main_thread = threading.current_thread()
    for t in threading.enumerate():
        if t is main_thread:
            continue
        t.join()
    print('%s 下载完成' % file_name)
 
if __name__ == '__main__':
    start = datetime.datetime.now().replace(microsecond=0)  
    download_file(url)
    end = datetime.datetime.now().replace(microsecond=0)
    print("用时: ", end='')
    print(end-start)
```

### 示例四

[参考](https://blog.csdn.net/weixin_34024034/article/details/90583730)

```python
import requests, sys, os, re, time
from optparse import OptionParser

class wget:
	def __init__(self, config = {}):
		self.config = {
			'block': int(config['block'] if config.has_key('block') else 1024),
		}
		self.total = 0
		self.size = 0
		self.filename = ''

	def touch(self, filename):
		with open(filename, 'w') as fin:
			pass

	def remove_nonchars(self, name):
		(name, _) = re.subn(ur'[\\\/\:\*\?\"\<\>\|]', '', name)
		return name

	def support_continue(self, url):
    # 判断是否支持断点续传
		headers = {
			'Range': 'bytes=0-4'
		}
		try:
			r = requests.head(url, headers = headers)
			crange = r.headers['content-range']
			self.total = int(re.match(ur'^bytes 0-4/(\d+)$', crange).group(1))
			return True
		except:
			pass
		try:
			self.total = int(r.headers['content-length'])
		except:
			self.total = 0
		return False


	def download(self, url, filename, headers = {}):
		finished = False
		block = self.config['block']
		local_filename = self.remove_nonchars(filename)
		tmp_filename = local_filename + '.downtmp'
		size = self.size
		total = self.total
		if self.support_continue(url):  # 支持断点续传
			try:
				with open(tmp_filename, 'rb') as fin:
					self.size = int(fin.read())
					size = self.size + 1
			except:
				self.touch(tmp_filename)
			finally:
				headers['Range'] = "bytes=%d-" % (self.size, )
		else:
			self.touch(tmp_filename)
			self.touch(local_filename)

		r = requests.get(url, stream = True, verify = False, headers = headers)
		if total > 0:
			print "[+] Size: %dKB" % (total / 1024)
		else:
			print "[+] Size: None"
		start_t = time.time()
		with open(local_filename, 'ab+') as f:
			f.seek(self.size)
			f.truncate()
			try:
				for chunk in r.iter_content(chunk_size = block): 
					if chunk:
						f.write(chunk)
						size += len(chunk)
						f.flush()
					sys.stdout.write('\b' * 64 + 'Now: %d, Total: %s' % (size, total))
					sys.stdout.flush()
				finished = True
				os.remove(tmp_filename)
				spend = int(time.time() - start_t)
				speed = int((size - self.size) / 1024 / spend)
				sys.stdout.write('\nDownload Finished!\nTotal Time: %ss, Download Speed: %sk/s\n' % (spend, speed))
				sys.stdout.flush()
			except:
				# import traceback
				# print traceback.print_exc()
				print "\nDownload pause.\n"
			finally:
				if not finished:
					with open(tmp_filename, 'wb') as ftmp:
						ftmp.write(str(size))

if __name__ == '__main__':
	parser = OptionParser()
	parser.add_option("-u", "--url", dest="url",  
	                  help="target url")
	parser.add_option("-o", "--output", dest="filename",  
	                  help="download file to save")
	parser.add_option("-a", "--user-agent", dest="useragent", 
					  help="request user agent", default='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/537.36 \
			(KHTML, like Gecko) Chrome/40.0.2214.111 Safari/537.36')
	parser.add_option("-r", "--referer", dest="referer", 
					  help="request referer")
	parser.add_option("-c", "--cookie", dest="cookie", 
					  help="request cookie", default = 'foo=1;')
	(options, args) = parser.parse_args()
	if not options.url:
		print 'Missing url'
		sys.exit()
	if not options.filename:
		options.filename = options.url.split('/')[-1]
	headers = {
		'User-Agent': options.useragent,
		'Referer': options.referer if options.referer else options.url,
		'Cookie': options.cookie
	}
	wget().download(options.url, options.filename)
```

### 图片音乐视频

```python
# -*- coding:utf-8 -*-
 
import re
import requests
from contextlib import closing
from lxml import etree
 
 
class Spider(object):
    """ crawl image """
    def __init__(self):
        self.index = 0
        self.url = "http://www.xiaohuar.com"
        self.proxies = {"http": "http://172.17.18.80:8080", "https": "https://172.17.18.80:8080"}
        pass
 
    def download_image(self, image_url):
        real_url = self.url + image_url
        print "downloading the {0} image".format(self.index)
        with open("{0}.jpg".format(self.index), 'wb') as f:
            self.index += 1
            f.write(requests.get(real_url, proxies=self.proxies).content)
            pass
        pass
 
    def start_crawl(self):
        start_url = "http://www.xiaohuar.com/hua/"
        r = requests.get(start_url, proxies=self.proxies)
        if r.status_code == 200:
            temp = r.content.decode("gbk")
            html = etree.HTML(temp)
            links = html.xpath('//div[@class="item_t"]//img/@src')
            map(self.download_image, links)
            # next_page_url = html.xpath('//div[@class="page_num"]//a/text()')
            # print next_page_url[-1]
            # print next_page_url[-2]
            # print next_page_url[-3]
            next_page_url = html.xpath(u'//div[@class="page_num"]//a[contains(text(),"下一页")]/@href')
            page_num = 2
            while next_page_url:
                print "download {0} page images".format(page_num)
                r_next = requests.get(next_page_url[0], proxies=self.proxies)
                if r_next.status_code == 200:
                    html = etree.HTML(r_next.content.decode("gbk"))
                    links = html.xpath('//div[@class="item_t"]//img/@src')
                    map(self.download_image, links)
                    try:
                        next_page_url = html.xpath(u'//div[@class="page_num"]//a[contains(text(),"下一页")]/@href')
                    except BaseException as e:
                        next_page_url = None
                        print e
                    page_num += 1
                    pass
                else:
                    print "response status code : {0}".format(r_next.status_code)
                pass
        else:
            print "response status code : {0}".format(r.status_code)
        pass
 
 
class ProgressBar(object):
    def __init__(self, title, count=0.0, run_status=None, fin_status=None, total=100.0, unit='', sep='/', chunk_size=1.0):
        super(ProgressBar, self).__init__()
        self.info = "[%s] %s %.2f %s %s %.2f %s"
        self.title = title
        self.total = total
        self.count = count
        self.chunk_size = chunk_size
        self.status = run_status or ""
        self.fin_status = fin_status or " " * len(self.status)
        self.unit = unit
        self.seq = sep
 
    def __get_info(self):
        # 【名称】状态 进度 单位 分割线 总数 单位
        _info = self.info % (self.title, self.status,
                             self.count / self.chunk_size, self.unit, self.seq, self.total / self.chunk_size, self.unit)
        return _info
 
    def refresh(self, count=1, status=None):
        self.count += count
        # if status is not None:
        self.status = status or self.status
        end_str = "\r"
        if self.count >= self.total:
            end_str = '\n'
            self.status = status or self.fin_status
        print self.__get_info(), end_str
 
 
def download_mp4(video_url):
    print video_url
    try:
        with closing(requests.get(video_url.strip().decode(), stream=True)) as response:
            chunk_size = 1024
            with open('./{0}'.format(video_url.split('/')[-1]), "wb") as f:
                for data in response.iter_content(chunk_size=chunk_size):
                    f.write(data)
                    f.flush()
 
    except BaseException as e:
        print e
        return
 
 
def mp4():
    proxies = {"http": "http://172.17.18.80:8080", "https": "https://172.17.18.80:8080"}
    url = "http://www.budejie.com/video/"
    r = requests.get(url)
    print r.url
    if r.status_code == 200:
        print "status_code:{0}".format(r.status_code)
        content = r.content
        video_urls_compile = re.compile("http://.*?\.mp4")
        video_urls = re.findall(video_urls_compile, content)
        print len(video_urls)
        # print video_urls
        map(download_mp4, video_urls)
    else:
        print "status_code:{0}".format(r.status_code)
 
 
def mp3():
    proxies = {"http": "http://172.17.18.80:8080", "https": "https://172.17.18.80:8080"}
    with closing(requests.get("http://www.futurecrew.com/skaven/song_files/mp3/razorback.mp3", proxies=proxies, stream=True)) as response:
        chunk_size = 1024
        content_size = int(response.headers['content-length'])
        progress = ProgressBar("razorback", total=content_size, unit="KB", chunk_size=chunk_size, run_status="正在下载",
                               fin_status="下载完成")
        # chunk_size = chunk_size < content_size and chunk_size or content_size
        with open('./file.mp3', "wb") as f:
            for data in response.iter_content(chunk_size=chunk_size):
                f.write(data)
                progress.refresh(count=len(data))
 
 
if __name__ == "__main__":
    t = Spider()
    t.start_crawl()   
    mp3()
    mp4()   
    pass
```

# 不同方法

## socket

### server

```python
import socket
import os
 
sock = socket.socket()
sock.bind(("127.0.0.1", 8080))
sock.listen(5)
 
had_recv = 0
 
while True:
    conn, client_address = sock.accept()
 
    first_recv = str(conn.recv(1024),encoding="utf-8")
    src_path, file_size, dst_path = first_recv.split(" ",3)
    total_size = int(file_size)
    if os.path.exists(dst_path):
        had_recv = os.stat(dst_path).st_size
        conn.sendall(bytes("Y-" + str(had_recv), encoding="utf-8"))
        # 为了避免粘包问题
        reponse = conn.recv(1024)
        print(str(reponse, encoding="utf-8"))
        f = open(dst_path, "ab")
    else:
        conn.sendall(bytes("N", encoding="utf-8"))
        # 为了避免粘包问题
        reponse = conn.recv(1024)
        print(str(reponse, encoding="utf-8"))
        f = open(dst_path, "wb")
    #文件已接收完，关闭连接，结束
    while True:
        if total_size == had_recv:
            conn.close()
            break
        data = conn.recv(1024)
        f.write(data)
        had_recv += len(data)
        print(had_recv,total_size)
```

### client

```python
#! /usr/bin/env python
# -*- coding:utf-8 -*-
 
import socket
import os
 
sock = socket.socket()
sock.bind(("127.0.0.1", 8080))
sock.listen(5)
 
had_recv = 0
 
while True:
    conn, client_address = sock.accept()
 
    first_recv = str(conn.recv(1024),encoding="utf-8")
    src_path, file_size, dst_path = first_recv.split(" ",3)
    total_size = int(file_size)
    if os.path.exists(dst_path):
        had_recv = os.stat(dst_path).st_size
        conn.sendall(bytes("Y-" + str(had_recv), encoding="utf-8"))
        # 为了避免粘包问题
        reponse = conn.recv(1024)
        print(str(reponse, encoding="utf-8"))
        f = open(dst_path, "ab")
    else:
        conn.sendall(bytes("N", encoding="utf-8"))
        # 为了避免粘包问题
        reponse = conn.recv(1024)
        print(str(reponse, encoding="utf-8"))
        f = open(dst_path, "wb")
    #文件已接收完，关闭连接，结束
    while True:
        if total_size == had_recv:
            conn.close()
            break
        data = conn.recv(1024)
        f.write(data)
        had_recv += len(data)
        print(had_recv,total_size)
```

## ftp

[参考](https://blog.csdn.net/weixin_30622181/article/details/98517968)

### 原理

要求

```
1、用户md5认证
2、支持多用户同时登陆（并发）
3、进入用户的命令行模式，支持cd切换目录，ls查看目录子文件
4、执行命令（ipconfig）
5、传输文件：a、支持断点续传，b、传输中显示进度条
```

思路
```
1.客户端用户登录和注册：
	a、客户端仅提供用户名和密码，选择登录或注册，
	b、服务器端进行注册并将加密后的密码写入文件，最后返回给客户端是否登录或注册成功
2.ls和cd命令
	a、客户端输入命令，服务器端处理并返回给客户端
3.执行命令：
	a、客户端发送需要执行的命令
	b、服务器端执行命令，并返回客户端需要接收该命令的次数s=r[0]+1,其中r=divmod（结果总长度，1024）
	c、客户端收到次数，告诉服务端已经收到
	d、服务端发送执行结果，客户端进行for循环接收该结果
4.发送文件：
	a、客户端输入文件路径（测试版路径为：f.png），发送文件名和文件大小
	b、服务器端检测指定目录是否含有该文件，如果没有，返回给客户端字符串s，即从头开始发送start，has_recv=0；如果有，即需要断点续传，返回给客户端已经上传了多少has_recv
	c、客户端接收返回值，并seek到has_recv的位置，进行循环收发，打印当前进度，直到传输完毕。

注：本程序可循环接收用户选择传输文件和执行命令
```
配置文件

```python
import os
 
BASE_DIR = os.path.dirname(os.path.dirname(__file__))  #配置文件的上层目录
NEW_FILENAME=os.path.join(BASE_DIR,'view')             #新文件目录
NAME_PWD=os.path.join(BASE_DIR,'db','name_pwd')        #用户名和密码目录
USER_FILE=os.path.join(BASE_DIR,'db')
```

### server

```python
import sys,os
import time
import socket
import hashlib
import pickle
import subprocess
import socketserver
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from config import settings
 

new = settings.NEW_FILENAME

class Myserver(socketserver.BaseRequestHandler):
 
    def recv_file(self):
        '''
        文件传输
        :return:
        '''
        conn=self.request
        a=str(conn.recv(1024),encoding='utf-8')
        file_size,file_name=a.split(',')
        new_file_name=os.path.join(new,file_name)
        if file_name in new:            #检测文件是否已存在，涉及断点续传
            has_recv=os.stat(new).st_size #计算临时文件大小
            conn.sendall(bytes(has_recv,encoding='utf-8'))
            with open(new_file_name,'ab') as f:  #追加模式
                while has_recv<=int(file_size):
                    data=conn.recv(1024)
                    f.write(data)
                    has_recv+=len(data)
        else:
            has_recv=0
            conn.sendall(bytes('s',encoding='utf-8')) # 客户端收到字符串s，从0开始发送
            with open(new_file_name,'wb') as f:
                while has_recv<=int(file_size):
                    data=conn.recv(1024)
                    f.write(data)
                    has_recv+=len(data)
 
    def command(self):
        '''
        执行命令
        :return:
        '''
        conn=self.request
        a=conn.recv(1024)
        ret=str(a,encoding='utf-8')
        ret2 = subprocess.check_output(ret, shell=True)
        r=divmod(len(ret2),1024)
        s=r[0]+1         #客户端需要接收的次数
        conn.sendall(bytes(str(s),encoding='utf-8'))
        conn.recv(1024)  #确认客户端收到需要接收的次数
 
        conn.sendall(ret2)
 
    def md5(self,pwd):
        '''
        对密码进行加密
        :param pwd: 密码
        :return:
        '''
        hash=hashlib.md5(bytes('xx7',encoding='utf-8'))
        hash.update(bytes(pwd,encoding='utf-8'))
        return hash.hexdigest()
 
 
    def login(self,usrname,pwd):
        '''
        登陆
        :param usrname: 用户名
        :param pwd: 密码
        :return:是否登陆成功
        '''
        conn=self.request
        s=pickle.load(open(settings.NAME_PWD,'rb'))
        if usrname in s:
             if s[usrname]==self.md5(pwd):        #和加密后的密码进行比较
                return True
             else:
                return False
        else:
            return False
 
 
    def regist(self,usrname,pwd):
        '''
        注册
        :param usrname: 用户名
        :param pwd: 密码
        :return:是否注册成功
        '''
 
        conn=self.request
        s=pickle.load(open(settings.NAME_PWD,'rb'))
        if usrname in s:
             return False
        else:
            s[usrname]=self.md5(pwd)
            mulu=os.path.join(settings.USER_FILE,usrname)
            os.makedirs(mulu,'a')
            pickle.dump(s,open(settings.NAME_PWD,'wb'))
            return True
 
    def before(self,usrname,pwd,ret):
        '''
        判断注册和登陆，并展示用户的详细目录信息，支持cd和ls命令
        :return:
        '''
        conn=self.request
        if ret=='1':
            r=self.login(usrname,pwd)
            if r:
                conn.sendall(bytes('y',encoding='utf-8'))
            else:
                conn.sendall(bytes('n',encoding='utf-8'))
        elif ret=='2':
            # print(usrname,pwd)
            r=self.regist(usrname,pwd)
            if r:
                conn.sendall(bytes('y',encoding='utf-8'))
            else:
                conn.sendall(bytes('n',encoding='utf-8'))
    def usr_file(self,usrname):
        '''
        展示用户的详细目录信息，支持cd和ls命令
        :param usrname: 用户名
        :return:
        '''
        conn=self.request
        conn.recv(1024)
        mulu=os.path.join(settings.USER_FILE,usrname)
        conn.sendall(bytes(mulu,encoding='utf-8'))
        while True:
            b=conn.recv(1024)
            ret=str(b,encoding='utf-8')
            try:
                a,b=ret.split(' ',1)
            except Exception as e:
                a=ret
            if a=='cd':
                if b=='..':
                    mulu=os.path.dirname(mulu)
                else:
                    mulu=os.path.join(mulu,b)
                conn.sendall(bytes(mulu,encoding='utf-8'))
            elif a=='ls':
                ls=os.listdir(mulu)
                print(ls)
                a=','.join(ls)
                conn.sendall(bytes(a,encoding='utf-8'))
            elif a=='q':
                break
 
 
    def handle(self):
        conn=self.request
        conn.sendall(bytes('welcome',encoding='utf-8'))
        b=conn.recv(1024)
        ret=str(b,encoding='utf-8')
        print(ret)
        conn.sendall(bytes('b ok',encoding='utf-8'))
        c=conn.recv(1024)
        r=str(c,encoding='utf-8')
        usrname,pwd=r.split(',')
        self.before(usrname,pwd,ret) #登陆或注册验证
        self.usr_file(usrname)  #展示用户的详细目录信息，支持cd和ls命令
        while True:
            a=conn.recv(1024)
            conn.sendall(bytes('收到a',encoding='utf-8'))
            ret=str(a,encoding='utf-8')
            if ret=='1':
                self.recv_file()
                # conn.sendall(bytes('file ok',encoding='utf-8'))
            elif ret=='2':
                self.command()
            elif ret=='q':
                break
            else:
                pass
 
if __name__=='__main__':
    sever=socketserver.ThreadingTCPServer(('127.0.0.1',9999),Myserver)
    sever.serve_forever()
```

### client

```python
import sys
import time
import os
import socket
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from config import settings
 

def send_file(file_path):
    '''
    发送文件
    :param file_name:文件名
    :return:
    '''
    size=os.stat(file_path).st_size
    file_name=os.path.basename(file_path)
    obj.sendall(bytes(str(size)+','+file_name,encoding='utf-8')) #发送文件大小和文件名
    ret=obj.recv(1024)   #接收已经传了多少
    r=str(ret,encoding='utf-8')
    if r=='s': #文件不存在，从头开始传
        has_send=0
    else:   #文件存在
        has_send=int(r)
 
    with open(file_path,'rb') as f:
        f.seek(has_send) #定位到已经传到的位置
        while has_send<size:
            data=f.read(1024)
            obj.sendall(data)
            has_send+=len(data)
            sys.stdout.write('\r')  #清空文件内容
            time.sleep(0.2)
            sys.stdout.write('已发送%s%%|%s' %(int(has_send/size*100),(round(has_send/size*40)*'★')))
            sys.stdout.flush()   #强制刷出内存
        print("上传成功\n")
 
def command(command_name):
    '''
    执行命令
    :param command_name:
    :return:
    '''
    obj.sendall(bytes(command_name,encoding='utf-8'))
    ret=obj.recv(1024)  #接收命令需要接收的次数
    obj.sendall(bytes('收到次数',encoding='utf-8'))
    r=str(ret,encoding='utf-8')
    for i in range(int(r)): #共需要接收int(r)次
        ret=obj.recv(1024)  #等待客户端发送
        r=str(ret,encoding='GBK')
        print(r)
 
def login(usrname,pwd):
    '''
    登陆
    :param usrname:用户名
    :param pwd:密码
    :return:是否登陆成功
    '''
    obj.sendall(bytes(usrname+','+pwd,encoding='utf-8'))
    ret=obj.recv(1024)
    r=str(ret,encoding='utf-8')
    if r=='y':
        return 1
    else:
        return 0
 
def regist(usrname,pwd):
    '''
    注册
    :param usrname:用户名
    :param pwd:密码
    :return:是否注册成功
    '''
    obj.sendall(bytes(usrname+','+pwd,encoding='utf-8'))
    ret=obj.recv(1024)
    r=str(ret,encoding='utf-8')
    if r=='y':
        return 1
    else:
        return 0
def before(usrname,pwd):
    '''
    选择登陆或注册，展示用户的详细目录信息，支持cd和ls命令
    :return:
    '''
    a=input('请选择1.登陆 2.注册')
    obj.sendall(bytes(a,encoding='utf-8'))
    obj.recv(1024)
    if a=='1':
        ret=login(usrname,pwd)
        if ret:
            print('登陆成功')
            return 1
        else:
            print('用户名或密码错误')
            return 0
    elif a=='2':
        ret=regist(usrname,pwd)
        if ret:
            print('注册成功')
            return 1
        else:
            print('用户名已存在')
            return 0
def usr_file(usrname):
    obj.sendall(bytes('打印用户文件路径',encoding='utf-8'))
    ret=obj.recv(1024)  #等待客户端发送
    r=str(ret,encoding='utf-8')
    print(r)
    while True:
        a=input('输入cd切换目录，ls查看目录详细信息，q退出>:')
 
        obj.sendall(bytes(a,encoding='utf-8'))
        if a=='q':
            break
        else:
            ret=obj.recv(1024)  #等待客户端发送
            r=str(ret,encoding='utf-8')
            if len(r)==1:#判断是cd结果还是ls的结果（ls只有一个子目录也可以直接打印）
                print(r)
            else:
                li=r.split(',')
                for i in li:
                    print(i)  #打印每一个子目录
 
def main(usrname,pwd):
    ret=obj.recv(1024)  #等待客户端发送
    r=str(ret,encoding='utf-8')
    print(r)
    result=before(usrname,pwd)#登陆或注册
    if result:
        usr_file(usrname)
        while True:
            a=input('请选择1.传文件 2.执行命令 q退出:')
            obj.sendall(bytes(str(a),encoding='utf-8'))
            ret=obj.recv(1024) #确认是否收到a
            r=str(ret,encoding='utf-8')
            print(r)
            if a=='1':
                b=input('请输入文件路径（测试版路径为：f.png）:')
                # b='f.png'
                if os.path.exists(b):
                    send_file(b)
                    obj.sendall(bytes('hhe',encoding='utf-8'))
                    # obj.recv(1024)
            elif a=='2':
                b=input('请输入command:')
                command(b)
            elif a=='q':
                break
            else:
                print('输入错误')
 
    obj.close()
 
if __name__ == '__main__':
    obj=socket.socket() #创建客户端socket对象
    obj.connect(('127.0.0.1',9999))
    usrname=input('请输入用户名')
    pwd=input('请输入密码')
    main(usrname,pwd)
```

## django

[参考](https://blog.csdn.net/kuanggudejimo/article/details/99638109)

- 实现方式

功能基于django.views.static.serve实现，实现的关键点是：
```
response中增加'Content-Range'、'Cache-Control'的参数
    
根据不同的情况为response设置不同的status
    
根据HTTP_RANGE对读取文件时的起始位置进行设置
```

- 视图函数

```python
# myproject/views_file.py
import re
import os
import stat
import mimetypes
import posixpath
from django.utils._os import safe_join
from django.utils.http import http_date
from django.views.static import was_modified_since
from django.http import Http404, FileResponse, HttpResponseNotModified
 
 
# 基于django.views.static.serve实现，支持大文件的断点续传（暂停/继续下载）
def get_file_response(request, path, document_root=None):
    # 防止目录遍历漏洞
    path = posixpath.normpath(path).lstrip('/')
    fullpath = safe_join(document_root, path)
    if os.path.isdir(fullpath):
        raise Http404('Directory indexes are not allowed here.')
    if not os.path.exists(fullpath):
        raise Http404('"%(path)s" does not exist' % {'path': fullpath})
 
    statobj = os.stat(fullpath)
 
    # 判断下载过程中文件是否被修改过
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
                              statobj.st_mtime, statobj.st_size):
        return HttpResponseNotModified()
 
    # 获取文件的content_type
    content_type, encoding = mimetypes.guess_type(fullpath)
    content_type = content_type or 'application/octet-stream'
 
    # 计算读取文件的起始位置
    start_bytes = re.search(r'bytes=(\d+)-', request.META.get('HTTP_RANGE', ''), re.S)
    start_bytes = int(start_bytes.group(1)) if start_bytes else 0
 
    # 打开文件并移动下标到起始位置，客户端点击继续下载时，从上次断开的点继续读取
    the_file = open(fullpath, 'rb')
    the_file.seek(start_bytes, os.SEEK_SET)
 
    # status=200表示下载开始，status=206表示下载暂停后继续，为了兼容火狐浏览器而区分两种状态
    # 关于django的response对象，参考：https://www.cnblogs.com/scolia/p/5635546.html
    # 关于response的状态码，参考：https://www.cnblogs.com/DeasonGuan/articles/Hanami.html
    # FileResponse默认block_size = 4096，因此迭代器每次读取4KB数据
    response = FileResponse(the_file, content_type=content_type, status=206 if start_bytes > 0 else 200)
 
    # 'Last-Modified'表示文件修改时间，与'HTTP_IF_MODIFIED_SINCE'对应使用，参考：https://www.jianshu.com/p/b4ecca41bbff
    response['Last-Modified'] = http_date(statobj.st_mtime)
 
    # 这里'Content-Length'表示剩余待传输的文件字节长度
    if stat.S_ISREG(statobj.st_mode):
        response['Content-Length'] = statobj.st_size - start_bytes
    if encoding:
        response['Content-Encoding'] = encoding
 
    # 'Content-Range'的'/'之前描述响应覆盖的文件字节范围，起始下标为0，'/'之后描述整个文件长度，与'HTTP_RANGE'对应使用
    # 参考：http://liqwei.com/network/protocol/2011/886.shtml
    response['Content-Range'] = 'bytes %s-%s/%s' % (start_bytes, statobj.st_size - 1, statobj.st_size)
 
    # 'Cache-Control'控制浏览器缓存行为，此处禁止浏览器缓存，参考：https://blog.csdn.net/cominglately/article/details/77685214
    response['Cache-Control'] = 'no-cache, no-store, must-revalidate'
    return response
```

- URL

```python
# myproject/urls.py
 
from django.urls import re_path
from django.conf import settings
from myproject import views_file
 
# MEDIA_ROOT是要下载的文件的存储路径的前半段，下面的配置中'files/.*'匹配到的路径则是后半段，两者合并就是要下载的文件的完整路径
urlpatterns = [re_path(r'^download/(files/.*)$', views_file.get_file_response, {'document_root': settings.MEDIA_ROOT})]
```

