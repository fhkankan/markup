# 安装启动

## 单机

### 系统中

#### 安装

deb

```shell
sudo apt-get install apt-transport-https ca-certificates dirmngr
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4

echo "deb https://repo.clickhouse.tech/deb/stable/ main/" | sudo tee \
    /etc/apt/sources.list.d/clickhouse.list
sudo apt-get update

sudo apt-get install -y clickhouse-server clickhouse-client

sudo service clickhouse-server start
clickhouse-client
```

#### 目录

目录结构

```shell
/etc/clickhouse-server
# 服务端的配置文件目录，包括全局配置config.xml和用户配置users.xml等。

/etc/clickhouse-client
# 客户端配置，包括conf.d文件夹和config.xml文件。

/var/lib/clickhouse
# 默认的数据存储目录（通常会修改默认路径配置，将数据保存到大容量磁盘挂载的路径）。

/var/log/clickhouse-server
# 默认保存日志的目录（通常会修改路径配置，将日志保存到大容量磁盘挂载的路径）。
```

配置文件

```shell
/etc/security/limits.d/clickhouse.conf
# 文件句柄数量的配置，该配置也可以通过config.xml的max_open_files修改

/etc/cron.d/clickhouse-server:cron
# 定时任务配置，用于恢复因异常原因中断的ClickHouse服务进程，其默认的配置如下。
```

可执行文件

```shell
# /usr/bin路径下的可执行文件

clickhouse  # 主程序的可执行文件。
clickhouse-client  # 一个指向ClickHouse可执行文件的软链接，供客户端连接使用。
clickhouse-server  # 一个指向ClickHouse可执行文件的软链接，供服务端启动使用。
clickhouse-compressor  # 内置提供的压缩工具，可用于数据的正压反解。
```

#### 启动

- 启动服务端

如果没有`service`，可以运行如下命令在后台启动服务

```shell
sudo /etc/init.d/clickhouse-server start
```

日志文件将输出在`/var/log/clickhouse-server/`文件夹。

如果服务器没有启动，检查`/etc/clickhouse-server/config.xml`中的配置。

您也可以手动从控制台启动服务器:

```shell
clickhouse-server --config-file=/etc/clickhouse-server/config.xml
```

在这种情况下，日志将被打印到控制台，这在开发过程中很方便。

如果配置文件在当前目录中，则不需要指定`——config-file`参数。默认情况下，它的路径为`./config.xml`。

对于建立service的情况下

```shell
# 启动服务
service clickhouse-server start

# 关闭服务
service clickhouse-server stop
```

- 配置访问限制

ClickHouse支持多配置文件管理。主配置文件是`/etc/clickhouse-server/config.xml`。其余文件须在目录`/etc/clickhouse-server/config.d`。

ClickHouse支持访问限制设置。它们位于`users.xml`文件(与`config.xml`同级目录)。默认情况下，允许`default`用户从任何地方访问，不需要密码。可查看`user/default/networks`。

配置允许远程连接

```
1. cd /etc/clickhouse-server/config.xml
2. 将<listen_host>::</listen_host>取消注释;
3. service clickhouse-server restart
```

- 启动clickhouse-client

启动服务后，您可以使用命令行客户端连接到它:

```shell
clickhouse-client
```

默认情况下，使用`default`用户并不携带密码连接到`localhost:9000`。还可以使用`--host`参数连接到指定服务器。

终端必须使用UTF-8编码。

### docker

安装

```shell
docker pull yandex/clickhouse-server
```

使用

```shell
# 开始服务实例
docker run -d --name some-clickhouse-server --ulimit nofile=262144:262144 yandex/clickhouse-server
# 使用宿主机磁盘做数据存储
mkdir $HOME/some_clickhouse_database
docker run -d --name some-clickhouse-server --ulimit nofile=262144:262144 --volume=$HOME/some_clickhouse_database:/var/lib/clickhouse yandex/clickhouse-server 

# 从本机客户端连接到它
docker run -it --rm --link some-clickhouse-server:clickhouse-server yandex/clickhouse-client --host clickhouse-server
```

配置

```shell
# 容器为HTTP接口公开 8123 端口，为本机客户端公开 9000 端口。 ClickHouse 配置用文件“config.xml”（文档）表示
# 使用自定义配置启动服务器实例
docker run -d --name some-clickhouse-server --ulimit nofile=262144:262144 -v /path/to/your/config.xml:/etc/clickhouse-server/config.xml yandex/clickhouse-server
```

实战

```shell
# 下载
docker pull yandex/clickhouse-server
# 检查宿主机端口占用情况
lsof -i:8123
lsof -i:9000
lsof -i:9009
# 默认无账号秘密访问
docker run -d --name=some-clickhouse-server \
--ulimit nofile=262144:262144 \
-p 8123:8123 -p 9009:9009 -p 9090:9000 \
yandex/clickhouse-server


# 设置账号密码
# 创建临时容器
docker run --rm -d --name=some-clickhouse-server \
--ulimit nofile=262144:262144 \
-p 8123:8123 -p 9009:9009 -p 9090:9000 \
yandex/clickhouse-server
# 复制临时容器内配置文件到宿主机
docker cp clickhouse-server:/etc/clickhouse-server/config.xml /app/cloud/clickhouse/conf/config.xml
docker cp clickhouse-server:/etc/clickhouse-server/users.xml /app/cloud/clickhouse/conf/users.xml
# 停止临时容器
docker stop clickhouse-server
# 创建default账号密码
PASSWORD=$(base64 < /dev/urandom | head -c8); echo "$PASSWORD"; echo -n "$PASSWORD" | sha256sum | tr -d '-'  # 会输出明码和sha256密码
# 创建root账号密码
PASSWORD=$(base64 < /dev/urandom | head -c8); echo "$PASSWORD"; echo -n "$PASSWORD" | sha256sum | tr -d '-'
# 修改账号密码配置
# 在/app/cloud/clickhouse/conf/users.xml把default账号设为只读权限，并设置密码 yandex-->users-->default-->profile 节点设为 readonly 注释掉 yandex-->users-->default-->password 节点 新增  yandex-->users-->default-->password_sha256_hex 节点，填入生成的密码
# 新增root账号
<root>
<password_sha256_hex>35542ded44184b1b4b6cd621e052662578025b58b4187176a3ad2b9548c8356e</password_sha256_hex>
	 <networks incl="networks" replace="replace">
		<ip>::/0</ip>
	</networks>
	<profile>default</profile>
	<quota>default</quota>
</root>
# 创建容器
docker run -d --name=clickhouse-server \
-p 8123:8123 -p 9009:9009 -p 9090:9000 \
--ulimit nofile=262144:262144 \
-v /app/cloud/clickhouse/data:/var/lib/clickhouse:rw \
-v /app/cloud/clickhouse/conf/config.xml:/etc/clickhouse-server/config.xml \
-v /app/cloud/clickhouse/conf/users.xml:/etc/clickhouse-server/users.xml \
-v /app/cloud/clickhouse/log:/var/log/clickhouse-server:rw \
yandex/clickhouse-server
```

## 集群

### 特性

1)    clickhouse的cluster环境中，每台server的地位是等价的，即不存在master-slave之说，是multi-master模式。

2)    各replicated表的宿主server上要在hosts里配置其他replicated表宿主server的ip和hostname的映射。

3)    上面描述的在不同的server上建立全新的replicated模式的表，如果在某台server上已经存在一张replicated表，并且表中已经有数据，这时在另外的server上执行完replicated建表语句后，已有数据会自动同步到其他server上面。

4)    如果zookeeper挂掉，replicated表会切换成read-only模式，不再进行数据同步，系统会周期性的尝试与zk重新建立连接。

5)    如果在向一张replicated表insert数据的时候zookeeper挂掉，这时候会抛一个异常，等到与zk重新建立连接以后，系统（其他replicated表所在server）会检查本地文件与预期文件（保存在zk上）的差别，如果是轻微的差别，直接同步覆盖，如果发现有数据块损坏或者识别不了，则将这些数据文件移动到“detached”子目录，然后重新根据zk所记录的文件信息进行副本的同步。

6)    drop掉某一台server上的replicated表，不会对其他server上面的replicated表造成影响。

### 安装

环境变量

```shell
#!/bin/bash

all_ip_list="xx.xx.xx.xx xx.xx.xx.xx"
kafka_ip_list="xx.xx.xx.xx xx.xx.xx.xx"
zk_ip_list="xx.xx.xx.xx xx.xx.xx.xx"
java_ip_list="xx.xx.xx.xx xx.xx.xx.xx"
ch_ip_list="xx.xx.xx.xx xx.xx.xx.xx"
py_ip_list="xx.xx.xx.xx xx.xx.xx.xx"
opuser_pwd="xxx"
root_pwd="xxx"
ssh_conf=" -o StrictHostKeyChecking=no "
ch_cluster="cdp_test"
ch_pwd="xxx"
mysql_host="xx.xx.xx.xx"
mysql_port="3306"
mysql_user="root"
mysql_pwd="xx"
```

#### zookeeper

安装

```shell
#!/bin/bash
source ../../*config/env.sh

zk_version="apache-zookeeper-3.7.0-bin"

# download kafka_2.13-2.8.0.tgz

if [ ! -f "./tmp/$zk_version.tar.gz" ]; then

echo 'download kafka'
mkdir -p ./tmp
cd ./tmp
wget "https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.7.0/apache-zookeeper-3.7.0-bin.tar.gz"
tar -xzf $zk_version.tar.gz
cd ..
echo 'download kafka done'

fi

myId=1
for i in $zk_ip_list;
do
echo $i is appoint ;

sshpass -p $opuser_pwd scp -r ./tmp/$zk_version opuser@$i:/opt/soft/
sshpass -p $opuser_pwd ssh $ssh_conf  opuser@$i "ln -s /opt/soft/$zk_version /opt/soft/zookeeper" ;
sshpass -p $root_pwd ssh $ssh_conf  root@$i "mkdir -p /opt/data";
sshpass -p $root_pwd ssh $ssh_conf  root@$i "chown -R opuser:opuser /opt/data/" ;
sshpass -p $opuser_pwd ssh $ssh_conf  opuser@$i "mkdir -p /opt/data/pub/data/zookeeper" ;
sshpass -p $opuser_pwd ssh $ssh_conf  opuser@$i "echo $myId > /opt/data/pub/data/zookeeper/myid" ;
sshpass -p $opuser_pwd ssh $ssh_conf  opuser@$i "mkdir -p /opt/data/run/conf/zookeeper" ;
sshpass -p $opuser_pwd ssh $ssh_conf  opuser@$i "mkdir -p /opt/data/pub/data/logs/zookeeper" ;

sshpass -p $opuser_pwd scp ../../*config/zookeeper.properties opuser@$i:/opt/data/run/conf/zookeeper/

((myId=myId + 1))
done

```

启动

```shell
#!/bin/bash
source ../../*config/env.sh

for i in $zk_ip_list;
do
echo $i is appoint ;

sshpass -p $opuser_pwd ssh $ssh_conf  opuser@$i "mkdir -p /opt/data/run/conf" ;
sshpass -p $opuser_pwd ssh $ssh_conf  opuser@$i "/opt/soft/zookeeper/bin/zkServer.sh start /opt/data/run/conf/zookeeper/zookeeper.properties" ;

sshpass -p $opuser_pwd scp ../../*config/zookeeper.properties opuser@$i:/opt/data/run/conf/

done
```

服务

```shell
#!/bin/bash
source ../../*config/env.sh

for i in $zk_ip_list;
do
echo $i is appoint ;

sshpass -p $opuser_pwd ssh $ssh_conf  opuser@$i "/opt/soft/zookeeper/bin/zkServer.sh $1 /opt/data/run/conf/zookeeper/zookeeper.properties" ;

done
```

#### clickhouse

安装

```shell
#!/bin/bash
source ../../*config/env.sh

for i in $ch_ip_list;
do
echo $i is appoint ;

#sshpass -p $root_pwd ssh $ssh_conf  root@$i "yum install yum-utils" ;
#sshpass -p $root_pwd ssh $ssh_conf  root@$i "rpm --import https://repo.clickhouse.tech/CLICKHOUSE-KEY.GPG" ;
#sshpass -p $root_pwd ssh $ssh_conf  root@$i "yum-config-manager --add-repo https://repo.clickhouse.tech/rpm/stable/x86_64" ;
#sshpass -p $root_pwd ssh $ssh_conf  root@$i "yum install clickhouse-server clickhouse-client" ;

#sshpass -p $root_pwd ssh $ssh_conf  root@$i "mkdir -p /opt/data/pub/logs/clickhouse" ;
#sshpass -p $root_pwd ssh $ssh_conf  root@$i "mkdir -p /opt/data/pub/data/clickhouse" ;
#sshpass -p $root_pwd ssh $ssh_conf  root@$i "chown -R clickhouse:clickhouse /opt/data/pub/data/clickhouse /opt/data/pub/logs/clickhouse" ;

sshpass -p $root_pwd scp ../../*config/clickhouse-config.xml root@$i:/etc/clickhouse-server/config.xml
sshpass -p $root_pwd scp ../../*config/clickhouse-users.xml root@$i:/etc/clickhouse-server/users.xml
sshpass -p $root_pwd ssh $ssh_conf  root@$i "chown clickhouse:clickhouse /etc/clickhouse-server/config.xml" ;
sshpass -p $root_pwd ssh $ssh_conf  root@$i "chown clickhouse:clickhouse /etc/clickhouse-server/users.xml" ;

done
```

重启脚本

```shell
#!/bin/bash
source ../../*config/env.sh

for i in $ch_ip_list;
do
echo $i is appoint ;

sshpass -p $root_pwd ssh $ssh_conf  root@$i "systemctl restart clickhouse-server" ;
#sshpass -p $root_pwd ssh $ssh_conf  root@$i "service clickhouse-server start" ;
```

### 验证

在完成上述配置之后，在各自机器上启动clickhouse-server，并开启clickhouse-client

```shell
# 启动server
service clickhouse-server start
# 启动客户端，-m参数支持多行输入
clickhouse-client -m
```

查询系统表验证集群配置是否已被加载

```
cdh04 :) select cluster,shard_num,replica_num,host_name,port,user from system.clusters;
```

看一下集群的分片信息(宏变量)：分别在各自机器上执行下面命令

```shell
cdh04 :) select * from system.macros;
cdh05 :) select * from system.macros;
cdh06 :) select * from system.macros;
```

