[TOC]

# 多进程

## 概述

- fork

```
Unix/Linux操作系统提供了一个fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。

子进程永远返回0，而父进程返回子进程的ID。
这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。

有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。

在Unix/Linux下，multiprocessing模块封装了fork()调用，使我们不需要关注fork()的细节。由于Windows没有fork调用，因此，multiprocessing需要“模拟”出fork的效果，父进程所有Python对象都必须通过pickle序列化再传到子进程去，如果multiprocessing在Windows下调用失败了，要先考虑是不是pickle失败了。
```

示例

```python
import os

print('Process (%s) start...' % os.getpid())
# Only works on Unix/Linux/Mac:
pid = os.fork()
print('test')
if pid == 0:
    print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))
else:
    print('I (%s) just created a child process (%s).' % (os.getpid(), pid))
```

结果

```
Process (76513) start...
test
I (76513) just created a child process (76514).
test
I am child process (76514) and my parent is 76513.
```

- multiprocessing

```
multiprocessing模块就是跨平台版本的多进程管理模块，可以实现多进程的程序设计
支持子进程、通信和数据共享，提供多种形式的同步机制及Process、Queue、Pipe、Lock等组件
同时支持本地并发和远程并发，有效避免了全局解释器锁(GIL)问题，可以更有效地利用CPU资源，尤其适用于多核或多CPU环境
```

进程创建于管理模块

| 组件        | 描述                                                         |
| ----------- | ------------------------------------------------------------ |
| process     | 用于创建子进程，可以实现多进程的创建、启动、关闭等操作       |
| pool        | 用于创建管理进程池，当子进程非常多且需要控制子进程数量时使用 |
| manager     | 通常与pool一同使用，用于资源共享                             |
| pipe        | 用于进程间的管道通信                                         |
| queue       | 用于进程通信                                                 |
| value,array | 用于进程通信，资源共享                                       |

子进程同步模块

| 组件      | 描述                         |
| --------- | ---------------------------- |
| condition | 条件变量                     |
| event     | 用来实现进程间的同步通信     |
| lock      | 锁                           |
| rlock     | 多重锁                       |
| semaphore | 用来控制对共享资源的访问数量 |

## Process

multiprocessing模块提供了一个Process类来代表一个进程对象，这个对象可以理解为是一个独立的进程，可以执行另外的事情

```python
# Process创建的实例对象
Process([group [, target [, name [, args [, kwargs]]]]])
```

参数

| name   | 说明                                                     |
| ------ | -------------------------------------------------------- |
| target | 如果传递了函数的引用，可以任务这个子进程就执行这里的代码 |
| args   | 给target指定的函数传递的参数，以元组的方式传递           |
| kwargs | 给target指定的函数传递命名参数                           |
| name   | 给进程设定一个名字，可以不设定                           |
| group  | 指定进程组，大多数情况下用不到                           |

实例方法

| 方法              | 说明                                                         |
| ----------------- | ------------------------------------------------------------ |
| `is_alive`        | 返回进程是否在运行                                           |
| `join([timeout])` | 阻塞当前上下文环境的进程，直到调用此方法的进程终止或到达指定的timeout(可选参数) |
| `start()`         | 启动子进程实例(创建子进程)，进程准备就绪，等待CPU调度        |
| `run()`           | `start()`调用run方法，如果实例进程时未指定传入target,则star默认执行run方法 |
| ` terminate()`    | 不管任务是否完成，立即终止子进程                             |

属性

| 属性     | 说明                                                         |
| -------- | ------------------------------------------------------------ |
| daemon   | 与线程setDeamon的功能一样(将父进程设置为守护进程，当父进程结束时，子进程也结束) |
| exitcode | 进程在运行时为None,如果为-N,则表示被信号N结束                |
| name     | 当前进程的别名，默认为Process-N，N为从1开始递增的整数        |
| pid      | 当前进程的进程号                                             |

### 创建进程

创建一个进程

```python
from multiprocessing import Process
import os

# 子进程要执行的代码
def run_proc(name):
    print('Run child process %s (%s)...' % (name, os.getpid()))

if __name__=='__main__':
	# os.getpid():获取当前程序进程号
    print('Parent process %s.' % os.getpid())
    # 传入子进程函数名和参数 
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    # 开启子进程
    p.start()
    # 等待子进程结束
    p.join()
    print('Child process end.')
```

创建多个进程

```python
import multiprocessing
import time

def process1(interval):
	while True:
		print('process1 is runing')
		time.sleep(interval)

def process2(interval):
	while True:
		print('process2 is runing')
		time.sleep(interval)
		
if __name__ == '__main__':
	p1 = multiprocessing.Process(target = process1, args = (2,))
	p2 = multiprocessing.Process(target = process2, args = (2,))
	p1.start();
	p2.start();
	
	while True:
		for p in multiprocessing.active_children():
			print('child Process:' + p.name + '\t,id:' + str(p.id) + 'is alive')
		print('main process is running')
        time.sleep(2)
```

对进程进行命名

```python
import multiprocessing
import time


def foo():
    name = multiprocessing.current_process().name  # 获取父进程的名字
    print("Starting %s \n" % name)
    time.sleep(3)
    print("Exting %s \n" % name)
    
if __name__ == "__main__":
    process_with_name = multiprocessing.Process(name='foo_process', target=foo)  # 设定名字
    process_with_default_name = multiprocessing.Process(target=foo)
    process_with_name.start()
    process_with_default_name.start()
```

在后台运行进程

```python
import multiprocessing
import time


def foo():
    name = multiprocessing.current_process().name
    print("Starting %s \n" % name)
    time.sleep(3)
    print("Exting %s \n" % name)
    
if __name__ == "__main__":
    background_process = multiprocessing.Process(name='background_process', target=foo)
    background_process.daemon = True  # 非后台模式下的进程有输出，在主程序结束后，后台进程自动结束
    NO_background_process = multiprocessing.Process(name='NO_background_process',target=foo)
    NO_background_process.daemon = False
    background_process.start()
    NO_background_process.start()
```

杀死进程

```python
import multiprocessing
import time

def foo():
    print('Starting function')
    time.sleep(0.1)
    print('Finished function')

if __name__ == "__main__":
    p = multiprocessing.Process(target=foo)
    print('Process before execution:', p, p.is_alive())  # 监控生命周期
    p.start()
    print('Process runing:', p, p.is_alive())
    p.terminate()  # 终止进程
    print('Process terminated:', p, p.is_alive())
    p.join()
    print('Process joined:', p , p.is_alive())
    print('Process exit code: ', p.exitcode)
```

### 在子类中使用

```
定义一个新的Process类的子类
重写__init__(self [,args])方法增加额外的参数
重写run(self [,args])方法实现Process启动后需要做的事情
创建好新的Process子类后，创建一个实例，然后调用start()方法来启动，该方法会调用run()方法
```

具体实现

```python
import multiprocessing
import time

class ChildProcess(multiprocessing.Process):
	def __init__(self, interval):
        multiprocessing.Process.__init__(self)
        self.interval = interval
        
    def run(self):
        while True:
            print('ChildProcess is runing')
            time.sleep(self.interval)
            
 
if __name__ == '__main__':
    p = ChildProcess(2)
    p.start()
    while True:
        print('MainProcess is running')
        time.sleep(2)
```

## 进程池

一个线程池或进程池(也被称为池化)指的是用来优化、简化程序内部线程/进程使用的软件管理器。通过池化，可以向pooler提交将由其执行的任务。这个池子里有一个待执行任务的内部队列，以及一些执行这些任务的线程或进程。池化中的一个常见概念是复用：一个线程(或进程)在其生命周期中，被多次用于执行不同的任务。复用减少了创建进程或线程的开销，提升了利用池化技巧的程序的性能。虽然复用不是非用不可的，但它却是促使程序员在其应用中使用池化的主要原因之一。

几乎所有服务器端应用都用到了池化，因为需要处理来自任意数量客户端的大量并发请求。而还有不少其他应用要求每个任务立刻执行，或者对执行任务的线程具备更大的控制权。这种情况下，池化不是最好选择。

### Pool

如果要启动大量的子进程，可以用进程池的方式批量创建子进程

Pool对象提供了大量的方法支持并行操作

```python
apply(func[, args[, kwds]])
# 调用函数func，并传递参数args和kwds，同时阻塞当前进程直至函数返回，函数func只会在进程池中的一个工作进程中运行；一直阻塞，知道结果就绪为止。

apply_async(func[, args[, kwds[, callback[, error_callback]]]])
# apply()的变形，返回结果对象，可以通过结果对象的get()方法获取其中的结果，参数callback和error_callback都是单参数函数，当结果对象可用时会自动调用callback，该调用失败时会自动调用error_callback；异步操作，并不会锁定主线程，直到所有子类都执行完毕为止。

map(func, iterable[, chunksize])
# 内置函数map()的并行版本，但只能接收一个可迭代对象作为参数，该方法会阻塞当前进程直至结果可用。该方法会把迭代对象iterable切分成多个块再作为独立的任务提交给进程池，块的大小可以通过参数chunksize(默认1)来设置

map_async(func, iterable[, chunksize[, callback[, error_callback]]])
# 与map()方法类似，但返回结果对象，需要使用结果对象的get()方法来获取其中的值。如果指定了回调，那么它就是可调用的，且会接收一个参数。当结果就绪时，回调就会使用到它(除非调用失败了)。回调应该立即执行，否则，处理结果的线程就会被阻塞住

imap(func, iterable[, chunksize[, callback[, error_callback]]])
# map()方法的惰性求值版本，返回迭代器对象

imap_unordered(func, iterable[, chunksize])
# 与imap()方法类似，但不保证结果会按参数iterable中原来元素的先后顺序返回

starmap(func, iterable[, chunksize])
# 类似于map()方法，但要求参数iterable中的元素为得带对象并可解包为函数func的参数

starmap_async(func, iterable[, chunksize[, callback[, error_back]]])
# 方法starmap()和map_async的组合，返回结果对象

close()
# 不允许再向进程池提交任务，当所有已提交任务完成后工作进程会退出

terminate()
# 立即结束工作进程，当线程池对象被回收时会自动调用该方法

join()
# 等待工作进程退出，在此之前必须先调用close()或treminate()
```

示例1

```python
from multiprocessing import Pool
import os, time, random

def long_time_task(name):
    print('Run task %s (%s)...' % (name, os.getpid()))
    start = time.time()
    time.sleep(random.random() * 3)
    end = time.time()
    print('Task %s runs %0.2f seconds.' % (name, (end - start)))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    # 定义一个进程池，最大进程数4
    p = Pool(4)
    for i in range(5):
        # 以异步并行的方式启动进程，如果要同步等待的方式，可以在每次启动进程之后调用res.get(),也可以使用pool.apply
    	# 每次循环将会用空闲出来的子进程去调用目标
        p.apply_async(long_time_task, args=(i,))
    print('Waiting for all subprocesses done...')
    p.close()	# 关闭进程池，关闭后pool不再接收新的请求
    p.join()	# 等待pool中所有子进程执行完成，必须放在close语句之后
    print('All subprocesses done.')
```

示例2


```python
from multiprocessing import Pool
import time

def f(x):
    return x*x

if __name__ == '__main__':
  	# 使用with语句
    with Pool(processes=4) as pool:
        # 返回结果对象，可以通过get()方法获取其中的值
        result = pool.apply_async(f, (10,))
        print(result.get(timeout=1))
        # 直接返回结果列表
        print(pool.map(f, range(10)))
        # 返回迭代器对象
        it - pool.imap(f, range(10))
        print(next(it))
        print(next(it))
        pritn(it.next(timeout=1))
        #  进入睡眠状态10s
        result = pool.apply_async(time.sleep,(10,))
        # 下面的代码会引发超时异常
        print(result.get(timeout=3))
```


并发计算二维数组每行的平均值

```python
from multiprocessing import Pool
from statistics import mean

def f(x):
    return mean(x)

if __name__ == '__main__':
    x = [list(range(10)), list(range(20,30)),
         list(range(50, 60)), list(range(80, 90))]
    with Pool(5) as p:  # 创建包含5个进程的进程池
        print(p.map(f,x)) # 并发运行
```

### ProcessPoolExecutor

从`Python3.2`开始，标准库为我们提供了`concurrent.futures`模块，它提供了`ThreadPoolExecutor`和`ProcessPoolExecutor`两个类，实现了对`threading`和`multiprocessing`的进一步抽象，不仅可以帮我们自动调度线程，还可以做到：

1. 主线程可以获取某一个线程（或者任务的）的状态，以及返回值。
2. 当一个线程完成的时候，主线程能够立即知道。
3. 让多线程和多进程的编码接口一致

示例

```python
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from concurrent.futures import ProcessPoolExecutor
#多进程编程
#耗cpu的操作，用多进程编程， 对于io操作来说， 使用多线程编程，进程切换代价要高于线程

#1. 对于耗费cpu的操作，多进程由于多线程
def fib(n):
    if n<=2:
        return 1
    return fib(n-1)+fib(n-2)

if __name__ == "__main__":
    with ThreadPoolExecutor(3) as executor:
        all_task = [executor.submit(fib, (num)) for num in range(25,40)]
        start_time = time.time()
        for future in as_completed(all_task):
            data = future.result()
            print("exe result: {}".format(data))
        print("last time is: {}".format(time.time()-start_time))

#2. 对于io操作来说，多线程优于多进程
def random_sleep(n):
    time.sleep(n)
    return n

if __name__ == "__main__":
    with ProcessPoolExecutor(3) as executor:
        all_task = [executor.submit(random_sleep, (num)) for num in [2]*30]
        start_time = time.time()
        for future in as_completed(all_task):
            data = future.result()
            print("exe result: {}".format(data))

        print("last time is: {}".format(time.time()-start_time))
```

## 管理进程间的状态

### Manager

Manager对象提供了不同进程间共享数据的方式，甚至可以在网络上不同机器上运行的进程间共享数据。Manager对象控制一个拥有`list,dict,Lock,RLock,Semphore,BoundedSemaphore,Condition,Event,Barrier,Queue,Value,Array,Namespace`等对象的服务端进程，并且允许其他进程通过代理来操作这些对象

管理器拥有如下属性

```
它会控制服务端进程，该进程会管理共享对象
当有人修改共享对象时，它会确保共享对象在所有进程中都会更新
```

使用Manager对象实现进程间数据交换

```python
# 示例1
from multiprocessing import Process, Manager

def f(d, l, t):
    d['name'] = 'Li Lei'
    d['age'] = 18
    d['sex'] = 'Male'
    d['address'] = 'Yantai'
    l.reverse()
    t.value = 3

if __name__ == "__main__":
    with Manager() as manager:
        d = manager.dict()
        l = manager.list(range(10))
        t = manager.Value('i', 0)
        p = Process(target=f, args=(d,l,t))
        p.start()
        p.join()
        for item in d.items():
            print(item)
        print(1)
        print(t.value)
 
# 示例2
import multiprocessing

def worker(dictionary, key, item):
    dictionary[key] = item

if __name__ == "__main__":
    # 声明管理器
    mgr = multiprocessing.Manager()
    # 创建字典类型的数据结构
    dictionary = mgr.dict()
    jobs = [multiprocessing.Process(target=worker, args=(dictionary, i, i*2)) for i in range(10)]
    for j in jobs:
        j.start()
    for j in jobs:
        j.join()
    print('Results: ', dictionary)
```

使用Manager对象实现不同机器上的进程跨网络共享数据

```python
# 1. 编写multiprocessing_server.py,启动服务进程，创建可共享的队列对象
from multiprocessing.managers import BaseManager
from queue import Queue

q = Queue()
class QueueManager(BaseManager):
    pass
QueueManager.register('get_queue', callable=lambda:q)
m = QueueManager(address=('', 30030), authkey=b'leili')
s = m.get_server()
s.serve_forever()

# 2.编写程序文件multiprocessing_client1.py,连接服务器进程，并往共享的队列中存入一些数据
from multiprocessing.managers import BaseManager
from queue import Queue

class QueueManager(BaseManager):
    pass
QueueManager.register('get_queue')
# 假设服务器的地址是10.2.1.2
m = QueueManager(address=('10.2.1.2', 30030), authkey=b'leili')
m.connect()
q = m.get_queue()
for i in range(3):
    q.put()

# 3.编写程序文件multiprocessing_client2.py,连接服务器进程，从共享对列对象中读取数据并输出显示
from multiprocessing.managers import BaseManager
from queue import Queue


class QueueManager(BaseManager):
    pass
QueueManager.register('get_queue')
# 假设服务器的地址是10.2.1.2
m = QueueManager(address=('10.2.1.2', 30030), authkey=b'leili')
m.connect()
q = m.get_queue()
for i in range(3):
    print(q.put())
```

创建和使用自定义的Manager对象与Proxy对象

```python
from multiprocessing import freeze_support
from multiprocessing.managers import BaseManager, BaseProxy
import operator

# 普通类
class Foo:
    def f(self):
        print('you called Foo.f()')

    def g(self):
        print('you called Foo.q')

    def _h(self):
        print('you caled Foo._h()')

# 生成器
def baz():
    for i in range(10):
        yield i*i

# 生成器对象的代理类
class GeneratorProxy(BaseProxy):
    __exposed = ['__next__']
    def __iter__(self):
        return self
    
    def __next__(self):
        return self._callmethod('__next__')

# 返回operator模块的函数
def get_operator_module():
    return operator


class MyManager(BaseManager):
    pass

# 注册Foo类，默认的公开成员f()和g()可以通过代理来访问
MyManager.register('Fool', Foo)
# 注册Foo类，明确指定成员g()和_h()可以通过代理来访问
MyManager.register('Foo2', Foo, expose=('g','_h'))
# 注册生成器函数baz，指定代理类型为GeneratorProxy
MyManager.register('baz', baz, proxytype=GeneratorProxy)
# 注册函数get_operator_module()，使其可以通过代理来访问
MyManager.register('operator',get_operator_module)

def test():
    manager = MyManager()
    manager.start()
    print('-'*20)
    # 创建对象
    f1 = manager.Fool()
    # 调用对象成员
    f1.f()
    f1.g()
    # 确认对象拥有哪些可访问的成员
    assert not hasattr(f1, '_h')
    assert sorted(f1._exposed_) == sorted(['f', 'g'])
    print('-'*20)
    f2 = manager.Foo2()
    f2.g()
    f2._h()
    assert not hasattr(f2, 'f')
    assert sorted(f2._exposed_) == sorted(['g', '_h'])
    print('-'*20)
    it = manager.baz()
    for i in it:
        print('<%d>'%i, end=' ')
    print()
    print('-'*20)
    op = manager.operator()
    print('op.add(23,45)=', op.add(23, 45))
    print('op.pow(2,94)=', op.pow(2, 94))
    print('op._exposed_=', op._exposes_)


if __name__ == "__main__":
    # 支持使用py2exe,Pyinstaller和cx_Freeze打包为windows可执行程序
    freeze_support()
    test()
```

## 进程间通信

并行应用的开发需要在进程间进行数据交换。Python的`multiprocessing`模块有两个通信通道，通过它们可以管理对象的交换，分别是：Pipe管道，Queue队列

### Queue

可以通过队列数据结构来共享数据。队列会返回一个进程共享队列，它是线程与进程安全的，任何可序列对象(python使用pickable模块来序列化对象)都可以通过它进行交换

- 多进程中的queue

Queue对象的方法

```
初始化Queue()对象时（例如：q=Queue()），若括号中没有指定最大可接收的消息数量，或数量为负值，那么就代表可接受的消息数量没有上限（直到内存的尽头）
```

| 方法                                 | 说明                                                         |
| ------------------------------------ | ------------------------------------------------------------ |
| `Queue.qsize()`                      | 返回当前队列包含的消息数量                                   |
| `Queue.empty()`                      | 如果队列为空，返回True，反之False                            |
| `Queue.full()`                       | 如果队列满了，返回True,反之False                             |
| `Queue.get([block[, timeout]])`      | 获取队列中的一条消息，然后将其从列队中移除，block默认值为True；<br/>1）如果block使用默认值，且没有设置timeout（单位秒），消息列队如果为空，此时程序将被阻塞（停在读取状态），直到从消息列队读到消息为止，如果设置了timeout，则会等待timeout秒，若还没读取到任何消息，则抛出"Queue.Empty"异常；<br/>2）如果block值为False，消息列队如果为空，则会立刻抛出"Queue.Empty"异常； |
| `Queue.get_nowait()`                 | 相当Queue.get(False)                                         |
| `Queue.put(item,[block[, timeout]])` | 将item消息写入队列，block默认值为True；<br/>1）如果block使用默认值，且没有设置timeout（单位秒），消息列队如果已经没有空间可写入，此时程序将被阻塞（停在写入状态），直到从消息列队腾出空间为止，如果设置了timeout，则会等待timeout秒，若还没空间，则抛出"Queue.Full"异常；<br/>2）如果block值为False，消息列队如果没有空间可写入，则会立刻抛出"Queue.Full"异常； |
| `Queue.put_nowait(item)`             | 相当`Queue.put(item, False)`                                 |
queue读写数据

```python
# 以Queue为例，在父进程中创建两个子进程，一个往Queue里写数据，一个从Queue里读数据
from multiprocessing import Process, Queue
import os, time, random

# 写数据进程执行的代码:
def write(q):
    print('Process to write: %s' % os.getpid())
    for value in ['A', 'B', 'C']:
        print('Put %s to queue...' % value)
        q.put(value)
        time.sleep(random.random())

# 读数据进程执行的代码:
def read(q):
    print('Process to read: %s' % os.getpid())
    while True:
        value = q.get(True)
        print('Get %s from queue.' % value)

if __name__=='__main__':
    # 父进程创建Queue，并传给各个子进程：
    q = Queue()
    pw = Process(target=write, args=(q,))
    pr = Process(target=read, args=(q,))
    # 启动子进程pw，写入:
    pw.start()
    # 启动子进程pr，读取:
    pr.start()
    # 等待pw结束:
    pw.join()
    # pr进程里是死循环，无法等待其结束，只能强行终止:
    pr.terminate()
```

生产者-消费者队列(mac无法运行)

```python
import multiprocessing
import random
import time


class Producer(multiprocessing.Process):
    def __init__(self, queue):
        multiprocessing.Process.__init__(self)
        self.queue = queue

    def run(self):
        for i in range(10):
            item = random.randint(0, 256)
            self.queue.put(item)
            print('Process Producer: item %d appended to queue %s' % (item, self.name))
            time.sleep(1)
            print('The size of queue is %d' % self.queue.qsize())


class Consumer(multiprocessing.Process):
    def __init__(self, queue):
        multiprocessing.Process.__init__(self)
        self.queue = queue

    def run(self):
        while True:
            if self.queue.empty():
                print('the queue is empty')
                break
            else:
                time.sleep(2)
                item = self.queue.get()
                print('Process Consumer: item %d popped by %s \n' % (item, self.name))
                time.sleep(1)


if __name__ == '__main__':
    queue = multiprocessing.Queue()
    process_producer = Producer(queue)
    process_consumer = Consumer(queue)
    process_producer.start()
    process_consumer.start()
    process_producer.join()
    process_consumer.join()
```

- 上下文对象context的Queue

```python
import multiprocessing as mp

def foo(q):
    q.put('hello word')

if __name__ == "__main__":
    ctx = mp.get_context('spawn')
    q = ctx.Queue()
    p = ctx.Processing(target=foo, args=(q,))
    p.start()
    print(q.get())
    p.join()
```

- pool进程池中的Queue

如果要使用Pool创建进程，需要使用multiprocessing.Manager()中的Queue()，而不是multiprocessing.Queue()

```python
# 修改import中的Queue为Manager
from multiprocessing import Manager,Pool
import os,time,random

def reader(q):
    print("reader启动(%s),父进程为(%s)" % (os.getpid(), os.getppid()))
    for i in range(q.qsize()):
        print("reader从Queue获取到消息：%s" % q.get(True))

def writer(q):
    print("writer启动(%s),父进程为(%s)" % (os.getpid(), os.getppid()))
    for i in "itcast":
        q.put(i)

if __name__=="__main__":
    print("(%s) start" % os.getpid())
    q = Manager().Queue()  # 使用Manager中的Queue
    po = Pool()
    po.apply_async(writer, (q,))

    time.sleep(1)  # 先让上面的任务向Queue存入数据，然后再让下面的任务开始从中取数据

    po.apply_async(reader, (q,))
    po.close()
    po.join()
    print("(%s) End" % os.getpid())
```

### Pipe

Pipe性能高于queue，故在两个进程间进行通信时优先选择pipe

使用管道实现进程间的数据交换。管道有两个端，一个接收端和一个发送端，相当于在两个进程之间建立了一个传输数据的管道

管道返回由管道所连接的一对连接对象。每个对象都拥有send/receive方法，实现进程间通信

```python
from multiprocessing import process, Pipe

def f(conn):
    # 向管道中发送数据
    conn.send('hello world')
    # 关闭管道
    conn.close()

if __name__ == "__main__":
    # 创建管道对象
    parent_conn, child_conn = Pipe()
    # 将管道的一方作为参数传递给子进程
    p = Process(target=f, args=(child_conn,))
    p.start()
    p.join()
    # 通过管道的另一方获取数据
    print(parent_conn.recv())
    parent_conn.close()
```

示例

```python
from multiprocessing import process, Pipe

def producer(pipe):
    pipe.send('produce over')

def consumer(pipe):
    print(pipe.recv())
    
     
if __name__ == "__main__":
    # 创建管道对象
    receive_pipe, send_pipe = Pipe()
    my_producer= Process(target=producer, args=(send_pipe,))
    my_consumer= Process(target=consumer, args=(receive_pipe,))
    my_producer.start()
	my_consumer.start()
    my_producer.join()
    my_consumer.join()
```

示例

```python
import multiprocessing

def create_items(pipe):
    """生成1～9数字"""
    output_pipe, _ = pipe
    for item in range(10):
        output_pipe.send(item)
    output_pipe.close()

def multiply_items(pipe_1, pipe_2):
    """对数字进行乘方"""
    close, input_pipe = pipe_1
    close.close()
    output_pipe, _ = pipe_2
    try:
        while True:
            item = input_pipe.recv()
            output_pipe.send(item * item)
    except EOFError:
        output_pipe.close()

if __name__ == "__main__":
    # 第一个管道
    # 返回一个双向管道所连接的一对连接对象
    pipe_1 = multiprocessing.Pipe(True)
    process_pipe_1 = multiprocessing.Process(target=create_items, args=(pipe_1,))
    process_pipe_1.start()
    # 第二个管道
    pipe_2 = multiprocessing.Pipe(True)
    process_pipe_2 = multiprocessing.Process(target=multiply_items, args=(pipe_1, pipe_2,))
    process_pipe_2.start()
    pipe_1[0].close()
    pipe_2[0].close()
    try:
        while True:
            print(pipe_2[1].recv())
    except EOFError:
        print('End')
```

## 进程同步

当多个进程需要访问共享资源时，为了避免冲突multiprocessing模块提供了多种机制实现进程间同步

### 共享内存

使用共享内存实现进程间数据传递，比较适合大量数据的场合

```python
from multiprocessing import Process, Value, Array

def f(n, a):
    n.value = 3.1415926
    for i in range(len(a)):
        a[i] = a[i]*a[i]

if __name__ == "__main__":
    # 实型
    num = Value('d', 0.0)
    # 整型数组
    arr = Array('i', range(10))
    # 创建进程对象
    p = Process(target=f, args=(num, arr))
    p.start()
    p.join()
    print(num.value)
    pritn(arr[:])
```

### Lock

锁机制通过对共享资源上锁的方式避免多个进程的访问冲突

```python
import multiprocessing
import sys

def process1(lock, f):
    with lock:
        fs = open(f, 'a+')
        times = 10
        while times > 0:
            fs.write('process1 write\n')
            times -= 1
        fs.close()
        
def process2(lock, f):
    # 上锁
    lock.acquire()
    try:
        fs = open(f, 'a+')
        times = 10
        while times > 0:
            fs.write('process2 write\n')
            tiems -= 1
        fs.close()
    finally:
        // 解锁
        lock.release()
        
        
if __name__ == '__main__':
    # 创建锁
    lock = multiprocessing.Lock()
    f = 'share.txt'
    p1 = multiprocessing.Process(target = process1, args=(lock, f))
    p2 = multiprocessing.Process(target = process2, args=(lock, f))
    p1.start()
    p2.start()
    p1.join()
    p2.join()
```

### RLock

RLock是Lock的递归版。`lock.aquire()`是请求锁，当前的锁为锁定状态时，`lock.acquire()`会阻塞等待锁释放。若写了两个`lock.aquire()`会产生死锁，则第二个`lock.aquire()`会永远等待在那里

使用RLock则不会出现这种情况，RLock支持在同一资源上多个锁，上多少把锁，就得释放多少次。

### Event

实现进程间的简单通信，一个进程会发出事件，其他进程会等待事件，Event对象有两种方法：`set()`与`clear()`，用于管理内部的标志

```python
from multiprocessing import Process, Event

def f(e, i):
    if e.is_set():
        e.wait()
        print('hello word', i)
        e.clear()
    else:
        e.set()
        
        
if __name__ == '__main__':
    e = Event()
    for num in range(10):
        Process(target=f, args=(e, num)).start()
```

### Semaphore

semaphore有信号量的意思，与Lock有些类似。Semaphore可以指定允许访问资源的进程数量。通俗来讲就是，该资源有多个门，每个门对应一把锁。一个进程访问了该资源，锁了门，还有其他门可以使用。如果所有的门都被锁了，那么新的进程就必须等待现有进程推出并释放锁后才可以访问

```python
import multiprocessing
import time

def process1():
	s.acquire()
	print('process1 acquire and it will sleep 5s')
    time.sleep(5)
    print('process1 release');
    s.release()
    
def process2():
    s.acquire()
    print('process2 acquire and it will sleep 5s')
    time.sleep(5)
    print('process2 release')
    s.release()
    
def process3():
    print('process3 try to start')
    s.acquire()
    print('process3 acquire and it will sleep 5s')
    time.sleep(5)
    print('process release')
    s.release()
    
if __name__ == '__main__':
    # 限制为最多两个进程同时访问
    s = multiprocessing.Semaphore(2)
    p1 = multiprocessing.Process(target = process1)
    p2 = multiprocessing.Process(target = process2)
    p3 = multiprocessing.Process(target = process3)
    # 依次启动3个进程，当前两个进程未推出时，进程3尝试访问失败，当进程1退出后，进程3才获得权限
    p1.start()
    time.sleep(1)
    p2.start()
    time.sleep(1)
    p3.start()
    time.sleep(1)
```

### Barrier

将一个程序划分为几个阶段，因为它要求所有进程都到达后才能开始执行。屏障后的代码不能与屏障前的代码并发执行

```python
import multiprocessing
from multiprocessing import Barrier, Lock, Process
from time import time
from datetime import datetime

def test_with_barrier(synchronizer, serializer):
    name = multiprocessing.current_process().name
    synchronizer.wait()
    now = time()
    with serializer:
        print("process %s ----> %s" %(name, datetime.fromtimestamp(now)))

def test_without_barrier():
    name = multiprocessing.current_process().name
    now = time()
    print("process %s ----> %s" %(name, datetime.fromtimestamp(now)))

if __name__ == "__main__":
    # create a barrier and lock. 
    synchronizer = Barrier(2)  # 2表示管理2个进程饿
    serializer = Lock()
    # create four processes
    Process(name='p1 - test_with_barrier', target=test_with_barrier, args=(synchronizer, serializer)).start()
    Process(name='p2 - test_with_barrier', target=test_with_barrier, args=(synchronizer, serializer)).start()
    Process(name='p3 - test_without_barrier', target=test_without_barrier).start()
    Process(name='p4 - test_without_barrier', target=test_without_barrier).start()
```

## Listener/Client

这两个对象是`multiprocessing.connection`模块提供的对象，可以在不同机器上的进程之间通过网络直接传输整数、实数、字符串、列表、元组、数组等各种类型的信息

使用Listener于Client对象在不同机器上传递消息，用来验证服务端是否存活

```python
# 服务端代码
from multiprocessing.connection import Listener
from time import sleep

with Listener(('', 6060), authkey=b'leili') as listener:
    with listener.accept() as conn:
        print('connection accepted from', listener.last_accepted)
        i = 0
        while True:
            conn.send(('server is alive', i))
            i += 1
            sleep(3)
            
# 客户端代码
from multiprocessing.connection import Client

with Client(('10.2.1.2', 6060), authkey=b'leili') as conn:
    while True:
        print(conn.recv())
```

## Subprocess

很多时候，子进程并不是自身，而是一个外部进程。我们创建了子进程后，还需要控制子进程的输入和输出。

`subprocess`模块可以让我们非常方便地启动一个子进程，连接子进程的输入输出管道，并获得子进程的返回码。

创建函数

```python
run()
# 会阻塞当前进程，子进程结束后返回码和其他信息的CompletedProcess对象
call()
# 会阻塞当前进程，子进程结束后直接得到返回码
Popen()
# 不阻塞当前进程，直接返回得到Popen对象，通过该对象可以对子进程进行更多的操作和控制
# Popen对象的kill()和terminate()方法可以用来结束该进程
# send_signal()可以给子进程发送指定信号
# wait()用来等待子进程运行结束
# pid用来显示子进程的ID号
```

demo

```
# 在Python代码中运行命令nslookup www.python.org,等同命令行直接运行
import subprocess

print('$ nslookup www.python.org')
r = subprocess.call(['nslookup', 'www.python.org'])
print('Exit code:', r)
```

如果子进程还需要输入，则可以通过`communicate()`方法输入：

```python
# 等同命令行输入set q=mx  python.org  exit
import subprocess

print('$ nslookup')
p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
output, err = p.communicate(b'set q=mx\npython.org\nexit\n')
print(output.decode('utf-8'))
print('Exit code:', p.returncode)
```

## 分布式进程

```
在Thread和Process中，应当优选Process，因为Process更稳定，而且，Process可以分布到多台机器上，而Thread最多只能分布到同一台机器的多个CPU上。

Python的multiprocessing模块不但支持多进程，其中managers子模块还支持把多进程分布到多台机器上。一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信。由于managers模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。
```

如果我们已经有一个通过`Queue`通信的多进程程序在同一台机器上运行，现在，由于处理任务的进程任务繁重，希望把发送任务的进程和处理任务的进程分布到两台机器上。怎么用分布式进程实现？

原有的`Queue`可以继续使用，但是，通过`managers`模块把`Queue`通过网络暴露出去，就可以让其他机器的进程访问`Queue`了。

1. 我们先看服务进程，服务进程负责启动`Queue`，把`Queue`注册到网络上，然后往`Queue`里面写入任务：

```python
# task_master.py

import random, time, queue
from multiprocessing.managers import BaseManager

# 发送任务的队列:
task_queue = queue.Queue()
# 接收结果的队列:
result_queue = queue.Queue()

# 从BaseManager继承的QueueManager:
class QueueManager(BaseManager):
    pass

# 把两个Queue都注册到网络上, callable参数关联了Queue对象:
QueueManager.register('get_task_queue', callable=lambda: task_queue)
QueueManager.register('get_result_queue', callable=lambda: result_queue)
# 绑定端口5000, 设置验证码'abc':
manager = QueueManager(address=('', 5000), authkey=b'abc')
# 启动Queue:
manager.start()
# 获得通过网络访问的Queue对象:
task = manager.get_task_queue()
result = manager.get_result_queue()
# 放几个任务进去:
for i in range(10):
    n = random.randint(0, 10000)
    print('Put task %d...' % n)
    task.put(n)
# 从result队列读取结果:
print('Try get results...')
for i in range(10):
    r = result.get(timeout=10)
    print('Result: %s' % r)
# 关闭:
manager.shutdown()
print('master exit.')
```

请注意，当我们在一台机器上写多进程程序时，创建的`Queue`可以直接拿来用，但是，在分布式多进程环境下，添加任务到`Queue`不可以直接对原始的`task_queue`进行操作，那样就绕过了`QueueManager`的封装，必须通过`manager.get_task_queue()`获得的`Queue`接口添加。

然后，在另一台机器上启动任务进程（本机上启动也可以）：

```python
# task_worker.py

import time, sys, queue
from multiprocessing.managers import BaseManager

# 创建类似的QueueManager:
class QueueManager(BaseManager):
    pass

# 由于这个QueueManager只从网络上获取Queue，所以注册时只提供名字:
QueueManager.register('get_task_queue')
QueueManager.register('get_result_queue')

# 连接到服务器，也就是运行task_master.py的机器:
server_addr = '127.0.0.1'
print('Connect to server %s...' % server_addr)
# 端口和验证码注意保持与task_master.py设置的完全一致:
m = QueueManager(address=(server_addr, 5000), authkey=b'abc')
# 从网络连接:
m.connect()
# 获取Queue的对象:
task = m.get_task_queue()
result = m.get_result_queue()
# 从task队列取任务,并把结果写入result队列:
for i in range(10):
    try:
        n = task.get(timeout=1)
        print('run task %d * %d...' % (n, n))
        r = '%d * %d = %d' % (n, n, n*n)
        time.sleep(1)
        result.put(r)
    except Queue.Empty:
        print('task queue is empty.')
# 处理结束:
print('worker exit.')
```

任务进程要通过网络连接到服务进程，所以要指定服务进程的IP。

现在，可以试试分布式进程的工作效果了。先启动`task_master.py`服务进程：

```
python3 task_master.py
```

`task_master.py`进程发送完任务后，开始等待`result`队列的结果。现在启动`task_worker.py`进程：

```
python3 task_worker.py
```

`task_worker.py`进程结束，在`task_master.py`进程中会继续打印出结果

**注意：**

Queue对象存储在`task_master.py`进程中，而`Queue`之所以能通过网络访问，就是通过`QueueManager`实现的。由于`QueueManager`管理的不止一个`Queue`，所以，要给每个`Queue`的网络调用接口起个名字，比如`get_task_queue`。

`authkey`是为了保证两台机器正常通信，不被其他机器恶意干扰。如果`task_worker.py`的`authkey`和`task_master.py`的`authkey`不一致，肯定连接不上

Queue的作用是用来传递任务和接收结果，每个任务的描述数据量要尽量小。比如发送一个处理日志文件的任务，就不要发送几百兆的日志文件本身，而是发送日志文件存放的完整路径，由Worker进程再去共享的磁盘上读取文件

## 多进程版文件复制器

```python
import multiprocessing
import os
import time
import random


def copy_file(queue, file_name,source_folder_name,  dest_folder_name):
    """copy文件到指定的路径"""
    f_read = open(source_folder_name + "/" + file_name, "rb")
    f_write = open(dest_folder_name + "/" + file_name, "wb")
    while True:
        time.sleep(random.random())
        content = f_read.read(1024)
        if content:
            f_write.write(content)
        else:
            break
    f_read.close()
    f_write.close()

    # 发送已经拷贝完毕的文件名字
    queue.put(file_name)


def main():
    # 获取要复制的文件夹
    source_folder_name = input("请输入要复制文件夹名字:")

    # 整理目标文件夹
    dest_folder_name = source_folder_name + "[副本]"

    # 创建目标文件夹
    try:
        os.mkdir(dest_folder_name)
    except:
        pass  # 如果文件夹已经存在，那么创建会失败

    # 获取这个文件夹中所有的普通文件名
    file_names = os.listdir(source_folder_name)

    # 创建Queue
    queue = multiprocessing.Manager().Queue()

    # 创建进程池
    pool = multiprocessing.Pool(3)

    for file_name in file_names:
        # 向进程池中添加任务
        pool.apply_async(copy_file, args=(queue, file_name, source_folder_name, dest_folder_name))

    # 主进程显示进度
    pool.close()

    all_file_num = len(file_names)
    while True:
        file_name = queue.get()
        if file_name in file_names:
            file_names.remove(file_name)

        copy_rate = (all_file_num-len(file_names))*100/all_file_num
        print("\r%.2f...(%s)" % (copy_rate, file_name) + " "*50, end="")
        if copy_rate >= 100:
            break
    print()


if __name__ == "__main__":
    main()
```

