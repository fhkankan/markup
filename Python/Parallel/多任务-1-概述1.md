[TOC]

# 并行概述

## 并行计算内存架构

根据可同时处理的指令数量与数据量的不同，计算机系统可分为如下四类

这种分类方法也叫做费林分类(Flynn's taxonomy)

```
单指令，单数据	SISD
单指令，多数据	SIMD
多指令，单数据	MISD
多指令，多数据	MIMD
```

- SISD

SISD计算系统是一个单处理器机器，它执行的每个指令都会操作单个数据流，在其系统中，机器指令是顺序处理的

在一个时钟周期中，CPU会执行如下操作：

```
获取：CPU从内存区域获取数据与指令，该内存区域叫做寄存器

解码：CPU对指令进行解码

执行：指令在数据上得到执行，操作结果会被存储到另一个寄存器中
```

当执行阶段完成后，CPU会对自身进行设置，从而开始另一个CPU周期。

在这类计算机中执行的算法是顺序的(串行)，因为他们并不包含任何并行。只拥有单个CPU的硬件系统就是SISD计算机

构成这种架构(冯诺伊曼架构)的主要元素有如下几项

```
中央存储单元	用于存储指令与程序数据
CPU			用于从存储单元中获取指令与数据，它会对指令进行解码并顺序地执行它们
I/O			指程序的输入输出数据
```

传统的单处理器计算机被归类为SISD系统

- MISD

该模型中的n个处理器，每一个都有自己的控制单元，它们共享同一个存储单元。在每个时钟周期内，从内存接收到的数据会被所有处理器同时处理，每个处理器都会按照从其控制单元中所接收到的指令顺序进行处理。在这种情况下，通过对相同的数据执行几个操作实现了并行(指令级并行)。这种架构能有效解决的问题相当特殊的，比如说与数据加密相关的问题等。出于这个原因，计算机MISD并未在商业上流行起来，其更多地是用在智力训练而非实际使用

- SIMD

SIMD计算机包含n个相同的处理器，每个处理器都有自己的本地内存，可以用于存储数据，所有处理器都处于单个指令流的控制之下；此外，还有n个数据流，分别针对每个处理器。在每个步骤中，处理器都会同时工作并执行相同的指令，不过是在不同的数据元素上执行。这是一种数据并行。SIMD架构要比MISD架构更加通用。大量应用所所涉及的诸多问题都可以通过SIMD计算机中的并行算法来解决。另一个有趣的特性是这些计算机所用的算法的设计、分析和实现相对容易。局限在于只有那些可以被分解为一系列子问题的问题(这些子问题要完全一样，每个子问题后面会通过相同的指令集同时解决)才能通过SIMD计算机来解决。对于根据该范式所开发出的超级计算机来说，必须提到的是Connection Machine与MPP。大量SIMD嵌入式单元的现代图形处理器单元(GPU)的出现使得这种计算范式得到更为广泛的使用。

- MIMD

根据费林的分类，这种并行计算机最为通用，也是更为强大的一种。它有n个处理器、n个指令流以及n个数据流。每个处理器都有自己的控制单元与本地内存，从计算角度来说，这使得MIMD架构要比SIMD更为强大。每个处理器都是在它自己的控制单元所发出的指令流的控制下来进行操作的；因此，处理器可以对不同的数据执行不同的程序，可以将一个大问题分解为多个不同的子问题，然后加以解决。MIMD架构师通过线程与进程级别的并行的帮助来实现的。这还意味着，处理器通常会异步执行。这类计算机用于解决那些拥有不规则结构的问题，而SIMD则要求问题的结构要规则才行。时至今日，这种架构已经用于到了很多PC、超级计算机以及计算机网络中。要注意的是，异步算法的设计、分析与实现不是那么容易的事情

## 内存组织

在评估并行架构时需要考虑的另一个防霾呢就是内存架构，也称为数据的访问方式。

无论处理单元速度有多快，若内存无法以足够的速度来维护并提供指令和数据，那么在性能上也不会有什么改进。想让内存的响应时间跟上处理器的速度，需要解决存储周期时间，它指的是连续两个操作之间所经过的时间。处理器的周期时间通常要比内存的周期时间短。当处理器开始传输数据时(向内存传输或是从内存获取)，内存将会在整个存储周期内被占用；在这期间，其他设备(I/O控制器、处理器、甚至是发出该请求的处理器自身)都无法使用内存，因为它要对请求作出响应。

对内存访问问题的解决方案有两个：共享内存系统和分布式内存系统。在共享内存系统中，有一个高层的虚拟内存，所有处理器都可以访问该内存中的数据与指令。在分布式内存系统中，则使用了分布式内存模型，其中每个处理器都有自己的本地内存，其他处理器是无法访问的。共享内存和分布式内存之间的差别在于虚拟内存或是从处理器的视角所看到的内存结构。从物理上来说，几乎每个系统内存都会被划分为不同的组建，它们之间的访问是独立的。共享内存与分布式内存的区别在与处理器单元的内存访问管理方式。

- 共享内存

共享内存的物理连接是相当简单的，总线结构可以支持共享相同通道的任意数量的设备。总线协议最初被设计为支持单个处理器，一个或多个磁盘一斤磁带处理器可以通过这里的共享内存进行通信。注意，每个处理器都会有一个与之相关联的高速缓存，因为很多时候处理器都需要本地内存中的数据或是指令。当一个处理器修改了同时被其他处理器所用的内存系统中的数据时就会产生问题。新值需要从处理器缓存中传递过来，而处理器缓存中的值已经被修改到了共享内存中；不过，接下来它需要传递给所有其他的处理器，这样老的值就无法正常使用了。这个问题叫做缓存一致性问题，这是内存一致性的一个特例，需要硬件实现来处理并发问题与同步，类似于线程编程一样。

共享内存系统的主要特点如下

```
- 内存对于所有处理器来说都是一样的，比如，与相同数据结构相关联的所有处理器都会使用同样的逻辑内存地址，这样就会访问到相同的内存位置

- 同步是通过控制处理器对共享内存的访问实现的。实际上，在某一时刻只有一个处理器能够访问到内存资源

- 当一个任务在访问共享内存时，另一个任务是不可以修改共享内存的位置的

- 数据共享是非常快的，两个任务间通信所需的时间等于读取单个内存位置的时间(取决于内存访问的速度)
```

共享内存系统中的内存访问如下

```
- 统一内存访问(UMA)
该系统的基本特点是，对于每个处理器以及内存的任何区域来说，对内存的访问时间都是恒定的。出于这个原因，这些系统又叫做对称多处理器(SMP)。这些系统相对来说比较容易实现，不过可伸缩型并不好，程序员需要负责同步的管理，这是通过在管理资源的程序中插入恰当的控制、信号量以及锁来实现

- 非统一内存访问(NUMA)
该架构将内存区域划分为高速访问区域(分配给每个处理器，是数据交换的公共区域)以及低速访问区域两种。这些系统又叫做分布式共享内存系统(DSM)。其可伸缩型非常好，不过开发起来难度颇大

- 无远程内存访问(NORMA)
内存在物理上被分配给各个处理器(本地内存)。所有的本地内存都是私有的，只能为本地处理器所访问。处理器之间的通信是通过用于消息交换的通信协议来实现的，叫做消息传递协议

- 仅缓存访问(COMA)
这些系统只有缓存。在分析NUMA架构时，其架构将数据的副本保存到缓存中，并且这些数据在主内存中还会有一个副本存在。该架构去处了主内存中的副本，只保留缓存，内存在物理上被分配给了各个处理器(本地内存)。所有的本地内存都是私有的，只能为本地处理器访问。处理器之间的通信是通过用于消息交换的通信协议来实现的，叫做消息传递协议
```

- 分布式内存

在分布式内存系统中，内存每个处理器关联在了一起，一个处理器只能访问到它自己的内存。也叫做“多计算机系统”。反映了系统元素本身是小型且完备的处理器于内存系统

这种组织方式优点：

```
1. 在通信总线或是开关层面上不会再出现冲突。每个处理器都可以使用其自己本地内存的全部宽带而不会妨碍其他处理器。
2. 无公共总线意味着对于处理器的数量不会再有固有的限制，系统的大小只受限于连接的处理器的网络
3. 不存在缓存一致性的问题。每个处理器负责管理自己的数据，不必再考虑副本的更新问题
```

缺点

```
主要的缺点是处理器之间的通信更加难以实现。如果一个处理器需哟啊另一个处理器内存中的数据，那么这两个处理器就需哟啊通过消息传递协议来交换消息。这会引入两个速度降低之源：一个处理器构建消息并向另一个处理器发送消息需要时间；另外，处理器还得停下来管理从其他处理器所接收到的消息。

针对分布式内存机器所设计的程序需要组织为一组独立的任务，这些任务之间通过消息进行通信
```

主要特点

```
- 物理上，内存在处理器之间是分布式的，每个本地内存都只会被其处理器所直接访问
- 同步是通过在处理器将(通信)移动数据(即便只是消息本身亦如此)来实现的
- 本地内存中数据的分割会影响机器的性能---划分的精确性非常重要，因为这样会将CPU之间的通信降到最低。除此之外，用于协调这些分解与组合操作的处理器必须能与对数据结构的每一部分进行操作的处理器高效通信
- 使用消息传递协议，这样CPU就可以通过数据包的交换来彼此通信。消息是信息的离散单元；它们拥有定义明确的身份，这样就可以对其进行区分了
```

> 大规模并行处理

MPP机器由成百上千个处理器组成(在一些机器上的规模可以达到成千上万)，这些处理器之间通过通信网络进行连接。世界上最开的计算机就是基于这样的架构的

> 工作站集群

这些处理系统基于传统计算机，它们之间通过通信网络进行连接。计算集群就属于这类。

在集群架构中，将节点定义为集群中的单个计算单元。对于用户来说，集群是完全透明的。所有硬件与软件的复杂型都被隐藏起来，在访问数据与应用时就好像它们都来自单个节点一样

```
- 容错集群
在该类集群中，节点的活动会被持续监控。当一个节点停止工作时，另一台机器会接管它的活动，其目标旨在通过架构的冗余来确保持续的服务

- 负载均衡集群
在该系统中，工作请求会被发送给活动较少的接待你。这确保了完成整个过程所需的时间会更少一些

- 高性能计算集群
在该系统中，每个接待你都会被配置以提供非常高的性能。整个过程依然会被划分为在多个节点上执行的多个任务，任务时并行化的，并且分布在不同的机器上。

```

> 异构架构

超级计算机中同构世界中引入的GPU加速器改变了之前超级计算机使用与编程方式的本质。尽管GPU提供了很高的性能，不过它们无法作为一种自洽的处理单元，因为它们总是要与CPU协同使用才行。因此，编程范式非常简单：CPU以串行方式进行控制与计算，将计算代价高昂并且需要很高并行度的任务分配给图形加速器。CPU与GPU之间的通信不仅可以通过高速总线来实现，还可以通过针对物理或是虚拟的单个内存区域的共享来实现。实际上，如果两台设备都没有自己的内存区域，那么可以通过各种编程模型所提供的软件库来访问共享内存区域，如CUDA和OpenCL。这种架构叫做异构架构。其中应用可以在单个地址空间中创建数据结构，并将任务发送给适合于其解析的设备硬件。多个处理任务可以在相同区域中安全地操作以避免数据一致性问题，这要归功于原子操作。因此，虽然CPU和GPU之间的协同效率不高，但借助这种新型架构，可以优化它们之间的交互以及并行应用的性能

## 并行编程模型

并行编程模型是硬件与内存架构的一种抽象。实际上，这些模型并非特定的，也没有指代特定类型的机器或是内存架构。它们可以在任何类型的机器上实现。相比于之前的划分来说，这些编程模型位于更高的层级上，表示软件必须被实现为执行一种并行计算的方式。每种模型都有与其他处理器共享信息的方式，从而访问内存并划分任务

- 共享内存模型

在该模型中，任务共享单个共享的内存区域，对共享资源的访问(读写数据)是异步的。有机制可以让程序员控制对共享内存的访问，比如锁或信号量。该模型的优势在于程序员不必清楚任务间的通信。从性能角度来说，该模型的一个严重缺点是使理解与管理数据的局部性变得更为困难；让数据成为使用它的处理器的局部数据可以减少内存访问、缓存刷新，以及多个处理器使用相同数据时所产生的总线流量

- 多线程模型

在该模型中，一个进程可以拥有多个执行流，比如，先常见一个顺序部分，随后创建一个系列任务并行执行。通常，这种模型会用在共享内存架构中。因此，管理线程间的同步非常重要，因为这些线程会操作共享内存，程序员必须防止多个线程同时更新相同的位置。现代的CPU在软件和硬件上都是多线程的。Posix线程就是软件多线程实现的经典例子。英特尔的超线程技术实现了硬件的多线程，如果一个线程停下来或是等待I/O操作，那么它会切换至另外一个线程。即便数据对齐是非线形的，我们也可以通过该模型实现并行

- 消息传递模型

消息传递模型通常用在每个处理器都有自己的内存(分布式内存系统)的场景下。更多的任务可以驻留在同一台物理机或是任意数量的机器上。程序员负责决定并行以及通过消息所进行的数据交换。该并行模型的实现需要在代码中用到特殊的软件库。目前已经有了大量的消息传递模型的实现。MPI(消息传递接口)模型的设计使用了分布式内存，但却成为并行编程的模型，多个平台也可以使用共享内存机器。

- 数据并行模型

在该模型中，多个任务可以操作相同的数据结构，不过每个任务都只会操作数据的不同部分。在共享内存架构中，所有的任务都可以通过共享内存与分布式内存架构来访问数据，其中的数据结构会被分割并驻留在每个任务的本次内存中。为了实现该模型，程序员需要开发一个程序来指定数据的分布与对齐方式。现代GPU在数据对齐的情况下处理速度非常快。

## 如何设计并行程序

- 任务分解

在该阶段，软件程序会被分解为任务或是一组指令，接下来在不同的处理器上执行以实现并行化。为了完成这个分解，可以使用如下方法

```
- 领域分解
将问题数据进行分解；应用对于使用不同部分数据的所有处理器来说是公共的。当待处理的数据量很大时通常会使用该方法

- 功能性分解
在这种情况下，问题被分解为任务，每个任务都会在所有可用数据上执行特定的操作

```

- 任务分配

在这个步骤中，指定好将任务分发到各个处理器上的机制。这个阶段非常重要，因为它会在各个处理器上建立工作负载的分配机制。在这里，负载均衡尤为重要；实际上，所有处理器必须要不间断地工作，从而避免较长时间的闲置状态。为了做到这一点，程序员需要考虑到系统之间可能存在的异构性，从而在性能更好的处理器上分配更多的任务。最后，想获得更改的并行效率，要尽可能地闲置处理器之间的通信，因为这常常是速度变慢以及资源消耗之源

- 聚集

聚集指的是将小任务与大任务合并到一起从而改进性能的过程。如果设计过程的前两个阶段将问题划分为一系列任务，但任务数量远远超过可用的处理器数量，同时计算机并未针对处理大量的小任务而进行特别的设计(有些架构如GPU则可以很好地解决这个问题，实际上还会从运行数以百万甚至数以亿计的任务中获益)，那么设计就是极其低效的。一般来说，这是因为任务需要与处理器或是线程进行通信，这样它们才能计算任务。大多数通信的代价不仅与所传输的数据量有关，每次通信操作也有固定的代价(如建立TCP连接时固有的延迟)。如果任务过小，那么这种固定的代价就很容易使设计变得毫无效率可言

- 映射

在并行算法设计过程的映射阶段，指定哪个任务将要执行。其目标是将总执行时间将到最低。这里必须做出权衡，因为两个主要的策略之间经常会彼此冲突

```
- 通信频繁的任务应该被放到同一个处理器中来增加局部性
- 可以并发执行的任务应该被方法哦不同的处理器中以增强并发性 

```

这就是映射问题，即NP完备。这样，一般来说，并不存在针对问题的多项式时间解决方案。对于相同大小的任务以及很容易确定通信模式的任务来说，映射是很直接的(还可以执行聚集以将映射到相同处理器的任务合并起来)。不过，如果任务的通信模式很难预测或事每个任务的工作量大小千差万别，那么就很难设计出一种高效的映射与聚集模式了，对于这类问题，可以通过负载均衡算法在运行期间确定聚集与映射策略。最难的是那种在程序执行期间通信量或事任务量发生变化的情况。对于这类问题，可以使用动态的负载均衡算法，它会在执行期间周期性地运行

> 动态映射

 不同的问题存在着多种负载均衡算法，有全局的，也有局部的。全局算法需要对待执行的计算有全局的掌控，通常会增加大量的成本。局部算法只以来于特定于任务本身的信息，相对于全局算法来说，这降低了成本，不过在寻找最优的聚集与映射时情况会变得更糟。不过，虽然映射本身效果更差，但降低的成本却会减少执行时间。如果除了执行开始或结束外任务之间很少通信，就可使用任务调度算法来简化处理器空闲时将任务映射到其上的操作。在任务调度算法中会维护一个任务池，任务会被放到这个池中，并由执行者取出

该模型中存在3种常见的方式

```
- 管理者/执行者
这是一种基本的动态映射模式，所有的使用者都会连接到中心化的管理者。管理者会不断向使用者发送任务并收集结果。这种策略对于数量较少的处理器来说可能是最合适的。可以通过提前获取任务使得通信与计算之间彼此重叠来改进该策略

- 层次化的管理者/执行者
这是管理者/执行者的一个变种，它有一个半分布式的层次；执行者被划分到组里，每个都与自己的管理者相关联。这些组管理者会与中央管理者进行通信(组管理者之间也可以通信)，同时执行者会从组管理者那里请求任务。这会将负载分配到几个管理者上，这样如果所有执行者都从同一个管理者请求任务，那么它就可以处理更多数量的处理器了

- 去中心化
在这种模式下，一切都是去中心化的。每个处理器会维护自己的任务池，并与其他处理器通信来请求任务。一个处理器选择其他处理器来请求任务的方式是不同的，这是根据问题来决定的

```

## 如何评估并行程序的性能

并行计算的关注点在于在相对较短的时间内解决大规模问题。影响该目标的因素有所用的硬件类型、问题的并行度以及所用的并行编程模型等。为了实现这一点，引入基本的概念分析，比较从原始序列所获得的并行算法。其性能是通过分析与量化所用的线程数与进程数来达成。

- 加速

加速是一种度量方式，用于展示以并行方式解决问题所带来的好处。它的定义是，在单个处理元素上解决一个问题所花费的时间$T_s$除以在p个相同的处理元素上解决同样问题所花费的时间$T_p$

用$S= T_s/T_p$来表示加速。如果$S=p$，那么加速就是线性的，这表示如果处理器数量增加，那么执行速度也会增加。当然，这是理想情况。当$T_s$是最佳的串行算法执行时间时，加速时绝对的；当$T_s$是单个处理器上并行算法的执行时间时，加速是相对的

条件如下

```
S = p	是线性的或理想状况下的加速
S < p	是真实的加速
S > p	是超线性加速

```

- 效率

在理想世界中，拥有p个处理元素的并行系统的加速等于p。通常情况下，一些时间会浪费在空闲或是通信上。效率是一个性能度量，它会估算相比于在通信与同步上所浪费的时间来说，处理器在解决一个任务时的利用率

用$E=S/p=T_s/(pT_p)$表示效率。线性加速算法的E值为1；在其他情况下，E值要小于1。如下列出了E值的3种情况

```
E = 1	线性的
E < 1	真实情况的
E << 1	一个并行效率很低的问题

```

- 可伸缩性

可伸缩性指的是并行机器的效能。是计算能力(并行速度)除以处理器数量的值。如果问题规模变大，同时处理器数量也随之增加，那么性能是不会有损耗的。通过增加不同的因子，可伸缩性系统会保持相同的性能或是改进性能。

- 阿姆达尔定律

阿姆达尔定律是用于设计处理器与并行算法的广泛使用的定律。它阐释了可获取的最大的加速是由程序中的串行组件所决定的$S=\frac{1}{1-P}$，其中，`1-P`表示程序中的串行组件(非并行部分)。这意味着如果一个程序中90%的代码是可并行的，但有10%的代码要保持串行，那么可获取的最大的加速度就是9，即使对于无限数量的处理器来说亦是如此

- 古斯塔夫定律

古斯塔夫定律基于如下假设

```
在增加问题维度时，其串行部分保持不变
在增加处理器数量时，每个处理器所处理的工作保持不变

```

这表明$S(P)=P-\alpha(P-1)$，其中P是处理器数量，S是加速，$\alpha$是任何并行进程中的非并行部分。它与阿姆达尔定律相反，阿姆达尔定律认为单个进程的执行时间是固定的，并且每个进程的并行执行时间会减少。因此，阿姆达尔定律基于固定问题规模这样一个假设；它假设一个问题的全部工作量并不会随着机器规模(即处理器数量)的变化而变化。古斯塔夫定律着重解决了阿姆达尔定律的缺陷，后者并未将解决任务时所涉及的全部计算资源量考虑在内。它表明，设定并行问题解决方案时间的最佳方式是将所有计算资源都考虑进来，基于这一点，它解决了阿姆达尔定律的问题

# 并发

- 概述

目前大多数编程语言直接支持并发，且其标准库还提供了一些封装程度较高的功能。并发可以用多种方式实现，这些方式之间最重要的区别在于如何访问"共享数据"(shared data)：是通过“共享内存”(shared memory)等方式直接访问，还是通过“进程间通信”(Inter-Process Communication，IPC)等方式间接访问。

基于线程的并发是指同一个系统进程里有各自独立的若干线程，它们都在并发执行任务，这些线程一般会依序访问共享内存，以此实现数据共享。程序员通常采用某种“锁定机制”来确保同一时间只有一个线程能够刚问数据

基于进程的并发是指多个进程独立地执行任务。这些进程一般通过IPC来访问共享数据，如果编程语言或程序库支持，也可以通过共享内存来实现数据共享

另外，有一种并发，它基于"并发等待"(concurrent waiting)，而不是"并发执行"(concurrent execution)，这种凡事通常用来实现异步I/O

- 选择

刚开始尽量按照非并发的方式来写程序。因为这种程序比并发程序简单，写起来快，而且测试起来也比较容易。等写好非并发的程序后，再看其运行速度是否足够快。如果不快，那么再编写并发版本，然后容运行结果及运行效率两方面来比较二者的优劣。

对于并发的类型，推荐采用多进程来实现计算密集型任务，而对于I/O密集型任务来说，多进程和多线程都可以。并发程序的好坏不仅与并发的类型有关，而且与并发的级别有关

可以把并发的级别分为三种：

```
低级并发(low-level concurrency)
直接用“原子操作”(atomic operation)所实现的并发。这种并发是给程序库的编写者用的，而应用程序的开发者则不需要它，因为这种写法容易出错，且极难调试。虽说python本身的并发机制一般是用底层操作实现出来的，但开发者不能用python语言编写这种级别的并发代码

中级并发(mid-level concurrency)
不直接使用原子操作，但会直接使用锁(lock)，大多语言提供的都是这种级别的并发。pythong的threading.Semaphore,threading.Lock及multiprocessing.Lock等类都支持中级并发。开发应用程序的人一般会使用中级并发，因为它们只能使用这个级别

高级并发(high-level concurrency)
既不直接使用原子操作，也不直接使用锁。从3.2开始，python提供了支持高级并发的concurrent.future模块，此外，queue.Queue及multiprocessing这两个"队列集合类"也支持高级并发。
```

# 多任务

考虑有这样一个程序，需要监听一个套接字的连接，并处理收到的连接。有以下三种解决方案

- 每次有新连接建立时就创建(fork)一个新进程，需要用到multiprocessing
- 每次有新连接建立时创建一个新线程，需要用threading
- 将这个新连接加入事件循环(event loop)中，并在事件发生时对其作出响应

## 进程/线程

- 概述

```
进程：程序的一次执行（程序装载入内存、系统分配资源运行）
每个进程都有自己的地址空间、内存、数据栈以及记录运行轨迹的辅助数据等；
因为各个进程有自己的内存空间、数据栈等，所有只能使用进程间通信(IPC)，而不能直接共享信息；
进程可以通过fork和spawn操作完成其他任务；
操作系统管理运行的所有进程，并为这些进程公平分配时间。

线程：所有线程运行在同一个进程中，共享相同的运行环境
每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。
线程有一个自己的指令指针，记录运行到什么地方。
线程的运行可能被抢占(中断)或暂时被挂起（睡眠），从而让其他线程运行(让步)。
一个进程中的各个线程之间共享一片数据空间，所以线程比进程更方便进行数据共享和通信。

线程是最小的执行单元，线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源.

对于操作系统来说，一个任务就是一个进程（Process)
在一个进程内部，同时运行多个“子任务”,进程内的这些“子任务”称为线程（Thread）

一个程序至少有一个进程,一个进程至少有一个线程.如何调度进程和线程，完全由操作系统决定，程序自己不能决定什么时候执行，执行多长时间。

多任务的实现有3种方式：
多进程模式；
多线程模式；
多进程+多线程模式

要实现多任务，通常我们会设计Master-Worker模式，Master负责分配任务，Worker负责执行任务，因此，多任务环境下，通常是一个Master，多个Worker。
如果用多进程实现Master-Worker，主进程就是Master，其他进程就是Worker。
如果用多线程实现Master-Worker，主线程就是Master，其他线程就是Worker。
```

- GIL

```
GIL：全局解释器锁。并不是python的特性，是在实现python解析器(CPython)时引入的概念

GIL是一个全局排他锁，每个线程在执行的过程都需要先获取GIL，保证同一时刻只有一个线程可以执行代码。

线程释放GIL锁的情况：
在python2中，GIL释放逻辑是当前线程遇见IO操作或者ticks计数达到100(ticks可视为python本身的一个计数器，专门用于GIL，每次释放后归零，这个计数可以通过sys.setcheckinterval来调整)时进行释放。
在python3中，GIL不使用ticks计数，改为使用计时器计数(执行时间达到阈值后，当前线程释放GIL)。

在多线程环境中，python虚拟机按以下方式执行：
1.设置GIL
2.切换到一个线程执行
3.运行指定数量的字节码指令或线程主动让出控制(可电泳time.sleep(0))
4.把线程设置为睡眠状态
5.解锁GIL
6.再次重复以上所有步骤

Python使用多进程是可以利用多核的CPU资源的。多进程中每个进程都有自己独立的GIL，从而避免出现进程间的GIL争抢。

在处理像科学计算 这类需要持续使用cpu的任务的时候，由于GIL锁的争抢，单线程会比多线程快
在处理像IO操作等可能引起阻塞的这类任务的时候，因为遇到IO阻塞会自动释放GIL锁，所以多线程会比单线程快
```

- 优缺点

```
多进程模式优点
稳定性高，因为一个子进程崩溃了，不会影响主进程和其他子进程。（当然主进程挂了所有进程就全挂了，但是Master进程只负责分配任务，挂掉的概率低）著名的Apache最早就是采用多进程模式。

多进程模式缺点
创建进程的代价大，在Unix/Linux系统下，用fork调用还行，在Windows下创建进程开销巨大。另外，操作系统能同时运行的进程数也是有限的，在内存和CPU的限制下，如果有几千个进程同时运行，操作系统连调度都会成问题。

多线程模式通常比多进程快一点
多线程模式致命的缺点
任何一个线程挂掉都可能直接造成整个进程崩溃，因为所有线程共享进程的内存。
由于数据访问顺序不一样，有可能导致手结果不一致的问题，形成竞态条件，故大多数线程库带有一系列同步原语，用于控制线程的执行和数据访问。

操作系统在切换进程或者线程时，它需要先保存当前执行的现场环境（CPU寄存器状态、内存页等），然后，把新任务的执行环境准备好（恢复上次的寄存器状态，切换内存页等），才能开始执行。这个切换过程虽然很快，但是也需要耗费时间。如果有几千个任务同时进行，操作系统可能就主要忙着切换任务，根本没有多少时间去执行任务了，这种情况最常见的就是硬盘狂响，点窗口无反应，系统处于假死状态。

在Windows下，多线程的效率比多进程要高，所以微软的IIS服务器默认采用多线程模式。由于多线程存在稳定性的问题，IIS的稳定性就不如Apache。为了缓解这个问题，IIS和Apache现在又有多进程+多线程的混合模式
```

## IO密集型/CPU密集型

```
CPU密集型
音视频数据编解码、复杂计算属于CPU密集型应用场景。在这种情况下，由于计算工作多，消耗CPU资源，因此不管是ticks计数还是定时器很快就会达到阈值，然后触发GIL的释放与再竞争。因为python多线程无法利用CPU多核，所以多线程不合适，多进程时合适的，每个进程都是独立地，它们都有自己的python解释器实例，不会争夺GIL。要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。

计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。一种方法是改用Cython，使用python写法，编译成c

IO密集型
网络操作、磁盘IO的任务都是IO密集型任务。在单线程下，当有IO操作时会进行IO等待，造成不必要的时间浪费，而开启多线程能在一个线程等待IO时切换到另一个线程，可以不浪费CPU资源，从而提升程序的执行效率，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，故python的多线程是合适的，但也有一个限度。当然，多进程也是可以的。

IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。
```

## 阻塞/非阻塞

- 阻塞

程序未得到所需计算资源时被挂起的状态。

程序在等待某个操作完成期间，自身无法继续干别的事情，则称该程序在该操作上是阻塞的。

常见的阻塞形式有：网络I/O阻塞、磁盘I/O阻塞、用户输入阻塞等。

阻塞是无处不在的，包括CPU切换上下文时，所有的进程都无法真正干事情，它们也会被阻塞。（如果是多核CPU则正在执行上下文切换操作的核不可被利用。）

- 非阻塞

程序在等待某操作过程中，自身不被阻塞，可以继续运行干别的事情，则称该程序在该操作上是非阻塞的。

非阻塞并**不是**在任何程序级别、任何情况下都可以存在的。

仅当程序封装的级别可以囊括独立的子程序单元时，它才可能存在非阻塞状态。

非阻塞的存在是因为阻塞存在，正因为某个操作阻塞导致的耗时与效率低下，我们才要把它变成非阻塞的。

```
阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态.
阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。
非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。

阻塞：进程给CPU传达一个任务之后，一直等待CPU处理完成，然后才执行后面的操作。

非阻塞：进程给CPU传达任我后，继续处理后续的操作，隔断时间再来询问之前的操作是否完成。这样的过程其实也叫轮询
```

阻塞、非阻塞、多路IO复用，都是同步IO，异步必定是非阻塞的，所以不存在异步阻塞和异步非阻塞的说法。真正的异步IO需要CPU的深度参与。换句话说，只有用户线程在操作IO的时候根本不去考虑IO的执行全部都交给CPU去完成，而自己只等待一个完成信号的时候，才是真正的异步IO。所以，拉一个子线程去轮询、去死循环，或者使用select、poll、epool，都不是异步。

## 同步/异步

- 同步

不同程序单元为了完成某个任务，在执行过程中需靠某种通信方式以**协调一致**，称这些程序单元是同步执行的。

例如购物系统中更新商品库存，需要用“行锁”作为通信信号，让不同的更新请求强制排队顺序执行，那更新库存的操作是同步的。

简言之，**同步意味着有序**。

- 异步

为完成某个任务，不同程序单元之间**过程中无需通信协调**，也能完成任务的方式。

不相关的程序单元之间可以是异步的。

例如，爬虫下载网页。调度程序调用下载程序后，即可调度其他任务，而无需与该下载任务保持通信以协调行为。不同网页的下载、保存等操作都是无关的，也无需相互通知协调。这些异步操作的完成时刻并不确定。

简言之，**异步意味着无序**。

```
同步和异步关注的是消息通信机制 
同步：在发出一个*调用*时，在没有得到结果之前，该*调用*就不返回。但是一旦调用返回，就得到返回值了。由*调用者*主动等待这个*调用*的结果。
异步：当一个异步过程调用发出后，调用者不会立刻得到结果。就直接返回了去执行其他的操作。在*调用*发出后，*被调用者*通过状态、通知来通知调用者，或通过回调函数处理这个调用。

同步：执行一个操作之后，等待结果，然后才继续执行后续的操作。

异步：执行一个操作后，可以去执行其他的操作，然后等待通知再回来执行刚才没执行完的操作。
```

## 并行/并发

- 并行

并行描述的是程序的执行状态。指多个任务同时被执行。

以利用富余计算资源（多核CPU）加速完成多个任务为目的。

- 并发

并发描述的是程序的组织结构。指程序要被设计成多个可独立执行的子任务。

以利用有限的计算机资源使多个任务可以被实时或近实时执行为目的。

```
单核CPU执行多线程：操作系统轮流让各个任务交替执行，真正的并行执行多任务只能在多核CPU上实现
多线程的执行方式和多进程是一样的，也是由操作系统在多个线程之间快速切换，让每个线程都短暂地交替运行，真正地同时执行多线程需要多核CPU才可能实现

并发：指的是任务数多余cpu核数，通过操作系统的各种任务调度算法，实现用多个任务“一起”执行（实际上总有一些任务不在执行，因为切换任务的速度相当快，看上去一起执行而已）

并行：指的是任务数小于等于cpu核数，即任务真的是一起执行的

并发：交替处理多个任务的能力
并行：同时处理多个任务的能力

多线程并不能真正的让多核cpu实现并行

原因 : cpython解释器中存在一个GIL(全局解释器锁),他的作用就是保证同一时刻只有一个线程可以执行代码,因此造成了我们使用多线程的时候无法实现并行

解决方案法 :
1:更换解释器 比如使用jpython(java实现的python解释器)
2:使用多进程完成多任务的处理
```

## 网络IO模型

网络IO都是通过Socket实现，Server在某一个端口持续监听，客户端通过Socket（IP+Port）与服务器建立连接（ServerSocket.accept），成功建立连接之后，就可以使用Socket中封装的InputStream和OutputStream进行IO交互了。针对每个客户端，Server都会创建一个新线程专门用于处理 

网络IO的本质是socket的读取，socket在linux系统被抽象为流，IO可以理解为对流的操作。对于一次IO访问，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间，所以一般会经历两个阶段：

```
1. 等待所有数据都准备好或者一直在等待数据，有数据的时候将数据拷贝到系统内核；
2. 将内核缓存中数据拷贝到用户进程中；
```

对于socket流而言：

```
1. 等待网络上的数据分组到达，然后被复制到内核的某个缓冲区；
2. 把数据从内核缓冲区复制到应用进程缓冲区中；
```

Linux平台一共有五种IO模型，每个模型有自己的优点和缺点，根据应用场景放入不同可以选择不同的IO模型

```
同步模型
阻塞IO（bloking IO）
非阻塞IO（non-blocking IO）
多路复用IO（multiplexing IO）
信号驱动式IO（signal-driven IO）

异步模型
异步IO（asynchronous IO）
```

默认情况下，网络IO是阻塞模式，即服务器线程在数据到来之前处于【阻塞】状态，等到数据到达，会自动唤醒服务器线程，着手进行处理。阻塞模式下，一个线程只能处理一个流的IO事件 

为了提升服务器线程处理效率，有以下三种思路

> （1）非阻塞【忙轮询】：采用死循环方式轮询每一个流，如果有IO事件就处理，这样可以使得一个线程可以处理多个流，但是效率不高，容易导致CPU空转
>
> （2）Select代理（无差别轮询）：可以观察多个流的IO事件，如果所有流都没有IO事件，则将线程进入阻塞状态，如果有一个或多个发生了IO事件，则唤醒线程去处理。但是还是得遍历所有的流，才能找出哪些流需要处理。如果流个数为N，则时间复杂度为O（N）
>
> （3）Epoll代理：Select代理有一个缺点，线程在被唤醒后轮询所有的Stream，还是存在无效操作。 Epoll会哪个流发生了怎样的I/O事件通知处理线程，因此对这些流的操作都是有意义的，复杂度降低到了O(1)

在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程去主动的check，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。而asynchronous IO则完全不同。它就像是用户进程将整个IO操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。

### 阻塞IO

```
							应用进程										内核
												系统调用
							recvfrom	----------->	无数据准备好
																									等待数据
进程阻塞于															 数据报准备好
recvfrom的调用											  	复制数据报
												返回数据成功指示	  			   将数据从内核复制到用户空间
							处理数据报	<-------------	复制完成
```

同步模型是效率最低的模型，每次只能处理完一个连接才能处理下一个，若是只有一个线程的话，若有一个连接一直占用，那么后来者之能等待。不适合高并发，不过最简单，符合惯性思维

说明

```
当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据。对于network io来说，很多时候数据在一开始还没有到达（比如，还没有收到一个完整的UDP包），这个时候kernel就要等待足够的数据到来。而在用户进程这边，整个进程会被阻塞。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。
所以，blocking IO的特点就是在IO执行的两个阶段都被block了。
```

示例

````python
# service.py
import socket

sk = socket.socket()
sk.bind(("127.0.0.1",8080))
sk.listen(5)
while 1:
    conn,addr=sk.accept()
    while 1:
        conn.send("hello client".encode("utf8"))
        data=conn.recv(1024)
        print(data.decode("utf8"))

# client.py
import socket

sk=socket.socket()
sk.connect(("127.0.0.1",8080))
while 1:
    data=sk.recv(1024)
    print(data.decode("utf8"))
    sk.send(b"hello server")
````

### 非阻塞式IO

```
							应用进程									内核
												 系统调用
							recvfrom	----------->	无数据准备好
												EWOULDBLOCK
									   		<-----------
							recvfrom	----------->	无数据准备好
												EWOULDBLOCK
									    	<-----------
        			recvfrom	----------->	无数据准备好
进程反复调用			   			EWOULDBLOCK
recvfrom等待						<-----------
返回成功指示																				等待数据
(轮询)												  			 数据报准备好
									  									复制数据报
										    返回数据成功指示	  			   将数据从内核复制到用户空间
							处理数据报	<-------------  复制完成
```

不会阻塞后面的代码，但是需要不停地显式询问内核数据是否准备好，一般通过while循环，而while循环会消耗大量的CPU，不适合高并发

说明

```
从图中可以看出，当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。
所以，用户进程其实是需要不断的主动询问kernel数据好了没有。

注意：
在网络IO时候，非阻塞IO也会进行recvform系统调用，检查数据是否准备好，与阻塞IO不一样，”非阻塞将大的整片时间的阻塞分成N多的小的阻塞, 所以进程不断地有机会 ‘被’ CPU光顾”。即每次recvform系统调用之间，cpu的权限还在进程手中，这段时间是可以做其他事情的，
也就是说非阻塞的recvform系统调用调用之后，进程并没有被阻塞，内核马上返回给进程，如果数据还没准备好，此时会返回一个error。进程在返回之后，可以干点别的事情，然后再发起recvform系统调用。重复上面的过程，循环往复的进行recvform系统调用。这个过程通常被称之为轮询。轮询检查内核数据，直到数据准备好，再拷贝数据到进程，进行数据处理。需要注意，拷贝数据整个过程，进程仍然是属于阻塞的状态。
```

示例

```python
# service.py
import time
import socket

sk = socket.socket(socket.AF_INET,socket.SOCK_STREAM)
sk.bind(('127.0.0.1',6667))
sk.listen(5)
sk.setblocking(False)  #设置成非阻塞状态
while True:
    try:  
        print ('waiting client connection .......')
        connection,address = sk.accept()   # 进程主动轮询
        print("+++",address)
        client_messge = connection.recv(1024)
        print(str(client_messge,'utf8'))
        connection.close()
    except Exception as e:  #捕捉错误
        print (e)
        time.sleep(4)  #每4秒打印一个捕捉到的错误
        
# client.py
import time
import socket

sk = socket.socket(socket.AF_INET,socket.SOCK_STREAM)
while True:
    sk.connect(('127.0.0.1',6667))
    print("hello")
    sk.sendall(bytes("hello","utf8"))
    time.sleep(2)
    break
```

### 多路复用

- 概述

与多进程和多线程技术相比，I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。

当前最流行，使用最广泛的高并发方案。同时可以监听多个连接，用的是单线程，利用空闲时间实现并发。这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗）

有三种实现方式，分别是select,poll,epoll

```
select--->效率最低，但有最大描述符限制，在linux为1024。
poll  ---->和select一样，但没有最大描述符限制。
epoll  --->效率最高，没有最大描述符限制，支持水平触发与边缘触发
```

多路I/O复用模型是利用 select、poll、epoll 可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。

Select,poll由于设计的问题，当处理连接过多会造成性能线性下降，而epoll是在前任的经验上做过改进的解决防范，不会有此问题。若场景是连接数不多，并且每个连接非常活跃，selct,poll是性能高于epoll的。

注意：Linux系统： select、poll、epoll；Windows系统：select；Mac系统：select、poll

应用场景

```
1. 当客户处理多个描述字时（一般是交互式输入和网络套接口），必须使用I/O复用。
2. 当一个客户同时处理多个套接口时，而这种情况是可能的，但很少出现。
3. 如果一个TCP服务器既要处理监听套接口，又要处理已连接套接口，一般也要用到I/O复用。
4. 如果一个服务器即要处理TCP，又要处理UDP，一般要使用I/O复用。
5. 如果一个服务器要处理多个服务或多个协议，一般要使用I/O复用。
```

模型描述

```
									应用进程								内核
													系统调用
进程受阻于select	 select		--------->	无数据报准备好
调用，等待可能多												 						等待数据
个套接字中的任何						返回可读条件			
一个变为可读							 <---------		数据报准备好
				   			recvfrom  --------->	复制数据报
数据复制到应用缓						 系统调用									将数据从内核复制到用户空间	   
冲期间进程阻塞											  		    
				   			处理数据报	<-----------	复制完成
                   				返回成功指示
```

说明

```
当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。

这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。（多说一句。所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）

在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。

注意1：select函数返回结果中如果有文件可读了，那么进程就可以通过调用accept()或recv()来让kernel将位于内核中准备到的数据copy到用户区。
注意2: select的优势在于可以处理多个连接，不适用于单个连接
```

示例

```python
# service.py
import socket
import select

sk = socket.socket()
sk.bind(("127.0.0.1",9904))
sk.listen(5)
while True:
    # sk.accept() #文件描述符
    r,w,e=select.select([sk,],[],[],5)  #输入列表，输出列表，错误列表,5: 是监听5秒
    for i in r:   #[sk,]
        conn,add=i.accept()
        print(conn)
        print("hello")
    print('>>>>>>')
    
# client.py
import socket

sk = socket.socket()
sk.connect(("127.0.0.1",9904))
while 1:
    inp=input(">>").strip()
    sk.send(inp.encode("utf8"))
    data=sk.recv(1024)
    print(data.decode("utf8"))
```

示例二

```python
# service.py
# 实现并发聊天功能 (select+IO多路复用，实现并发）
import socket
import select

sk = socket.socket()
sk.bind(("127.0.0.1",8801))
sk.listen(5)
inputs=[sk,]
while True:  #监听sk和conn
    r,w,e=select.select(inputs,[],[],5) #conn发生变化,sk不变化就走else
    print(len(r))
    #判断sk or conn 谁发生了变化
    for obj in r:
        if obj==sk:
            conn,add=obj.accept()
            print(conn)
            inputs.append(conn)
        else:
            data_byte=obj.recv(1024)
            print(str(data_byte,'utf8'))
            inp=input('回答%s号客户>>>'%inputs.index(obj))
            obj.sendall(bytes(inp,'utf8'))

    print('>>',r)

# client.py
import socket

sk=socket.socket()
sk.connect(('127.0.0.1',8801))
while True:
    inp=input(">>>>")
    sk.sendall(bytes(inp,"utf8"))
    data=sk.recv(1024)
    print(str(data,'utf8'))
```

- 触发

IO多路复用中的两种触发方式：

```
水平触发:
如果文件描述符已经就绪可以非阻塞的执行IO操作了,此时会触发通知.允许在任意时刻重复检测IO的状态, 没有必要每次描述符就绪后尽可能多的执行IO. 

边缘触发:
如果文件描述符自上次状态改变后有新的IO活动到来,此时会触发通知.在收到一个IO事件通知后要尽可能 多的执行IO操作,因为如果在一次通知中没有执行完IO那么就需要等到下一次新的IO活动到来才能获取到就绪的描述符.

select, poll就属于水平触发。
epoll：即可以采用水平触发,也可以采用边缘触发。
信号驱动式IO就属于边缘触发。
```

> 水平触发

只有高电平或低电平的时候才触发

1-----高电平---触发

0-----低电平---不触发

示例

```python
#水平触发
# service.py
import socket
import select

sk = socket.socket()
sk.bind(("127.0.0.1",9904))
sk.listen(5)
while True:
    r,w,e=select.select([sk,],[],[],5)  #input输入列表，output输出列表，erron错误列表,5: 是监听5秒
    for i in r:   #[sk,]
        print("hello")
    print('>>>>>>')
    
# client.py
import socket

sk = socket.socket()
sk.connect(("127.0.0.1",9904))
while 1:
    inp=input(">>").strip()
    sk.send(inp.encode("utf8"))
    data=sk.recv(1024)
    print(data.decode("utf8"))
```

> 边缘触发

1---------高电平--------触发

0---------低电平--------触发

示例

```python
# service.py
import socket
import select

sk = socket.socket()
sk.bind(("127.0.0.1",9904))
sk.listen(5)
inp=[sk,]
while True:
    r,w,e=select.select(inp,[],[],5)  #[sk,conn]，5是每隔几秒监听一次
    for i in r:   #[sk,]
        conn,add=i.accept()  #发送系统调用
        print(conn)
        print("hello")
        inp.append(conn)
        # conn.recv(1024)
    print('>>>>>>')
    
# client.py
import socket

sk = socket.socket()
sk.connect(("127.0.0.1",9904))
while 1:
    inp=input(">>").strip()
    sk.send(inp.encode("utf8"))
    data=sk.recv(1024)
    print(data.decode("utf8"))
```



### 信号驱动式

```
				应用进程															内核
							  			系统调用sigaction
				建立SIGIO的		--------------->		无数据准备好
				信号处理程序	 																	等待数据
进程继续执行							  							
							  				递交SIGIO
				信号处理程序	  <--------------- 		数据报准备好
         recvfrom 	  --------------->    拷贝数据报
                				系统调用			 
数据拷贝到应用缓									  			  						将数据从内核拷贝到用户空间
冲区期间进程阻塞									  
                				返回成功指示			
         处理数据报	   <---------------		 拷贝完成
```

使用信号，让内核在描述符就绪时发送SIGIO信号通知应用程序，称这种模型为信号驱动式I/O（signal-driven I/O）。

首先开启套接字的信号驱动式I/O功能，并通过sigaction系统调用安装一个信号处理函数。该系统调用将立即返回，我们的进程继续工作，也就是说进程没有被阻塞。当数据报准备好读取时，内核就为该进程产生一个SIGIO信号。随后就可以在信号处理函数中调用recvfrom读取数据报，并通知主循环数据已经准备好待处理，也可以立即通知主循环，让它读取数据报。

无论如何处理SIGIO信号，这种模型的优势在于等待数据报到达期间进程不被阻塞。主循环可以继续执行 ，只要等到来自信号处理函数的通知：既可以是数据已准备好被处理，也可以是数据报已准备好被读取。

 

### 异步非阻塞

考虑到CPU和IO之间巨大的速度差异，一个任务在执行的过程中大部分时间都在等待IO操作，单进程单线程模型会导致别的任务无法并行执行，因此，我们才需要多进程模型或者多线程模型来支持多任务并发执行。

现代操作系统对IO操作已经做了巨大的改进，最大的特点就是支持异步IO。如果充分利用操作系统提供的异步IO支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为事件驱动模型，Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。由于系统总的进程数量十分有限，因此操作系统调度非常高效。用异步IO编程模型来实现多任务是一个主要的趋势。

对应到Python语言，单线程的异步编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。

```
					应用进程										内核
											系统调用
					aio_read		------->		无数据报准备好
											<-------								等待数据
												返回				   
进程继续执行											  数据报准备好
																	复制数据报
																							将数据从内核复制到用户空间
					信号处理		递交在aio_read   	   
					程序处理		 <-------		  复制完成
					数据报			 中指定的信号
```

说明

```
用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。
```

### selectors

selectors模块会选择一个最优的操作系统实现方式

Select_model.py

```python
import selectors
import socket

sel = selectors.DefaultSelector()

def accept(sock, mask):
    conn, addr = sock.accept()  # Should be ready
    print('accepted', conn, 'from', addr)
    conn.setblocking(False)  #设置成非阻塞
    sel.register(conn, selectors.EVENT_READ, read) #conn绑定的是read

def read(conn, mask):
    try:
        data = conn.recv(1000)  # Should be ready
        if not data:
            raise Exception
        print('echoing', repr(data), 'to', conn)
        conn.send(data)  # Hope it won't block
    except Exception as e:
        print('closing', conn)
        sel.unregister(conn)  #解除注册
        conn.close()

sock = socket.socket()
sock.bind(('localhost', 8090))
sock.listen(100)
sock.setblocking(False)
#注册
sel.register(sock, selectors.EVENT_READ, accept)
print("server....")

while True:
    events = sel.select() #监听[sock,conn1,conn2]
    print("events",events)
    #拿到2个元素，一个key,一个mask
    for key, mask in events:
        # print("key",key)
        # print("mask",mask)
        callback = key.data  #绑定的是read函数
        # print("callback",callback)
        callback(key.fileobj, mask)  #key.fileobj=sock,conn1,conn2
```

Client.py

```python
import socket

sk=socket.socket()

sk.connect(("127.0.0.1",8090))
while 1:
    inp=input(">>>")
    sk.send(inp.encode("utf8")) #发送内容
    data=sk.recv(1024)  #接收信息
    print(data.decode("utf8"))  #打印出来
```





