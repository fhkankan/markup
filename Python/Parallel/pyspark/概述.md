# pySpark

[文档](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)

Spark是一个开源的、通用的并行计算与分布式计算框架。特点：基于内存计算，适合迭代计算，兼容多种应用场景，同时还兼容Hadoop生态系统中的组建，并且有很强的容错性。设计目的是全栈式解决批处理、结构化数据查询、流计算、图计算和机器学习等业务和应用，适用于需要多次操作特定数据集的应用场合，需要反复操作的次数越多，所需读取的数据量越大，效率提升越大

集成了Sark SQL(分布式SQL查询引擎，提供一个DataFrame编程抽象)、Spark Streaming(把流式计算分解成一系列短小的批处理计算)、MLib(提供机器学习服务)、GraphX(提供图计算服务)、SparkR(R on Spark)等子框架，为不同应用领域的从业者提供了全新的大数据处理方式。

为了适应迭代计算，Spark把经常被重用的数据缓存到内存汇总以提高数据读取和操作速度，比Hadoop快近百倍，支持Java,Scala,Python, R等多种语言。除map和reduce之外，Spark还支持filter,foreach,reduceByKey,aggregate以及SQL查询、流式查询等

PySpark 包含在 Apache Spark 网站上提供的 Spark 官方版本中。对于 Python 用户，PySpark 还提供从 PyPI 安装 pip。这通常用于本地使用或作为客户端连接到集群而不是自行设置集群。此页面包含使用 pip、Conda 安装 PySpark、手动下载和从源构建的说明。

## 安装

- 限制

python3.6以上

- 安装

```shell
# 普通安装
pip install pyspark

# 指定依赖安装
pip install pyspark[sql]

# 指定hadoop版本安装
PYSPARK_HADOOP_VERSION=2.7 pip install pyspark
```

