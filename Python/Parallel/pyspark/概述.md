# pySpark

[文档](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)

Spark是一个开源的、通用的并行计算与分布式计算框架。特点：基于内存计算，适合迭代计算，兼容多种应用场景，同时还兼容Hadoop生态系统中的组建，并且有很强的容错性。设计目的是全栈式解决批处理、结构化数据查询、流计算、图计算和机器学习等业务和应用，适用于需要多次操作特定数据集的应用场合，需要反复操作的次数越多，所需读取的数据量越大，效率提升越大

集成了Sark SQL(分布式SQL查询引擎，提供一个DataFrame编程抽象)、Spark Streaming(把流式计算分解成一系列短小的批处理计算)、MLib(提供机器学习服务)、GraphX(提供图计算服务)、SparkR(R on Spark)等子框架，为不同应用领域的从业者提供了全新的大数据处理方式。

为了适应迭代计算，Spark把经常被重用的数据缓存到内存汇总以提高数据读取和操作速度，比Hadoop快近百倍，支持Java,Scala,Python, R等多种语言。除map和reduce之外，Spark还支持filter,foreach,reduceByKey,aggregate以及SQL查询、流式查询等

PySpark 包含在 Apache Spark 网站上提供的 Spark 官方版本中。对于 Python 用户，PySpark 还提供从 PyPI 安装 pip。这通常用于本地使用或作为客户端连接到集群而不是自行设置集群。此页面包含使用 pip、Conda 安装 PySpark、手动下载和从源构建的说明。

## 安装

- 限制

python3.6以上

- 安装

```shell
# 普通安装
pip install pyspark

# 指定依赖安装
pip install pyspark[sql]

# 指定hadoop版本安装
PYSPARK_HADOOP_VERSION=2.7 pip install pyspark
```

## 快速开始

这是 PySpark DataFrame API 的简短介绍和快速入门。 PySpark DataFrames 被懒惰地评估。它们是在 RDD 之上实现的。 Spark 转换数据时，它不会立即计算转换，而是计划稍后如何计算。当诸如`collect()`之类的操作被显式调用时，计算开始。

Apache Spark 文档站点中还有其他有用的信息， [Spark SQL and DataFrames](https://spark.apache.org/docs/latest/sql-programming-guide.html), [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html), [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html), [Spark Streaming Programming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html) and [Machine Learning Library (MLlib) Guide](https://spark.apache.org/docs/latest/ml-guide.html).

PySpark 应用程序从初始化 SparkSession 开始，它是 PySpark 的入口点，如下所示。如果通过 pyspark 可执行文件在 PySpark shell 中运行它，shell 会自动在变量 spark 中为用户创建会话。

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()
```

### 创建DataFrame

PySpark DataFrame 可以通过 `pyspark.sql.SparkSession.createDataFrame` 创建，通常通过传递一个列表、元组、字典和 pyspark.sql.Rows、一个 Pandas DataFrame 和一个由这样的列表组成的 RDD 来创建。 `pyspark.sql.SparkSession.createDataFrame` 使用 `schema` 参数来指定 DataFrame 的架构。省略时，PySpark 通过从数据中抽取样本来推断相应的模式。

使用rows列创建

```python
from datetime import datetime, date
import pandas as pd
from pyspark.sql import Row

df = spark.createDataFrame([
    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),
    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),
    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))
])
```

使用schema参数创建

```python
df = spark.createDataFrame([
    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),
    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),
    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))
], schema='a long, b double, c string, d date, e timestamp')
```

使用pandas的dataframe创建

```python
pandas_df = pd.DataFrame({
    'a': [1, 2, 3],
    'b': [2., 3., 4.],
    'c': ['string1', 'string2', 'string3'],
    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],
    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]
})
df = spark.createDataFrame(pandas_df)
```

使用由元组组成的RDD创建

```python
rdd = spark.sparkContext.parallelize([
    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),
    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),
    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))
])
df = spark.createDataFrame(rdd, schema=['a', 'b', 'c', 'd', 'e'])
```

展示

```python
df.show()
df.printSchema()
```

### 查看数据

查看顶部行

```python
df.show(1)
```

或者，您可以启用`spark.sql.repl.eagerEval.enabled`配置，以便在 Jupyter 等笔记本中对 PySpark DataFrame 进行预先评估。要显示的行数可以通过 `spark.sql.repl.eagerEval.maxNumRows` 配置来控制。

```python
spark.conf.set('spark.sql.repl.eagerEval.enabled', True)
```

行可以垂直显示

```python
df.show(1, vertical=True)
```

查看schema和列的名字

```python
df.columns
df.printSchema()
```

查看DataFrame的概要

```python
df.select("a", "b", "c").describe().show()
```

`DataFrame.collect()` 将分布式数据作为Python中的本地数据收集到驱动端。请注意，当数据集太大而无法放入驱动程序端时，这可能会引发内存不足错误，因为它将所有数据从执行程序收集到驱动程序端。

```python
df.collect()
```

为了避免抛出内存不足异常，请使用 `DataFrame.take() `或 `DataFrame.tail()`

```python
df.take(1)
```

PySpark DataFrame 还提供转换回 Pandas DataFrame 以利用 Pandas API。请注意，toPandas 还将所有数据收集到驱动程序端，当数据太大而无法装入驱动程序端时，很容易导致内存不足错误。 

```python
df.toPandas()
```

### 选择访问

PySpark DataFrame 是惰性求值的，简单地选择一列不会触发计算，但会返回一个 Column 实例。

```
df.a
```

大多数列操作返回列

```python
from pyspark.sql import Column
from pyspark.sql.functions import upper

type(df.c) == type(upper(df.c)) == type(df.c.isNull())
```

这些列可用于从 DataFrame 中选择列。例如， `DataFrame.select()` 获取返回另一个 DataFrame 的 Column 实例。

```python
df.select(df.c).show()
```

分配新的列实例

```python
df.withColumn('upper_c', upper(df.c)).show()
```

要选择行的子集，请使用 `DataFrame.filter()`

```python
df.filter(df.a == 1).show()
```

### 应用函数

PySpark 支持各种 UDF 和 API，以允许用户执行 Python 本机函数。另请参阅最新的 [Pandas UDFs](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#pandas-udfs-aka-vectorized-udfs) 和 [Pandas Function APIs](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#pandas-function-apis).。

例如，下面的示例允许用户在 Python 原生函数中直接使用 pandas series 中的 API。

```python
import pandas
from pyspark.sql.functions import pandas_udf

@pandas_udf('long')
def pandas_plus_one(series: pd.Series) -> pd.Series:
    # Simply plus one by using pandas Series.
    return series + 1

df.select(pandas_plus_one(df.a)).show()
```

另一个例子是 DataFrame.mapInPandas，它允许用户直接使用 Pandas DataFrame 中的 API，没有任何限制，例如结果长度。

```python
def pandas_filter_func(iterator):
    for pandas_df in iterator:
        yield pandas_df[pandas_df.a == 1]

df.mapInPandas(pandas_filter_func, schema=df.schema).show()
```

### 分组数据

PySpark DataFrame 还提供了一种使用通用方法拆分-应用-组合策略来处理分组数据的方法。它按特定条件对数据进行分组，对每个组应用一个函数，然后将它们组合回 DataFrame。

```python
df = spark.createDataFrame([
    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],
    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],
    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])
df.show()

# 分组并使用avg()函数
df.groupby('color').avg().show()
```

使用 Pandas API 对每个组应用 Python 本机函数。

```python
def plus_mean(pandas_df):
    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())

df.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()
```

共同分组和应用功能

```python
df1 = spark.createDataFrame(
    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],
    ('time', 'id', 'v1'))

df2 = spark.createDataFrame(
    [(20000101, 1, 'x'), (20000101, 2, 'y')],
    ('time', 'id', 'v2'))

def asof_join(l, r):
    return pd.merge_asof(l, r, on='time', by='id')

df1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(
    asof_join, schema='time int, id int, v1 double, v2 string').show()
```

### 导入导出

CSV 简单易用。 Parquet 和 ORC 是高效紧凑的文件格式，读写速度更快。 PySpark 中还有许多其他数据源可用，例如 JDBC、文本、二进制文件、Avro 等。另请参阅 Apache Spark 中最新的 Spark SQL、DataFrames 和数据集指南文档。

CSV

```python
df.write.csv('foo.csv', header=True)
spark.read.csv('foo.csv', header=True).show()
```

Parquet

```python
df.write.parquet('bar.parquet')
spark.read.parquet('bar.parquet').show()
```

ORC

```python
df.write.orc('zoo.orc')
spark.read.orc('zoo.orc').show()
```

### 使用SQL

DataFrame 和 Spark SQL 共享相同的执行引擎，因此它们可以无缝地互换使用。

例如，您可以将 DataFrame 注册为表并轻松运行 SQL，如下所示

```python
df.createOrReplaceTempView("tableA")
spark.sql("SELECT count(*) from tableA").show()
```

此外，UDF 可以开箱即用地在 SQL 中注册和调用

```python
@pandas_udf("integer")
def add_one(s: pd.Series) -> pd.Series:
    return s + 1

spark.udf.register("add_one", add_one)
spark.sql("SELECT add_one(v1) FROM tableA").show()
```

这些 SQL 表达式可以直接混合用作 PySpark 列。

```python
from pyspark.sql.functions import expr

df.selectExpr('add_one(v1)').show()
df.select(expr('count(*)') > 0).show()
```

