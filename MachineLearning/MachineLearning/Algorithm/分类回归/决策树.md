# 决策树

是一种非参数学习算法，可以实现分类和回归任务，甚至是多输出任务。

决策树也是随机森林的基本组成部分。

- 优缺点

优点
1. 天然可以解决多分类问题，也可以解决回归问题
2. 非常好的可解释性
3. 不需要进行特征缩放

缺点

1.决策边界横平竖直，对于倾斜的样本分类易过拟合

2.对个别数据敏感


## 原理

### 信息熵

信息是用来消除随机不确定性的东西。

**信息量**：一个事件的信息量就是这个事件发生概率的负对数，单位bit
$$
-\log P(x)
$$

一个事情发生的概率越小，信息量越大。

熵在信息论中代表随机变量不确定度的度量

熵越大，数据的不确定性越高；熵越小，数据的不确定性越低

**信息熵**：一个事件有很多结果，那么所有结果携带信息量的期望就是信息熵。结果是一个大于等于零的值，熵值越高，说明事件的不确定性越大。而当有信息表明不确定性越大的事件的结果时，该条信息的价值越高。
$$
H = -\sum_{i=1}^k{p_ilog(p_i)}
$$
二分类
$$
H = -xlog(x)-(1-x)log(1-x)
$$

**条件熵**：在某一个条件下，随机变量的不确定度
$$
H(Y|X) = \sum_{i=1}^{k}P(x_i)H(Y|X=x_i)
$$
**信息增益**：信息增益 = 信息熵-条件熵

代表了在一个条件下，信息复杂度(不确定性)减少的程度

### 基尼系数

基尼系数越高，数据的不确定性越高，基尼系数越小，数据的不确定性越低
$$
G = \sum_{i=1}^k{p_i}(1-p_i) = 1 - \sum_{i=1}^k{p_i^2}
$$
二分类
$$
G = 1-x^2 - (1-x)^2 = -2x^2 + 2x
$$

- 信息熵VS基尼系数

大多数情况下，并没有大的不同，产生的树都很相似。

基尼系数的计算速度略快一些，故常用此做默认选择。

他们的不同在于，基尼系数倾向于从树中分裂出最常见的类别，而信息熵则倾向于生产更平衡的树。

### CART

sklearn的决策树实现是CART（Classification And Regression Tree），该算法仅生成二叉树：非叶子结点永远只有两个子节点。

首先，使用单个特征 $k$ 和阈值 $t_k$ 将训练集分成两个子集。一旦将训练集一分为二，将使用相同的逻辑，继续分裂子集，然后是子集的子集，一次循环递进，直到抵达最大深度（由超参数`max_depth`控制），或是再也找不到能够降低不纯度的分裂，它才会停止。

特征和阈值的选取，根据产生出最纯子集（受其大小加权）的$k$ 和 $t_k$ 就是经算法搜索确定的 $(t, t_k)$。

算法尝试最小化的成本函数为
$$
J(k,t_k) = \frac{m_{left}}{m}G_{left} + \frac{m_{right}}{m}G_{right} 
$$
 其中，$G_{left/right}$ 衡量左/右子集的不纯度，$m_{left/right}$ 衡量左/右子集的实例数量

CART算法是一个贪婪算法：从顶层开始搜索最优分裂，然后每层重复这个过程。几层分裂之后，它并不会检视这个分裂的不纯度是否为可能的最低值。贪婪算法通常会产生一个相当不错的解，但不能保证是最优解。

其他的实现方式有ID3, C4.5, C5.0

```
CART 算法
基尼(gini)系数   最小的准则：若属性基尼系数越小，该属性优先判断


ID3算法
信息增益 最大的准则：若属性信息增益越大，该属性优先判断
生成的决策树其节点可以有两个以上的子节点

C4.5算法
信息增益比 最大的准则：若属性信息增益比越大，该属性优先判断
```

### 时间复杂度

预测时需要从根到叶遍历决策树。通常来说，决策树大致平衡，因此遍历决策树需要经历大约 $O(\log_2(m))$ 个节点。而每个节点只需要检查一个特征值，所以总体预测复杂度也只是 $O(\log_2(m))$ ，与特征数量无关。如此，即使处理大型数据集，预测也很快。

训练时在每一个节点，算法都需要在所有样本熵比较所有特征（若是设置了`max_features` 会少一些）。这导致训练的复杂度为 $O(n\times m\log(m))$。对于小型数据集（几千个）,sklearn可以通过对数据预处理（设置`presort=True`）来加快训练，但是对于较大训练集而言，可能会减慢训练的速度。

由于常常产生过拟合，故采用剪枝

### 正则化超参数

决策树极少对训练数据做出假设。如果不加以限制，树的结构将跟随训练集变化，严密拟合，并且很可能过度拟合。这种模型被称为**非参数模型**，并不是说它不包含任何参数，而是指在训练之前没有确定参数的数量，导致模型结构自由而紧密地贴近数据。

为了避免过度拟合，需要在训练过程中降低决策树的自由度。正则化超参数的选择取决于所使用的模型，但通常来说，至少可以 限制决策树的最大深度。在sklearn中，由超参数`max_depth`控制（默认为`None`），减少`max_depth`，可以降低过度拟合的风险。

剪枝：降低复杂度，解决过拟合

剪枝：预剪枝(更常用)、后剪枝

预剪枝：限制深度，叶子节点个数，叶子节点样本数，信息增益量等

后剪枝：通过一定的衡量标准$C_{\alpha}(T)=C(T)+\alpha\cdot|T_{leaf}|$，叶子节点越多，损失越大（$C(T)$是叶子节点gini*叶子节点样本数，$T_{leaf}$是叶子节点数）

## 实现

### 信息熵

```python
import numpy as np
import matplotlib.pyplot as plt

def entropy(p):
      return -p * np.log(p) - (1 - p) * np.log(1 - p)

x = np.linspace(0.01, 0.99, 200)
plt.plot(x, entropy(x))
plt.show()
# 在两类别信息平均分配时，系统最不稳定
```

### 寻找最优划分

使用信息熵

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from collections import Counter
from math import log

iris = datasets.load_iris()
X = iris.data[:, 2:]  # 取后两个维度特征
y = iris.target


dt_clf = DecisionTreeClassifier(max_depth=2, criterion="gini")
dt_clf.fit(X, y)

def split(X, y, d, value):
      index_a = (X[:, d] <= value)
    index_b = (X[:, d] > value)
    return X[index_a], X[index_b], y[index_a], y[index_b]

def entropy(y):
      counter = Counter(y)
    res = 0.0
    for num in counter.value():
          p = num / len(y)
        res += -p * log(p)
    return res

def try_split(X, y):
      best_entropy = float('inf')  # 正无穷
    best_d, best_v = -1, -1
    for d in range(X.shpe[1]):
          sorted_index = np.argsort(X[:, d])
        for i in range(1, len(X)):
              if X[sorted_index[i-1], d] != X[sorted_index[i], d]:
                      v = (X[sorted_index[i-1], d] + X[sorted_index[i], d]) / 2
                X_l, X_r, y_l, y_r = split(X, y, d, v)
                    e = entropy(y_l) + entropy(y_r)
                if e < best_entropy:
                      best_entropy, best_d, best_v = e, d, v
    return best_entropy, best_d, best_v

best_entropy, best_d, best_v = try_split(X, y)
print("best_entropy =", best_entropy)
print("best_d =", best_d)
print("best_v =", best_v)

X1_l, X1_r, y1_l, y1_r = split(X, y, best_d, best_v)
print(entropy(y1_1))
print(entropy(y1_r))

best_entropy2, best_d2, best_v2 = try_split(X1_r, y1_r)
print("best_entropy =", best_entropy2)
print("best_d =", best_d2)
print("best_v =", best_v2)

X2_l, X2_r, y2_l, y2_r = split(X1_r, y1_r, best_d2, best_v2)
print(entropy(y2_1))
print(entropy(y2_r))
```

使用基尼系数

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from collections import Counter
from math import log

iris = datasets.load_iris()
X = iris.data[:, 2:]  # 取后两个维度特征
y = iris.target

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.scatter(X[y==2, 0], X[y==2, 1])
plt.show()

dt_clf = DecisionTreeClassifier(max_depth=2, criterion="entropy")
dt_clf.fit(X, y)

def split(X, y, d, value):
      index_a = (X[:, d] <= value)
    index_b = (X[:, d] > value)
    return X[index_a], X[index_b], y[index_a], y[index_b]

def gini(y):
      counter = Counter(y)
    res = 1.0
    for num in counter.value():
          p = num / len(y)
        res -= p ** 2
    return res

def try_split(X, y):
      best_g = float('inf')
    best_d, best_v = -1, -1
    for d in range(X.shpe[1]):
          sorted_index = np.argsort(X[:, d])
        for i in range(1, len(X)):
              if X[sorted_index[i-1], d] != X[sorted_index[i], d]:
                      v = (X[sorted_index[i-1], d] + X[sorted_index[i], d]) / 2
                X_l, X_r, y_l, y_r = split(X, y, d, v)
                    g = gini(y_l) + gini(y_r)
                if e < best_g:
                      best_g, best_d, best_v = g, d, v
    return best_g, best_d, best_v

best_g, best_d, best_v = try_split(X, y)
print("best_g =", best_g)
print("best_d =", best_d)
print("best_v =", best_v)

X1_l, X1_r, y1_l, y1_r = split(X, y, best_d, best_v)
print(gini(y1_1))
print(gini(y1_r))

best_g2, best_d2, best_v2 = try_split(X1_r, y1_r)
print("best_g =", best_g2)
print("best_d =", best_d2)
print("best_v =", best_v2)

X2_l, X2_r, y2_l, y2_r = split(X1_r, y1_r, best_d2, best_v2)
print(gini(y2_1))
print(gini(y2_r))
```

## sklearn

### 估算类别概率

决策树可以估算出某个实例属于特定类别k的概率：首先，跟随决策树找到该实例的叶子结点，然后返回该节点中类别k的训练实例的占比。

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

np.random.seed(42)

iris = load_iris()
X = iris.data[:, 2:]  # petal length and width
y = iris.target

tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)
tree_clf.fit(X, y)

res_p = tree_clf.predict_proba([[5, 1.5]])  # 预测类别中的概率
res_c = tree_clf.predict(([[5, 1.5]]))  # 预测类别
print(res_p, res_c)
"""
[[0. 0.90740741 0.09259259]] 
[1]
"""
```

### 不同方法分类

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier

iris = datasets.load_iris()
X = iris.data[:, 2:]  # 取后两个特征
y = iris.target

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.scatter(X[y==2, 0], X[y==2, 1])
plt.show()

# 决策边界
def plot_decision_boundary(model, axis):
      x0, x1 = np.meshgrid(
            np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1)
          np.linspace(axis[2], axisp3), int((axis[3]-axis[2])*100)).reshape(-1, 1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]
    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)
    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(["#EF9A9A", "#FF59D", "#90CAF9"])

    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)

# 使用信息熵
dt_clf = DecisionTreeClassifier(max_depth=2, criterion="entropy")
dt_clf.fit(X, y)

plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.scatter(X[y==2, 0], X[y==2, 1])
plt.show() 

# 使用基尼系数
dt_clf = DecisionTreeClassifier(max_depth=2, criterion="gini")
dt_clf.fit(X, y)

plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.scatter(X[y==2, 0], X[y==2, 1])
plt.show() 
```

网格搜索

```python
from __future__ import print_function

import numpy as np
import multiprocessing

from sklearn.datasets import load_digits
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV


# For reproducibility
np.random.seed(1000)


if __name__ == '__main__':
    # Load dataset
    digits = load_digits()

    # Define a param grid
    param_grid = [
        {
            'criterion': ['gini', 'entropy'],
            'max_features': ['auto', 'log2', None],
            'min_samples_split': [2, 10, 25, 100, 200],
            'max_depth': [5, 10, 15, None]
        }
    ]

    # Create and train a grid searh
    gs = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=param_grid,
                      scoring='accuracy', cv=10, n_jobs=multiprocessing.cpu_count())
    gs.fit(digits.data, digits.target)

    print(gs.best_estimator_)
    print('Decision tree score: %.3f' % gs.best_score_)
```



### 超参数剪枝

```python
"""
DecisionTreeClassifier类中超参数有：
max_depth(最大深度)
max_leaf_nodes(最大叶节点数量)
max_features(分类每个节点评估的最大特征数量)
min_samples_split(分裂前节点必须有的最小样本数量)
min_samples_leaf(叶节点必须有的最小样本数量)
min_weight_fraction_leaf(跟min_samples_leaf一样，但表现为加权实例总数的占比)
增大超参数min_*或是减少max_*将使模型正则化
"""
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier

X, y = datasets.make_moons(noise=0.25, random_state=666)

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

# 决策边界
def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
          np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
          np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]
    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)
    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])

    plt.contourf(x0, x1, zz, cmap=custom_cmap)


# 默认使用基尼系数划分, 无深度限制，过拟合
dt_clf = DecisionTreeClassifier()
dt_clf.fit(X, y)

plot_decision_boundary(dt_clf, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

# 深度降低为2，泛化能力增强
dt_clf2 = DecisionTreeClassifier(max_depth=2)
dt_clf2.fit(X, y)

plot_decision_boundary(dt_clf2, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

# 最小可划分叶子节点样本数，若节点样本数少于此，则不会再继续选择最优特征进行划分
# min_samples_split值越高，泛化能力越强
dt_clf3 = DecisionTreeClassifier(min_samples_split=10)
dt_clf3.fit(X, y)

plot_decision_boundary(dt_clf3, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

# 叶子节点最少样本数，若样本数少于此，则被剪枝
# min_samples_leaf值越高，泛化能力越强
dt_clf4 = DecisionTreeClassifier(min_samples_leaf=10)
dt_clf4.fit(X, y)

plot_decision_boundary(dt_clf4, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()

# 最大叶子节点数，可以防止过拟合
# max_leaf_nodes值越高，泛化能力越差
dt_clf5 = DecisionTreeClassifier(max_leaf_nodes=4)
dt_clf5.fit(X, y)

plot_decision_boundary(dt_clf5, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()
```

### 不稳定性

对训练集的小变化敏感

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier

iris = datasets.load_iris()
X = iris.data[:, 2:]  # 取后两个特征
y = iris.target

plt.scatter(X[y == 0, 0], X[y == 0, 1])
plt.scatter(X[y == 1, 0], X[y == 1, 1])
plt.scatter(X[y == 2, 0], X[y == 2, 1])
plt.show()


# 决策边界
def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1] - axis[0]) * 100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3] - axis[2]) * 100)).reshape(-1, 1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]
    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)
    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(["#EF9A9A", "#FF59D3", "#90CAF9"])

    plt.contourf(x0, x1, zz, cmap=custom_cmap)


# 使用信息熵
dt_clf = DecisionTreeClassifier(max_depth=2, criterion="entropy")
dt_clf.fit(X, y)

plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(X[y == 0, 0], X[y == 0, 1])
plt.scatter(X[y == 1, 0], X[y == 1, 1])
plt.scatter(X[y == 2, 0], X[y == 2, 1])
plt.show()

# 删除特殊点
X_new = np.delete(X, 138, axis=0)
y_new = np.delete(y, 138)

tree_clf = DecisionTreeClassifier(max_depth=2, criterion="entropy")
tree_clf.fit(X_new, y_new)

plot_decision_boundary(tree_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(X[y == 0, 0], X[y == 0, 1])
plt.scatter(X[y == 1, 0], X[y == 1, 1])
plt.scatter(X[y == 2, 0], X[y == 2, 1])
plt.show()

```

对数据集的旋转敏感

```python
import numpy as np
import os
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from matplotlib.colors import ListedColormap

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training"


def image_path(fig_id):
    return os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id)


def save_fig(fig_id, tight_layout=True):
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(image_path(fig_id) + ".png", format='png', dpi=300)


def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):
    x1s = np.linspace(axes[0], axes[1], 100)
    x2s = np.linspace(axes[2], axes[3], 100)
    x1, x2 = np.meshgrid(x1s, x2s)
    X_new = np.c_[x1.ravel(), x2.ravel()]
    y_pred = clf.predict(X_new).reshape(x1.shape)
    custom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])
    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)
    if not iris:
        custom_cmap2 = ListedColormap(['#7d7d58', '#4c4c7f', '#507d50'])
        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)
    if plot_training:
        plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], "yo", label="Iris-Setosa")
        plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], "bs", label="Iris-Versicolor")
        plt.plot(X[:, 0][y == 2], X[:, 1][y == 2], "g^", label="Iris-Virginica")
        plt.axis(axes)
    if iris:
        plt.xlabel("Petal length", fontsize=14)
        plt.ylabel("Petal width", fontsize=14)
    else:
        plt.xlabel(r"$x_1$", fontsize=18)
        plt.ylabel(r"$x_2$", fontsize=18, rotation=0)
    if legend:
        plt.legend(loc="lower right", fontsize=14)


np.random.seed(6)
Xs = np.random.rand(100, 2) - 0.5
ys = (Xs[:, 0] > 0).astype(np.float32) * 2

angle = np.pi / 4
rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])
Xsr = Xs.dot(rotation_matrix)

tree_clf_s = DecisionTreeClassifier(random_state=42)
tree_clf_s.fit(Xs, ys)
tree_clf_sr = DecisionTreeClassifier(random_state=42)
tree_clf_sr.fit(Xsr, ys)

plt.figure(figsize=(11, 4))
plt.subplot(121)
plot_decision_boundary(tree_clf_s, Xs, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False)
plt.subplot(122)
plot_decision_boundary(tree_clf_sr, Xsr, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False)

save_fig("sensitivity_to_rotation_plot")
plt.show()

```

### 回归问题

决策树也可以解决回归问题

```python
from sklearn.tree import DecisionTreeRegressor
tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)
```

- 预测值

使用模型进行预测时，每个区域的预测值永远等于该区域内实例的目标平均值。算法分裂每个区域的方法，就是使最多的训练实例尽可能接近这个预测值

```python
import numpy as np
import os
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.tree import DecisionTreeRegressor

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training"


def image_path(fig_id):
    return os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id)


def save_fig(fig_id, tight_layout=True):
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(image_path(fig_id) + ".png", format='png', dpi=300)


def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel="$y$"):
    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)
    y_pred = tree_reg.predict(x1)
    plt.axis(axes)
    plt.xlabel("$x_1$", fontsize=18)
    if ylabel:
        plt.ylabel(ylabel, fontsize=18, rotation=0)
    plt.plot(X, y, "b.")
    plt.plot(x1, y_pred, "r.-", linewidth=2, label=r"$\hat{y}$")


np.random.seed(42)
m = 200
X = np.random.rand(m, 1)
y = 4 * (X - 0.5) ** 2
y = y + np.random.randn(m, 1) / 10

tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)
tree_reg2 = DecisionTreeRegressor(random_state=42, max_depth=3)
tree_reg1.fit(X, y)
tree_reg2.fit(X, y)

plt.figure(figsize=(11, 4))
plt.subplot(121)
plot_regression_predictions(tree_reg1, X, y)
for split, style in ((0.1973, "k-"), (0.0917, "k--"), (0.7718, "k--")):
    plt.plot([split, split], [-0.2, 1], style, linewidth=2)
plt.text(0.21, 0.65, "Depth=0", fontsize=15)
plt.text(0.01, 0.2, "Depth=1", fontsize=13)
plt.text(0.65, 0.8, "Depth=1", fontsize=13)
plt.legend(loc="upper center", fontsize=18)
plt.title("max_depth=2", fontsize=14)

plt.subplot(122)
plot_regression_predictions(tree_reg2, X, y, ylabel=None)
for split, style in ((0.1973, "k-"), (0.0917, "k--"), (0.7718, "k--")):
    plt.plot([split, split], [-0.2, 1], style, linewidth=2)
for split in (0.0458, 0.1298, 0.2873, 0.9040):
    plt.plot([split, split], [-0.2, 1], "k:", linewidth=1)
plt.text(0.3, 0.5, "Depth=2", fontsize=13)
plt.title("max_depth=3", fontsize=14)

save_fig("tree_regression_plot")
plt.show()

```

- 正则化

与分类任务一样，决策树在处理回归问题时，也容易过度拟合，可以采用超参数限制来正则化

```python
import numpy as np
import os
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.tree import DecisionTreeRegressor

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training"


def image_path(fig_id):
    return os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id)


def save_fig(fig_id, tight_layout=True):
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(image_path(fig_id) + ".png", format='png', dpi=300)


def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel="$y$"):
    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)
    y_pred = tree_reg.predict(x1)
    plt.axis(axes)
    plt.xlabel("$x_1$", fontsize=18)
    if ylabel:
        plt.ylabel(ylabel, fontsize=18, rotation=0)
    plt.plot(X, y, "b.")
    plt.plot(x1, y_pred, "r.-", linewidth=2, label=r"$\hat{y}$")


np.random.seed(42)
m = 200
X = np.random.rand(m, 1)
y = 4 * (X - 0.5) ** 2
y = y + np.random.randn(m, 1) / 10

tree_reg1 = DecisionTreeRegressor(random_state=42)
# min_samples_leaf正则化
tree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)
tree_reg1.fit(X, y)
tree_reg2.fit(X, y)

x1 = np.linspace(0, 1, 500).reshape(-1, 1)
y_pred1 = tree_reg1.predict(x1)
y_pred2 = tree_reg2.predict(x1)

plt.figure(figsize=(11, 4))

plt.subplot(121)
plt.plot(X, y, "b.")
plt.plot(x1, y_pred1, "r.-", linewidth=2, label=r"$\hat{y}$")
plt.axis([0, 1, -0.2, 1.1])
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", fontsize=18, rotation=0)
plt.legend(loc="upper center", fontsize=18)
plt.title("No restrictions", fontsize=14)

plt.subplot(122)
plt.plot(X, y, "b.")
plt.plot(x1, y_pred2, "r.-", linewidth=2, label=r"$\hat{y}$")
plt.axis([0, 1, -0.2, 1.1])
plt.xlabel("$x_1$", fontsize=18)
plt.title("min_samples_leaf={}".format(tree_reg2.min_samples_leaf), fontsize=14)

save_fig("tree_regression_regularization_plot")
plt.show()

```

## 可视化

graphviz是一个开源图形可视化软件包

### 安装

```shell
# ubuntu
sudo apt-get install graphviz
# mac
brew install graphviz
```

### 使用

生成dot数据

```python
# 通过graphviz生成dot数据
dot_data = tree.export_graphviz(
	dt_reg,  # 训练后的模型
  	out_file = None,
  	feature_names = housing.feature_names[6:8],  # 特征
  	filled = True,
  	impurity = False,
  	rounded = True
)
```

ipython转换图片

```shell
# 安装pydotplus
pip install pydotplus

# 使用
import pydotplus
from IPython.display import Image

graph = pydotplus.graph_from_dot_data(dot_data)
graph.get_nodes()[7].set_fillcolor("#FFF2DD")
Image(graph.create_png())
graph.write_png("demo.png")
```

graphviz的dot命令转换图片

```shell
dot -Tpng iris_tree.dot -o iris_tree.png
```

### 示例

```python
import numpy as np
import os
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from matplotlib.colors import ListedColormap

np.random.seed(42)

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training"


def image_path(fig_id):
    return os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id)


def save_fig(fig_id, tight_layout=True):
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(image_path(fig_id) + ".png", format='png', dpi=300)


iris = load_iris()
X = iris.data[:, 2:]  # petal length and width
y = iris.target

tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)
tree_clf.fit(X, y)

# 导出dot数据
export_graphviz(
    tree_clf,
    out_file=image_path("iris_tree.dot"),
    feature_names=iris.feature_names[2:],
    class_names=iris.target_names,
    rounded=True,
    filled=True
)

# 绘制边界
def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):
    x1s = np.linspace(axes[0], axes[1], 100)
    x2s = np.linspace(axes[2], axes[3], 100)
    x1, x2 = np.meshgrid(x1s, x2s)
    X_new = np.c_[x1.ravel(), x2.ravel()]
    y_pred = clf.predict(X_new).reshape(x1.shape)
    custom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])
    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)
    if not iris:
        custom_cmap2 = ListedColormap(['#7d7d58', '#4c4c7f', '#507d50'])
        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)
    if plot_training:
        plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], "yo", label="Iris-Setosa")
        plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], "bs", label="Iris-Versicolor")
        plt.plot(X[:, 0][y == 2], X[:, 1][y == 2], "g^", label="Iris-Virginica")
        plt.axis(axes)
    if iris:
        plt.xlabel("Petal length", fontsize=14)
        plt.ylabel("Petal width", fontsize=14)
    else:
        plt.xlabel(r"$x_1$", fontsize=18)
        plt.ylabel(r"$x_2$", fontsize=18, rotation=0)
    if legend:
        plt.legend(loc="lower right", fontsize=14)


plt.figure(figsize=(8, 4))
plot_decision_boundary(tree_clf, X, y)
plt.plot([2.45, 2.45], [0, 3], "k-", linewidth=2)
plt.plot([2.45, 7.5], [1.75, 1.75], "k--", linewidth=2)
plt.plot([4.95, 4.95], [0, 1.75], "k:", linewidth=2)
plt.plot([4.85, 4.85], [1.75, 3], "k:", linewidth=2)
plt.text(1.40, 1.0, "Depth=0", fontsize=15)
plt.text(3.2, 1.80, "Depth=1", fontsize=13)
plt.text(4.05, 0.5, "(Depth=2)", fontsize=11)

save_fig("decision_tree_decision_boundaries_plot")
plt.show()
```

