# 线性回归

线性模型是最直观的机器学习模型，通过学习属性与目标属性的线性关系来解释数据的规律

思想简单，实现容易，结果具有良好的可解释性

可解释性

```
对于系数，若为正，表示特征与目标值正相关；若为负，表示特征与目标值负相关
```

只能解决线性问题

## 简单线性回归

### 损失函数

假设找到了最佳拟合的直线方程
$$
y = ax + b
$$
对于每一个样本点$x^{(i)}$，根据直线方程，预测值为
$$
\hat{y}^{(i)} = ax^{(i)} + b
$$
则损失函数为
$$
\sum_{i=1}^m{(y^{(i)}-\hat{y}^{(i)})^2}=\sum_{i=1}^m{(y^{(i)}-ax^{(i)}-b)^2}=J(a,b)
$$
求$J(a,b)$函数的最小值，采用求导
$$
\frac{\partial{J(a,b)}}{\partial{a}}=\sum_{i=1}^m{2(y^{(i)}-ax^{(i)}-b)}(-x^{(i)})=0
$$

$$
\frac{\partial{J(a,b)}}{\partial{b}}=\sum_{i=1}^m{2(y^{(i)}-ax^{(i)}-b)(-1)}=0
$$

求得a,b为
$$
a = \frac{\sum_{i=1}^m{(x^{(i)}-\bar{x})(y^{(i)}-\bar{y})}}{\sum_{i=1}^m{(x^{(i)}-\bar{x})^2}}
$$

$$
b = \bar{y}-a\bar{x}
$$

### 自实现

- 循环

简单实现

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.array([1., 2., 3., 4., 5.])
y = np.array([1., 3., 2., 3., 5.])

x_mean = np.mean(x)
y_mean = np.mean(y)
num = 0.0  # 分子
d = 0.0  # 分母
for x_i, y_i in zip(x, y):
  	num += (x_i-x_mean)*(y_i-y_mean)
    d += (x_i-x_mean)**2
a = num / d
b = y_mean - a * x_mean
y_hat = a * x + b

x_predict = 6
y_predict = a * x_predict + b
print(y_predict, a, b)
```

类创建

```python
# SimpleLinearRegression.py
import numpy as np
from .metrics import r2_score


class SimpleLinearRegression1:

    def __init__(self):
        """初始化Simple Linear Regression模型"""
        self.a_ = None
        self.b_ = None

    def fit(self, x_train, y_train):
        """根据训练数据集x_train, y_train训练Simple Linear Regression模型"""
        assert x_train.ndim == 1, \
            "Simple Linear Regressor can only solve single feature training data."
        assert len(x_train) == len(y_train), \
            "the size of x_train must be equal to the size of y_train"

        x_mean = np.mean(x_train)
        y_mean = np.mean(y_train)
				num = 0.0
        d = 0.0
        for x, y in zip(x_train, y_train):
            num += (x - x_mean) * (y - y_mean)
          	d += (x - x_mean) ** 2
        self.a_ = num / d
        self.b_ = y_mean - self.a_ * x_mean

        return self

    def predict(self, x_predict):
        """给定待预测数据集x_predict，返回表示x_predict的结果向量"""
        assert x_predict.ndim == 1, \
            "Simple Linear Regressor can only solve single feature training data."
        assert self.a_ is not None and self.b_ is not None, \
            "must fit before predict!"

        return np.array([self._predict(x) for x in x_predict])

    def _predict(self, x_single):
        """给定单个待预测数据x，返回x的预测结果值"""
        return self.a_ * x_single + self.b_

    def score(self, x_test, y_test):
        """根据测试数据集 x_test 和 y_test 确定当前模型的准确度"""

        y_predict = self.predict(x_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return "SimpleLinearRegression()"
```

使用

```python
from playML.SimpleLinearRegression import SimpleLinearRegression1

reg1 = SimpleLinearRegression1()
reg1.fit(x, y)
y_predict = reg1.predict(np.array([x_predict]))
print(y_predict)
print(reg1.a_)
print(reg1.b_)
```

- 向量化

```python
# SimpleLinearRegression.py
import numpy as np
from .metrics import r2_score


class SimpleLinearRegression:

    def __init__(self):
        """初始化Simple Linear Regression模型"""
        self.a_ = None
        self.b_ = None

    def fit(self, x_train, y_train):
        """根据训练数据集x_train, y_train训练Simple Linear Regression模型"""
        assert x_train.ndim == 1, \
            "Simple Linear Regressor can only solve single feature training data."
        assert len(x_train) == len(y_train), \
            "the size of x_train must be equal to the size of y_train"

        x_mean = np.mean(x_train)
        y_mean = np.mean(y_train)

        self.a_ = (x_train - x_mean).dot(y_train - y_mean) / (x_train - x_mean).dot(x_train - x_mean)
        self.b_ = y_mean - self.a_ * x_mean

        return self

    def predict(self, x_predict):
        """给定待预测数据集x_predict，返回表示x_predict的结果向量"""
        assert x_predict.ndim == 1, \
            "Simple Linear Regressor can only solve single feature training data."
        assert self.a_ is not None and self.b_ is not None, \
            "must fit before predict!"

        return np.array([self._predict(x) for x in x_predict])

    def _predict(self, x_single):
        """给定单个待预测数据x，返回x的预测结果值"""
        return self.a_ * x_single + self.b_

    def score(self, x_test, y_test):
        """根据测试数据集 x_test 和 y_test 确定当前模型的准确度"""

        y_predict = self.predict(x_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return "SimpleLinearRegression()"

```

## 多元线性回归

### 正规方程解

假设找到了最佳拟合的直线方程
$$
y = \theta_0 + \theta_1X_1 + \theta_2X + \cdots + \theta_nX_n
$$
对于每一个样本点$x^{(i)}$，根据直线方程，预测值为
$$
\hat{y}^{(i)} = \theta_0 + \theta_1X_1^{(i)} + \theta_2X_2^{(i)} + \cdots + \theta_nX_n^{(i)}
$$
可转换为
$$
\theta = (\theta_0,\theta_1,\theta_2\cdots \theta_n)^T
$$

$$
X^{(i)} = (X_0^{(i)},X_1^{(i)},X_2^{(i)},\cdots,X_n^{(i)}), X_0^{(i)}\equiv1
$$

$$
\hat{y}^{(i)} = X^{(i)}\cdot\theta
$$

可得到
$$
X_b = \left( \begin{array}{ccc} 1 & X_1^{(1)} & X_2^{(1)} & \ldots & X_n^{(1)} \\ 1 &  X_1^{(2)} & x_2^{(2)} & \ldots & X_n^{(2)} \\ \cdots &&&& \cdots \\ 1 & X_1^{(m)} & X_2^{(m)} &\ldots & X_n^{(m)}\end{array} \right)
$$

$$
\theta = \left( \begin{array}{ccc} \theta_0 \\ \theta_1 \\ \theta_2 \\ \ldots\\ \theta_n \end{array} \right)
$$

$$
\hat{y} = X_b\cdot\theta
$$

则损失函数为
$$
\sum_{i=1}^m{(y^{(i)}-\hat{y}^{(i)})^2}=\sum_{i=1}^m{(y^{(i)}-X_b\cdot\theta)^2}=(y-X_b\cdot\theta)^T(y-X_b\cdot\theta)=J(\theta)
$$
求$J(a,b)$函数的最小值，采用求导
$$
\frac{\partial{J(\theta)}}{\partial{\theta}}
$$
求得$\theta$为(正规方程解)
$$
\theta = (X_b^TX_b)^{-1}X_b^Ty
$$
时间复杂度过高:O(n^3)，优化后也有O(n^2.4)

无量纲，故不需归一化

与梯度下降比较：

优点：不需要学习率、不需要迭代、不需要归一化、可以直接求出结果

缺点：当特征向量过多（如>10000），则计算时间过长；不适用其他的模型算法

####  自实现

类创建

```python
# LinearRegression.py
import numpy as np
from .metrics import r2_score

class LinearRegression:

    def __init__(self):
        """初始化Linear Regression模型"""
        self.coef_ = None  # 系数
        self.intercept_ = None  # 截距
        self._theta = None

    def fit_normal(self, X_train, y_train):
        """根据训练数据集X_train, y_train训练Linear Regression模型"""
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)  # 解正规方程

        self.intercept_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self

    def predict(self, X_predict):
        """给定待预测数据集X_predict，返回表示X_predict的结果向量"""
        assert self.intercept_ is not None and self.coef_ is not None, \
            "must fit before predict!"
        assert X_predict.shape[1] == len(self.coef_), \
            "the feature number of X_predict must be equal to X_train"

        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        return X_b.dot(self._theta)

    def score(self, X_test, y_test):
        """根据测试数据集 X_test 和 y_test 确定当前模型的准确度"""

        y_predict = self.predict(X_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return "LinearRegression()"
```

使用

```python
import numpy as np
from sklearn import datasets
from playML.model_selection import train_test_split
from playML.LinearRegression import LinearRegression

boston = datasets.load_boston()
X = boston.data
y = boston.target

X = X[y < 50.0]
y = y[y < 50.0]

X_train, X_test, y_train, y_train = train_test_split(x, y, sees=666)

reg = LinearRegression()
reg.fit_noraml(X_train, y_train)
reg.score(X_test, y_test)
print(reg.coef_, reg.interception_)
```

#### sklearn

```python
from sklearn.linear_model import LinearRegression
 
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
lin_reg.score(X_test, y_test)
print(lin_reg.intercept_, lin_reg.coef_)
print(np.argsort(lin_reg.coef_))
```

#### tensorflow

- 求逆矩阵

$Ax=b$ 求解 $x=(A^TA)^{-1}A^Tb$

```python
# Given Ax=b, solving for x:
#  x = (t(A) * A)^(-1) * t(A) * b
#  where t(A) is the transpose of A

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.python.framework import ops
ops.reset_default_graph()

# Create graph
sess = tf.Session()

# Create the data
x_vals = np.linspace(0, 10, 100)
y_vals = x_vals + np.random.normal(0, 1, 100)

# Create design matrix
x_vals_column = np.transpose(np.matrix(x_vals))
ones_column = np.transpose(np.matrix(np.repeat(1, 100)))
A = np.column_stack((x_vals_column, ones_column))

# Create b matrix
b = np.transpose(np.matrix(y_vals))

# Create tensors
A_tensor = tf.constant(A)
b_tensor = tf.constant(b)

# Matrix inverse solution
tA_A = tf.matmul(tf.transpose(A_tensor), A_tensor)
tA_A_inv = tf.matrix_inverse(tA_A)
product = tf.matmul(tA_A_inv, tf.transpose(A_tensor))
solution = tf.matmul(product, b_tensor)

solution_eval = sess.run(solution)

# Extract coefficients
slope = solution_eval[0][0]
y_intercept = solution_eval[1][0]

print('slope: ' + str(slope))
print('y_intercept: ' + str(y_intercept))

# Get best fit line
best_fit = []
for i in x_vals:
  best_fit.append(slope*i+y_intercept)

# Plot the results
plt.plot(x_vals, y_vals, 'o', label='Data')
plt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3)
plt.legend(loc='upper left')
plt.show()
```

- 矩阵分解

当矩阵比较大时效率低，使用矩阵分解，使用内建的Cholesky矩阵分解法，将一个矩阵分解为上三角矩阵和下三角矩阵。

$Ax=b$ 等价于 $LL^{'}x=b$，先求解$Ly=b$，然后求解$L^{'}x=y$ 得到系数矩阵$x$

```python
# Given Ax=b, and a Cholesky decomposition such that
#  A = L*L' then we can get solve for x via
# 1) L*y=t(A)*b
# 2) L'*x=y

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.python.framework import ops
ops.reset_default_graph()

# Create graph
sess = tf.Session()

# Create the data
x_vals = np.linspace(0, 10, 100)
y_vals = x_vals + np.random.normal(0, 1, 100)

# Create design matrix
x_vals_column = np.transpose(np.matrix(x_vals))
ones_column = np.transpose(np.matrix(np.repeat(1, 100)))
A = np.column_stack((x_vals_column, ones_column))

# Create b matrix
b = np.transpose(np.matrix(y_vals))

# Create tensors
A_tensor = tf.constant(A)
b_tensor = tf.constant(b)

# Find Cholesky Decomposition
tA_A = tf.matmul(tf.transpose(A_tensor), A_tensor)
L = tf.cholesky(tA_A)

# Solve L*y=t(A)*b
tA_b = tf.matmul(tf.transpose(A_tensor), b)
sol1 = tf.matrix_solve(L, tA_b)

# Solve L' * y = sol1
sol2 = tf.matrix_solve(tf.transpose(L), sol1)

solution_eval = sess.run(sol2)

# Extract coefficients
slope = solution_eval[0][0]
y_intercept = solution_eval[1][0]

print('slope: ' + str(slope))
print('y_intercept: ' + str(y_intercept))

# Get best fit line
best_fit = []
for i in x_vals:
  best_fit.append(slope*i+y_intercept)

# Plot the results
plt.plot(x_vals, y_vals, 'o', label='Data')
plt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3)
plt.legend(loc='upper left')
plt.show()
```

## 批量随机梯度下降

### 批量梯度下降

对于梯度下降的每一步，都是基于完整的训练集。

- 特点

注意：数据需要归一化，否则可能不收敛
优点： 对于多维度数据，梯度下降比正规方程速度更快
缺点：由于要计算所有样本进行计算，大批量样本速度会慢

- 超参数学习率

对于学习率，若设置过低，最终虽能找到最优解，但是需要太长时间；若设置过高，可能跳出了最优解。

为了找到合适的学习率，可以使用网格搜索，但是需要限制迭代次数。

若是设置迭代次数过低，则可能在离最优解很远时就停止；若是设置过高，则模型达到最优解后，继续迭代参数不再变化，浪费时间。

一般在开始时设置一个比较大的迭代次数，但是当梯度向量的值变得很微小时中断算法，也就是当它的范数变得低于 $\epsilon$ 时，因为这时梯度下降已经达到了最小值。

#### 原理
$$
-\eta\nabla{J}
$$

其中$\nabla{J}$为
$$
\nabla{J} = (\frac{\partial{J}}{\partial{\theta_0}},\frac{\partial{J}}{\partial{\theta_1}},\ldots,\frac{\partial{J}}{\partial{\theta_n}})
$$
应用至损失函数
$$
\sum_{i=1}^m{(y^{(i)}-\hat{y}^{(i)})^2} = \sum_{i=1}^m{(y^{(i)}-X_b\cdot\theta)^2}
$$
使线性回归中的损失函数更改为
$$
\frac{1}{m}\sum_{i=1}^m{(y^{(i)}-\hat{y}^{(i)})^2}=\frac{1}{m}\sum_{i=1}^m{(y^{(i)}-X_b\cdot\theta)^2}=J(\theta) = MSE(y, \hat{y})
$$

$$
\nabla{J(\theta)} = \left( \begin{array}{ccc} {\partial{J}}/{\partial{\theta_0}} \\ {\partial{J}}/{\partial{\theta_1}} \\ \ldots\\ {\partial{J}}/{\partial{\theta_n}} \end{array} \right)=\frac{2}{m}\cdot\left( \begin{array}{ccc} {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}} \\ {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_1^{(i)}}} \\ \ldots\\ {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_n^{(i)}}} \end{array} \right)
$$

#### 自实现

- 循环

模拟梯度下降

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
x = 2 * np.random.random(size=100)
y = x * 3. + 4. + np.random.normal(size=100)
X = x.reshape(-1, 1)

def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
    except:
        return float('inf')

def dJ(theta, X_b, y):
  	res = enp.empty(len(theta))
    res[0] = np.sum(X_b.dot(theta) - y)
    for i in range(1, len(theta)):
      	res[i] = (X_b.dot(theta) - y).doct(X-b[:,i])
    return res * 2 / len(X_b)
  
def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):

    theta = initial_theta
    cur_iter = 0

    while cur_iter < n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
            break
        cur_iter += 1

    return theta
  
X_b = np.hstack([np.ones(len(X), 1), X])
initial_theta = np.zeros(X-b.shape[1])
eta = 0.01
theta = gradient_descent(X_b, y, initial_theta, eta)
print(theta[0], theta[1:])
```

类创建

```python
# LinearRegression.py
import numpy as np
from .metrics import r2_score

class LinearRegression:

    def __init__(self):
        """初始化Linear Regression模型"""
        self.coef_ = None
        self.intercept_ = None
        self._theta = None

    def fit_gd(self, X_train, y_train, eta=0.01, n_iters=1e4):
        """根据训练数据集X_train, y_train, 使用梯度下降法训练Linear Regression模型"""
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"

        def J(theta, X_b, y):
            try:
                return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
            except:
                return float('inf')

        def dJ(theta, X_b, y):
            res = np.empty(len(theta))
            res[0] = np.sum(X_b.dot(theta) - y)
            for i in range(1, len(theta)):
                res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])
            return res * 2 / len(X_b)

        def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):

            theta = initial_theta
            cur_iter = 0

            while cur_iter < n_iters:
                gradient = dJ(theta, X_b, y)
                last_theta = theta
                theta = theta - eta * gradient
                if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
                    break

                cur_iter += 1

            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        self._theta = gradient_descent(X_b, y_train, initial_theta, eta, n_iters)

        self.intercept_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self

    def predict(self, X_predict):
        """给定待预测数据集X_predict，返回表示X_predict的结果向量"""
        assert self.intercept_ is not None and self.coef_ is not None, \
            "must fit before predict!"
        assert X_predict.shape[1] == len(self.coef_), \
            "the feature number of X_predict must be equal to X_train"

        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        return X_b.dot(self._theta)

    def score(self, X_test, y_test):
        """根据测试数据集 X_test 和 y_test 确定当前模型的准确度"""

        y_predict = self.predict(X_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return "LinearRegression()"

```

使用

```python
# 数据集
import numpy as np
from sklearn import datasets
from playML.model_selection import train_test_split
from playML.LinearRegression import LinearRegression

boston = datasets.load_boston()
X = boston.data
y = boston.target

X = X[y < 50.0]
y = y[y < 50.0]

X_train, X_test, y_train, y_train = train_test_split(x, y, sees=666)

lin_reg = LinearRegression()
lin_reg.fit_gd(X, y)
lin_req.coef_
lin_req.intercept_
```

- 向量化

$$
\nabla{J(\theta)} = \left( \begin{array}{ccc} {\partial{J}}/{\partial{\theta_0}} \\ {\partial{J}}/{\partial{\theta_1}} \\ \ldots\\ {\partial{J}}/{\partial{\theta_n}} \end{array} \right)=\frac{2}{m}\cdot\left( \begin{array}{ccc} {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_0^{(i)}}} \\ {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_1^{(i)}}} \\ \ldots\\ {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_n^{(i)}}} \end{array} \right)
$$

转换为
$$
\frac{2}{m}\cdot(X_b^{(1)}\theta-y^{(1)},X_b^{(1)}\theta-y^{(1)},\ldots,X_b^{(m)}\theta-y^{(m)})\cdot\left( \begin{array}{ccc} X_0^{(1)} & X_1^{(1)} & \ldots & X_n^{(1)} \\ X_0^{(2)} & X_1^{(2)} & \ldots & X_n^{(2)} \\ \ldots\\ X_0^{(m)} & X_1^{(m)} & \ldots & X_n^{(m)} \end{array} \right) = \frac{2}{m}\cdot(X_b\theta-y)^T\cdot{X_b}
$$
行列转换
$$
\nabla{J(\theta)}=\frac{2}{m}\cdot{X_b^T}\cdot{(X_b\theta-y)}
$$


实现

```python
# LinearRegression.py
import numpy as np
from .metrics import r2_score

class LinearRegression:

    def __init__(self):
        """初始化Linear Regression模型"""
        self.coef_ = None
        self.intercept_ = None
        self._theta = None

    def fit_gd(self, X_train, y_train, eta=0.01, n_iters=1e4):
        """根据训练数据集X_train, y_train, 使用梯度下降法训练Linear Regression模型"""
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"

        def J(theta, X_b, y):
            try:
                return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
            except:
                return float('inf')

        def dJ(theta, X_b, y):
            # res = np.empty(len(theta))
            # res[0] = np.sum(X_b.dot(theta) - y)
            # for i in range(1, len(theta)):
            #     res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])
            # return res * 2 / len(X_b)
            return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(X_b)

        def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):

            theta = initial_theta
            cur_iter = 0

            while cur_iter < n_iters:
                gradient = dJ(theta, X_b, y)
                last_theta = theta
                theta = theta - eta * gradient
                if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
                    break

                cur_iter += 1

            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        self._theta = gradient_descent(X_b, y_train, initial_theta, eta, n_iters)

        self.intercept_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self

    def predict(self, X_predict):
        """给定待预测数据集X_predict，返回表示X_predict的结果向量"""
        assert self.intercept_ is not None and self.coef_ is not None, \
            "must fit before predict!"
        assert X_predict.shape[1] == len(self.coef_), \
            "the feature number of X_predict must be equal to X_train"

        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        return X_b.dot(self._theta)

    def score(self, X_test, y_test):
        """根据测试数据集 X_test 和 y_test 确定当前模型的准确度"""

        y_predict = self.predict(X_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return "LinearRegression()"
```

使用

```python
# 数据集
import numpy as np
from sklearn import datasets
from playML.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from playML.LinearRegression import LinearRegression

boston = datasets.load_boston()
X = boston.data
y = boston.target

X = X[y < 50.0]
y = y[y < 50.0]

X_train, X_test, y_train, y_train = train_test_split(x, y, sees=666)

standardScaler = StandardSclar()
standarScaler.fit(X_train)
X_train_standard = standardScalar.transform(X_train)

lin_reg = LinearRegression()
lin_reg.fit_gd(X_train_standard, y_train)
lin_req.coef_
lin_req.intercept_
X_test_standard = standardScalar.transform(X_test)
lin_req.score(X_test_standard, y_test)
```

### 随机梯度下降

在计算每一步的梯度时，在训练集中随机选择一个实例，并且仅基于该单歌实例来计算梯度。

- 特点

对于样本数量过多，使用梯度下降可能会较慢，使用随机梯度下降进行优化

具有一定不可预知性，并不是一直在下降，可能有部分上升

可能跳出局部最优解，但缺点是永远定位不出最小值。

可用的解决方法是降低学习率：开始的步长比较大，这样有助于快速进展和逃离局部最小值，然后越来越小，让算法尽量靠近全局最小值。这个过程叫做**模拟退火**。确定每个迭代学习率的函数叫做学习计划。


#### 原理

$$
\nabla{J(\theta)} = \left( \begin{array}{ccc} {\partial{J}}/{\partial{\theta_0}} \\ {\partial{J}}/{\partial{\theta_1}} \\ \ldots\\ {\partial{J}}/{\partial{\theta_n}} \end{array} \right)=\frac{2}{m}\cdot\left( \begin{array}{ccc} {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_0^{(i)}}} \\ {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_1^{(i)}}} \\ \ldots\\ {\sum_{i=1}^m{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_n^{(i)}}} \end{array} \right)
$$

可以选择一项作为搜索方向
$$
2\cdot\left( \begin{array}{ccc} {{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_0^{(i)}}} \\ {{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_1^{(i)}}} \\ \ldots\\ {{(X_b^{(i)}\theta-y^{(i)})}\cdot{X_n^{(i)}}} \end{array} \right)=2\cdot(X_b^{(i)})^T\cdot(X_b^{(i)}-y^{(i)})
$$
对于$\eta $的值
$$
\eta = \frac{a}{iters+b}
$$
#### 实现

- 自定义

```python
import numpy as np
import matplotlib.pyplot as plt

m = 100000
np.random.seed(666)
x = 2 * np.random.random(size=m)
y = x * 4. + 3. + np.random.normal(0, 3, size=m)
X = x.reshape(-1, 1)

def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
    except:
        return float('inf')

def dJ_sgd(theta, X_b_i, y_i):
    return X_b_i.T.dot(X_b_i.dot(theta) - y_i) * 2.
  
def sgd(X_b, y, initial_theta, n_iters):
  	t0 = 5
    t1 = 50
    def learning_rate(t):
      	return t0 / (t + t1)
    theta = initial_theta
    for cur_iter in range(n_iters):
      	rand_i = np.random.randint(len(X_b))
        grdient = dJ_sgd(theta, X_b[rand_i], y[rand_i])
        theta = theta - learning_rate(cur_iter) * grdient
    return theta
  
X_b = np.hstack([np.ones(len(X), 1), X])
initial_theta = np.zeros(X-b.shape[1])
theta = sgd(X_b, y, initial_theta, n_iters=len(X_b)//3)
print(theta[0], theta[1:])
```

- 类创建

```python
import numpy as np
from .metrics import r2_score

class LinearRegression:

    def __init__(self):
        """初始化Linear Regression模型"""
        self.coef_ = None
        self.intercept_ = None
        self._theta = None

    def fit_sgd(self, X_train, y_train, n_iters=5, t0=5, t1=50):
        """根据训练数据集X_train, y_train, 使用梯度下降法训练Linear Regression模型"""
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"
        assert n_iters >= 1

        def dJ_sgd(theta, X_b_i, y_i):
            return X_b_i * (X_b_i.dot(theta) - y_i) * 2.

        def sgd(X_b, y, initial_theta, n_iters, t0=5, t1=50):

            def learning_rate(t):
                return t0 / (t + t1)

            theta = initial_theta
            m = len(X_b)

            for cur_iter in range(n_iters):
                indexes = np.random.permutation(m)
                X_b_new = X_b[indexes]
                y_new = y[indexes]
                for i in range(m):
                    gradient = dJ_sgd(theta, X_b_new[i], y_new[i])
                    theta = theta - learning_rate(cur_iter * m + i) * gradient

            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.random.randn(X_b.shape[1])
        self._theta = sgd(X_b, y_train, initial_theta, n_iters, t0, t1)

        self.intercept_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self

    def predict(self, X_predict):
        """给定待预测数据集X_predict，返回表示X_predict的结果向量"""
        assert self.intercept_ is not None and self.coef_ is not None, \
            "must fit before predict!"
        assert X_predict.shape[1] == len(self.coef_), \
            "the feature number of X_predict must be equal to X_train"

        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        return X_b.dot(self._theta)

    def score(self, X_test, y_test):
        """根据测试数据集 X_test 和 y_test 确定当前模型的准确度"""

        y_predict = self.predict(X_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return "LinearRegression()"

```

使用

```python
# 模拟
import numpy as np
import matplotlib.pyplot as plt
from playML.LinearRegression import LinearRegression

m = 100000
np.random.seed(666)
x = 2 * np.random.random(size=m)
y = x * 4. + 3. + np.random.normal(0, 3, size=m)
X = x.reshape(-1, 1)

lin_req = LinearRegression()
lin_req.fit_sgd(X, y, n_iters=2)
print(lin_req.coef_)
print(lin_req.intercept_)

# 数据集
from sklearn import datasets
from playML.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from playML.LinearRegression import LinearRegression

boston = datasets.load_boston()
X = boston.data
y = boston.target
X = X[y < 50.0]
y = y[y < 50.0]

X_train, X_test, y_train, y_test = train_test_split(X, y, sed=666)

standardScaler = StandardScalar()
standardScaler.fit(X_train)
X_train_standard = standardScaler.transform(X_train)
X_test_standard = standardScaler.transform(X_test)

lin_req = LinearRegression()
lin_req.fit_sgd(X_train_standard, y_train, n_ites=5)
lin_req.score(X_test_standard, y_test)
```

#### sklearn

实现更为复杂，效率更为优化，只能用于线性回归

```python
from sklearn.linear_model import SGDRegressor

sgd_req = SGDRegressor()
# sgd_req = SGDRegressor(n_iter=100)
sgd_req.fit(X_train_standard, y_train, n_ites=5)
sgd_req.score(X_test_standard, y_test)
```

### 小批量梯度下降

每一步的梯度计算，既不是基于整个训练集也不是基于单个示例，而是基于一小部分随机的实例集也就是小批量。

相比随机梯度下降，小批量梯度下降的主要优势在于可以从矩阵运算的硬件优化中获得显著的性能提升，特别是需要用到图形处理器时。

线性回归算法比较

| 算法           | m很大 | 是否支持核外 | n很大 | 超参数   | 是否需要缩放 | scikit-learn     |
| -------------- | ----- | ------------ | ----- | -------- | ------------ | ---------------- |
| 标准方程       | 快    | 否           | 慢    | 0        | 否           | LinearRegression |
| 批量梯度下降   | 慢    | 否           | 快    | 2        | 是           | n/a              |
| 随机梯度下降   | 快    | 是           | 快    | $\geq 2$ | 是           | SGDRegressor     |
| 小批量梯度下降 | 快    | 是           | 快    | $\geq 2$ | 是           | n/a              |

## 实现

### 自实现

```python
import numpy as np

"""
    这是线性回归算法的类，其中主要包括BGD、SGD、MBGD
    三种梯度下降算法和正则方程方法这4种实现算法。在调整
    梯度下降算法调整参数的损失函数为训练数据集的均方误差。
"""

class LinearRegression(object):
    def __init__(self,input_data,realresult,theta = None):
        """
        :param input_data: 输入数据
        :param realresult: 真实结果
        :param theta: 线性回归的参数，默认是None,即可以没有
        """
        # 获得输入数据集的形状
        row,col = np.shape(input_data)
        # 构造输入数据数组
        self.InputData = [0]*row
        # 给每组输入数据增添常数项1
        for (index,data) in enumerate(input_data):
            Data = [1.0]
            # 把input_data拓展到Data内，即把input_data的每一维数据添加到Data
            Data.extend(list(data))
            self.InputData[index] = Data
        self.InputData = np.array(self.InputData)
        # 构造输入数据对应的结果
        self.Result = realresult
        # thetha参数不为None时，利用thetha构造模型参数
        if theta is not None:
            self.Theta = theta
        else:
            # 随机生成服从标准正态分布的参数
            self.Theta = np.random.normal((col+1,1))

    def Cost(self):
        """
        这是计算损失函数的函数
        """
        # 在线性回归里的损失函数定义为真实结果与预测结果之间的均方误差
        # 首先计算输入数据的预测结果
        predict = self.InputData.dot(self.Theta).T
        # 计算真实结果与预测结果之间的均方误差
        cost = predict-self.Result.T
        cost = np.average(cost**2)
        return cost

    def BGD(self,alpha):
        """
        这是利用BGD算法进行一次迭代调整参数的函数
        :param alpha: 学习率
        """
        # 定义梯度增量数组
        gradient_increasment = []
        # 对输入的训练数据及其真实结果进行依次遍历
        for (input_data,real_result) in zip(self.InputData,self.Result):
            # 计算每组input_data的梯度增量，并放入梯度增量数组
            g = (real_result-input_data.dot(self.Theta))*input_data
            gradient_increasment.append(g)
        # 按列计算属性的平均梯度增量
        avg_g = np.average(gradient_increasment,0)
        # 改变平均梯度增量数组形状
        avg_g = avg_g.reshape((len(avg_g),1))
        # 更新参数Theta
        self.Theta = self.Theta + alpha*avg_g

    def SGD(self,alpha):
        """
        这是利用SGD算法进行一次迭代调整参数的函数
        :param alpha: 学习率
        """
        # 首先将数据集随机打乱，减小数据集顺序对参数调优的影响
        shuffle_sequence = self.Shuffle_Sequence()
        self.InputData = self.InputData[shuffle_sequence]
        self.Result = self.Result[shuffle_sequence]
        # 对训练数据集进行遍历，利用每组训练数据对参数进行调整
        for (input_data,real_result) in zip(self.InputData,self.Result):
            # 计算每组input_data的梯度增量
            g = (real_result-input_data.dot(self.Theta))*input_data
            # 调整每组input_data的梯度增量的形状
            g = g.reshape((len(g),1))
            # 更新线性回归模型参数
            self.Theta = self.Theta + alpha * g

    def Shuffle_Sequence(self):
        """
        这是在运行SGD算法或者MBGD算法之前，随机打乱后原始数据集的函数
        """
        # 首先获得训练集规模，之后按照规模生成自然数序列
        length = len(self.InputData)
        random_sequence = list(range(length))
        # 利用numpy的随机打乱函数打乱训练数据下标
        random_sequence = np.random.permutation(random_sequence)
        return random_sequence          # 返回数据集随机打乱后的数据序列

    def MBGD(self,alpha,batch_size):
        """
        这是利用MBGD算法进行一次迭代调整参数的函数
        :param alpha: 学习率
        :param batch_size: 小样本规模
        """
        # 首先将数据集随机打乱，减小数据集顺序对参数调优的影响
        shuffle_sequence = self.Shuffle_Sequence()
        self.InputData = self.InputData[shuffle_sequence]
        self.Result = self.Result[shuffle_sequence]
        # 遍历每个小批量样本
        for start in np.arange(0, len(shuffle_sequence), batch_size):
            # 判断start+batch_size是否大于数组长度，
            # 防止最后一组小样本规模可能小于batch_size的情况
            end = np.min([start + batch_size, len(shuffle_sequence)])
            # 获取训练小批量样本集及其标签
            mini_batch = shuffle_sequence[start:end]
            Mini_Train_Data = self.InputData[mini_batch]
            Mini_Train_Result = self.Result[mini_batch]
            # 定义梯度增量数组
            gradient_increasment = []
            # 对小样本训练集进行遍历，利用每个小样本的梯度增量的平均值对模型参数进行更新
            for (data,result) in zip(Mini_Train_Data,Mini_Train_Result):
                # 计算每组input_data的梯度增量，并放入梯度增量数组
                g = (result - data.dot(self.Theta)) * data
                gradient_increasment.append(g)
            # 按列计算每组小样本训练集的梯度增量的平均值，并改变其形状
            avg_g = np.average(gradient_increasment, 0)
            avg_g = avg_g.reshape((len(avg_g), 1))
            # 更新模型参数theta
            self.Theta = self.Theta + alpha * avg_g

    def getNormalEquation(self):
        """
        这是利用正规方程计算模型参数Thetha
        """
        """
          0.001*np.eye(np.shape(self.InputData.T))是
          防止出现原始XT的行列式为0，即防止原始XT不可逆
        """
        # 获得输入数据数组形状
        col,rol = np.shape(self.InputData.T)
        # 计算输入数据矩阵的转置
        XT = self.InputData.T+0.001*np.eye(col,rol)
        # 计算矩阵的逆
        inv = np.linalg.inv(XT.dot(self.InputData))
        # 计算模型参数Thetha
        self.Theta = inv.dot(XT.dot(self.Result))

    def train_BGD(self,iter,alpha):
        """
        这是利用BGD算法迭代优化的函数
        :param iter: 迭代次数
        :param alpha: 学习率
        """
        # 定义训练损失数组，记录每轮迭代的训练数据集的训练损失
        Cost = []
        # 开始进行迭代训练
        for i in range(iter):
            # 利用学习率alpha，结合BGD算法对模型进行训练
            self.BGD(alpha)
            # 记录每次迭代的训练损失
            Cost.append(self.Cost())
        Cost = np.array(Cost)
        return Cost

    def train_SGD(self,iter,alpha):
        """
        这是利用SGD算法迭代优化的函数
        :param iter: 迭代次数
        :param alpha: 学习率
        """
        # 定义训练损失数组，记录每轮迭代的训练数据集的训练损失
        Cost = []
        # 开始进行迭代训练
        for i in range(iter):
            # 利用学习率alpha，结合SGD算法对模型进行训练
            self.SGD(alpha)
            # 记录每次迭代的训练损失
            Cost.append(self.Cost())
        Cost = np.array(Cost)
        return Cost

    def train_MBGD(self,iter,batch_size,alpha):
        """
        这是利用MBGD算法迭代优化的函数
        :param iter: 迭代次数
        :param batch_size: 小样本规模
        :param alpha: 学习率
        """
        # 定义训练损失数组，记录每轮迭代的训练数据集的训练损失
        Cost = []
        # 开始进行迭代训练
        for i in range(iter):
            # 利用学习率alpha，结合MBGD算法对模型进行训练
            self.MBGD(alpha,batch_size)
            # 记录每次迭代的训练损失
            Cost.append(self.Cost())
        Cost = np.array(Cost)
        return Cost

    def test(self,test_data):
        """
        这是对测试数据集的线性回归预测函数
        :param test_data: 测试数据集
        """
        # 定义预测结果数组
        predict_result = []
        # 对测试数据进行遍历
        for data in test_data:
            # 预测每组data的结果
            predict_result.append(self.predict(data))
        predict_result = np.array(predict_result)
        return predict_result

    def predict(self,data):
        """
        这是对一组测试数据预测的函数
        :param data: 测试数据
        """
        # 对测试数据加入1维特征，以适应矩阵乘法
        tmp = [1.0]
        tmp.extend(data)
        data = np.array(tmp)
        # 计算预测结果,计算结果形状为(1,)，为了分析数据的方便
        # 这里只返矩阵的第一个元素
        predict_result = data.dot(self.Theta)[0]
        return predict_result
```

### sklearn

```python
# 标准方程
from sklearn.linear_model import LinearRegression
# 随机梯度
from sklearn.linear_model import SGDRegressor
```

### tensorflow

```python
# y = Ax + b
#
# We will use the iris data, specifically:
#  y = Sepal Length
#  x = Petal Width

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from sklearn import datasets
from tensorflow.python.framework import ops

ops.reset_default_graph()
tf.compat.v1.disable_eager_execution()

# Create graph
sess = tf.compat.v1.Session()

# Load the data
# iris.data = [(Sepal Length, Sepal Width, Petal Length, Petal Width)]
iris = datasets.load_iris()
x_vals = np.array([x[3] for x in iris.data])
y_vals = np.array([y[0] for y in iris.data])

```

随机

```python
# Initialize placeholders
x_data = tf.compat.v1.placeholder(shape=[1], dtype=tf.float32)
y_target = tf.compat.v1.placeholder(shape=[1], dtype=tf.float32)

# Create variables for linear regression
A = tf.Variable(tf.compat.v1.random_normal(shape=[1]))
b = tf.Variable(tf.compat.v1.random_normal(shape=[1]))

# Declare model operations
model_output = tf.add(tf.multiply(x_data, A), b)

# Declare loss function (L2 loss)
loss = tf.reduce_mean(tf.square(y_target - model_output))

# Declare optimizer
my_opt = tf.compat.v1.train.GradientDescentOptimizer(0.05)
train_step = my_opt.minimize(loss)

# Initialize variables
init = tf.compat.v1.global_variables_initializer()
sess.run(init)

# Training loop
loss_vec = []
for i in range(100):
    rand_index = np.random.choice(len(x_vals))
    rand_x = [x_vals[rand_index]]
    rand_y = [y_vals[rand_index]]
    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})
    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})
    loss_vec.append(temp_loss)
    if (i+1)%25==0:
        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b)))
        print('Loss = ' + str(temp_loss))

# Get the optimal coefficients
[slope] = sess.run(A)
[y_intercept] = sess.run(b)

# Get best fit line
best_fit = []
for i in x_vals:
  best_fit.append(slope*i+y_intercept)

# Plot the result
plt.plot(x_vals, y_vals, 'o', label='Data Points')
plt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3)
plt.legend(loc='upper left')
plt.title('Sepal Length vs Petal Width')
plt.xlabel('Petal Width')
plt.ylabel('Sepal Length')
plt.show()

# Plot loss over time
plt.plot(loss_vec, 'k-')
plt.title('L2 Loss per Generation')
plt.xlabel('Generation')
plt.ylabel('L2 Loss')
plt.show()
```

批量

```python
# Declare batch size
batch_size = 25

# Initialize placeholders
x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)
y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)

# Create variables for linear regression
A = tf.Variable(tf.random_normal(shape=[1,1]))
b = tf.Variable(tf.random_normal(shape=[1,1]))

# Declare model operations
model_output = tf.add(tf.matmul(x_data, A), b)

# Declare loss function (L2 loss)
loss = tf.reduce_mean(tf.square(y_target - model_output))

# Declare optimizer
my_opt = tf.train.GradientDescentOptimizer(0.05)
train_step = my_opt.minimize(loss)

# Initialize variables
init = tf.global_variables_initializer()
sess.run(init)

# Training loop
loss_vec = []
for i in range(100):
    rand_index = np.random.choice(len(x_vals), size=batch_size)
    rand_x = np.transpose([x_vals[rand_index]])
    rand_y = np.transpose([y_vals[rand_index]])
    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})
    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})
    loss_vec.append(temp_loss)
    if (i+1)%25==0:
        print('Step #' + str(i+1) + ' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b)))
        print('Loss = ' + str(temp_loss))

# Get the optimal coefficients
[slope] = sess.run(A)
[y_intercept] = sess.run(b)

# Get best fit line
best_fit = []
for i in x_vals:
  best_fit.append(slope*i+y_intercept)

# Plot the result
plt.plot(x_vals, y_vals, 'o', label='Data Points')
plt.plot(x_vals, best_fit, 'r-', label='Best fit line', linewidth=3)
plt.legend(loc='upper left')
plt.title('Sepal Length vs Petal Width')
plt.xlabel('Petal Width')
plt.ylabel('Sepal Length')
plt.show()

# Plot loss over time
plt.plot(loss_vec, 'k-')
plt.title('L2 Loss per Generation')
plt.xlabel('Generation')
plt.ylabel('L2 Loss')
plt.show()
```





