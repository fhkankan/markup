# 高斯过程

Gaussian Process

高斯过程是一种假设训练数据来自无限空间、并且各特征都符合高斯分布的有监督建模方法。高斯过程是一种概率模型，无论是回归或分类预测都以高斯分布标准差的方式给出预测置信区间估计。

## 随机过程

高斯过程来源于数学中的随机过程理论。随机过程是研究一组无限个随机变量内在规律的学科。

假设需要训练一个预测某城市在任意时间居民用电量的模型。简化期间，在改模型中可以用当前温度、年内天数、当天时间作为数据特征，将居民用电量作为目标标签数值。用邮件度学习的思维，首先需要收集历史用电数据，比如在2020年的每天中午12:00收集并记录数据，这样得到一组包含N=365条数据的训练数据。

但显然用电在一天内的不同时间段是有变化的，随着精度要求的增加，采样的训练数据可以无限增加。如果把每次采样的目标值用电量y都看成一个随机变量，那么单条采样就是一个随机分布事件的结果，N条数据就是多个堆积分布采样的结果，而整个被学习空间就是由无数个随机变量构成的随机过程了！

要把实实在在采样得到的Y值看成随机变量，主要是由于：1.所有数据的产生本身就是随机的，2.数据的采集是有噪声存在的。对于这样的场景，无论如何都不可能给出一个精确值的预测，即使给出后碰巧符合也是运气不错。更合理的预测方式应该是一个置信区间预测。

## 无限维高斯分布

如果把每个随机变量都用高斯分布进行建模，那么整个随机过程就是一个高斯过程了。高斯过程能成为随机过程最广泛的应用之一是因为高斯分布本省的诸多优良特性。

- 高斯分布的特点

1. 可标准化：一个高斯分布可由均值和标准差唯一确定，用符号$N(\mu, \sigma)$表示。并且任意高斯分布可以转化为用$\mu=0,\sigma=1$的标准正态分布表达。
2. 方便统计：高斯分布中约69.27%的样本落在$(\mu-\sigma, \mu+\sigma)$之间，约95%的样本落在$(\mu-2\sigma, \mu+2\sigma)$ 指尖，约有99%的样本落在$(\mu-3\sigma, \mu+3\sigma)$之间。
3. 多元高斯分布：n元高斯分布描述n个随机变量的联合概率分布，由均值向量$<\mu_1,\mu_2,\cdots,\mu_n>$和协方差矩阵$\Sigma$唯一确定，其中$\Sigma$是一个$n\times n$的矩阵，每个矩阵元素描述n个随机变量两两之间的协方差。
4. 和与差：设有任意两个独立的高斯分布$U,V$，那么它们的和与差也是高斯分布。
5. 部分与整体：多元高斯分布的条件分布仍然是多元高斯分布，也可理解为多元高斯分布的子集也是多元高斯分布

- 核函数

将高斯过程看成无限维的多元高斯分布，则机器学习的训练过程目标就是学习该无限维高斯分布的子集，也是一个多元高斯分布的参数：均值向量$<\mu_1,\mu_2,\cdots,\mu_n>$和协方差矩阵$\Sigma$。

协方差矩阵中的元素用于表征两两样本之间的协方差，核方法应用在了协方差矩阵上，使得多元高斯分布也具有了表征高维空间样本之间关系的能力，也就是具备了表征非线性数据的能力。此时的协方差矩阵可以表示为
$$
\Sigma = K_{XX} = \left( \begin{array}{ccc} k(x_1,x_1) & \ldots & k(x_1, x_N)  \\ \vdots & \ddots & \vdots \\ k(x_N,x_1) & \ldots & k(x_N, x_N)  \end{array} \right)
$$
其中，符号$K_{XX}$ 表示样本数据特征集$X$的核函数矩阵，用$k()$表示所选取的核函数，$x_1,x_2,\cdots, x_n$等是单个样本的特征向量。

- 预测

设样本目标值是Y，被预测的变量是$Y_*$，由高斯分布的特性可知，由训练数据与被预测数据组成的随机变量集合仍然符合多元高斯分布，即
$$
\left( \begin{array}{ccc} Y \\ Y_* \end{array} \right)\sim N \left( {\left( {\begin{array}{ccc} u \\ u_* \end{array}}\right)\left( {\begin{array}{ccc} K_{XX} &K_{X_*X} \\ K_{XX_*}&K_{X_*X_*} \end{array}}\right) }\right)
$$
其中，$u_*$是待求变量$Y_*$的均值，$K_{X_*X}$是样本数据与预测数据特征的协方差矩阵，$K_{X_*X_*}$是预测数据特征的协方差矩阵。

由多元高斯特性可知$Y_*$，也满足高斯分布$N(u_*, \Sigma)$，并且可以直接用公式推到出该分布的超参数
$$
u_* = K_{X_*X}^TK^{-1}Y\\
\Sigma = K_{X_*X_*}-K_{X_*X}^TK^{-1}K_{XX}
$$
该公式通过如下已知的数据直接计算出预测数据期望与方差：1.寻来呢数据的特征向量集X和目标向量Y；2.预测数据的特征向量集$X_*$;3.核函数$k()$及在训练过程中优化的核超参数

与大多数机器学习模型不同的是，由于高斯过程在预测过程中仍然需要用到原始训练数据，因此导致该方法通常在高维特征和超多训练样本的场景下显得运算效率低。但也正是因此，才能提供其他学习模型不具备的基于概率分布的预测。

- 白噪声处理

在建模中已经知道堆积过程需要考虑采样数据存在噪声的情况。用高斯分布的观点来看，就是在计算训练数据协方差矩阵$K_{XX}$的对角元素上增加噪声分量。因此协方差矩阵变为如下
$$
\Sigma = K_{XX} = \left( \begin{array}{ccc} k(x_1,x_1) & \ldots & k(x_1, x_N)  \\ \vdots & \ddots & \vdots \\ k(x_N,x_1) & \ldots & k(x_N, x_N)  \end{array} \right) + \alpha \left( \begin{array}{ccc} 1 & \ldots & 0  \\ \vdots & \ddots & \vdots \\ 0 & \ldots & 1  \end{array} \right)
$$
其中$\alpha$ 是模型训练者需要定义的噪声估计参数，该值越大模型抗噪声能力越强，但容易产生欠拟合。

## 实战

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, Product

# 原函数
def func(x):
    return x*np.sin(x) -x

# 数据
X = np.linspace(0, 10, 20)  # 20个训练样本数据的特征值
y = func(X) + np.random.normal(0, 0.5, X.shape[0])  # 样本目标值，并加入噪声
x = np.linspace(0, 10, 200)  # 测试样本特征值

# 定义两个核函数，并取它们的积
kernel = Product(C(0.1), RBF(10, (1e-2, 1e2)))

# 初始化模型：传入核函数对象，优化次数、噪声超参数
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3, alpha=0.3)
# 训练
gp.fit(X.reshape(-1, 1), y)
# 预测
y_pred, sigma = gp.predict(x.reshape(-1, 1), return_std=True)

fig = plt.figure()
plt.plot(x, (x), 'r:', label=u'$f(x=x\, \sin(x)-x$')
plt.plot(X, y, 'r.', markersize=10, label=u'Observations')
plt.plot(x, y_pred, 'b-', label = u'Prediction')
plt.fill(np.concatenate([x, x[::-1]]), np.concatenate([y_pred - 2*sigma, (y_pred + 2 * sigma)[::-1]]), alpha=0.3, fc='b', label='95% confidence')  # 填充置信区间
plt.legend(loc='lower left')
plt.show()
```

