# 稳健回归

线性回归对数据的异常值非常敏感，因为系数会使得平方误差实现最小化，因此超平面被迫靠近异常值产生更高的误差。但是，在大多数实际应用中，期望能够区分属于正常数据和异常值的点。

## 随机抽样一致算法

线性回归的一类常见问题是由于异常值的存在而引起的，由于普通最小二乘法考虑了这些异常点，因而容易导致结果(相关的系数)出现偏差。

随机抽样一致算法（RANSAC）提供了一个方法来避免这个问题：在将数据集分解为有效数据和异常值后，通过后续迭代与回归器一起工作。该方法仅对有效样本进行训练（样本进行内部评估或通过调用方法`is_data_valid()` 进行样本评估），并重新评估所有样本，以验证它们是否仍然是有效数据或已变为异常值，该过程在固定次数的迭代之后或达到期望得分时结束。

```python
from __future__ import print_function

import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression, RANSACRegressor


# For reproducibility
np.random.seed(1000)

nb_samples = 200
nb_noise_samples = 150


def show_dataset(X, Y):
    fig, ax = plt.subplots(1, 1, figsize=(30, 25))

    ax.scatter(X, Y)
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.grid()

    plt.show()


if __name__ == '__main__':
    # Create dataset
    X = np.arange(-5, 5, 0.05)

    Y = X + 2
    Y += np.random.uniform(-0.5, 0.5, size=nb_samples)

    for i in range(nb_noise_samples, nb_samples):
        Y[i] += np.random.uniform(12, 15)

    # Show the dataset
    show_dataset(X, Y)

    # Create a linear regressor
    lr = LinearRegression(normalize=True)
    lr.fit(X.reshape(-1, 1), Y.reshape(-1, 1))
    print('Standard regressor: y = %.3fx + %.3f' % (lr.coef_, lr.intercept_))  # 有异常值，导致斜率很大

    # Create RANSAC regressor
    rs = RANSACRegressor(lr)
    rs.fit(X.reshape(-1, 1), Y.reshape(-1, 1))
    print('RANSAC regressor: y = %.3fx + %.3f' % (rs.estimator_.coef_, rs.estimator_.intercept_))  # 与无异常值数据集回归效果一致
```

## Huber回归

另一种方法是基于经过修改的额损失函数，称为Huber损失(对于单个样本)。

```python
from __future__ import print_function

import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression, HuberRegressor


# For reproducibility
np.random.seed(1000)

nb_samples = 500
nb_noise_samples = 50


def show_dataset(X, Y):
    fig, ax = plt.subplots(1, 1, figsize=(30, 25))

    ax.scatter(X, Y)
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.grid()

    plt.show()


if __name__ == '__main__':
    # Create dataset
    X = np.arange(-5, 5, 10.0 / float(nb_samples))

    Y = X + 2
    Y += np.random.uniform(-0.5, 0.5, size=nb_samples)

    noisy_samples = np.random.choice(np.arange(0, nb_samples), size=nb_noise_samples, replace=False)

    for i in noisy_samples:
        Y[i] += np.random.uniform(0, 10.0)

    # Show the dataset
    show_dataset(X, Y)

    # Create a linear regressor
    lr = LinearRegression(normalize=True)
    lr.fit(X.reshape(-1, 1), Y.reshape(-1, 1))
    print('Standard regressor: y = %.3fx + %.3f' % (lr.coef_, lr.intercept_))

    # Create a Huber regressor
    hr = HuberRegressor(epsilon=1.25)
    hr.fit(X.reshape(-1, 1), Y)
    print('Huber regressor: y = %.3fx + %.3f' % (hr.coef_, hr.intercept_))
```

