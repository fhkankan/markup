# 随机森林

随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。

是一种集成方法，通过集成多个比较简单的评估器形成累积效果。

- 优缺点

优点

1. 能够解决单个决策树不稳定的情况

2. 能够处理具有高维特征的输入样本，而且不需要降维（使用的是特征子集）
3. 对于缺省值问题也能够获得很好得结果（投票）

4. 在训练完成后，能够给出哪些feature比较重要

5. 容易做成并行化方法，速度比较快

6. 可以进行可视化， 便于分析

缺点

随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合


## 原理

Bagging + Base Estimator: Decision Tree

决策树在节点划分上，在随机的特征子集上寻找最优划分特征。

注意：理论上越多的树效果越好，但实际上基本超过一定数量就上下浮动了

- Extra-Tree

随机森林里单颗树的生长过程中，每个节点在分裂时仅考虑到一个随机子集所包含的特征。如果对每个特征使用随机阈值，而不是搜索得出的最佳阈值，则可能让决策树生长得更加随机。

这种极端随机的巨册书组成的森林，被称为**极端随机树**集成（Exta-Tree）。

决策树在节点划分上，使用随机的特征和随机的阈值

提供了额外的随机性，抑制过拟合，以更高的偏差换取了更低的方差。

比常规的决策树有更快的训练速度，因为在每个节点上找到每个特征的最佳阈值时决策树生长中最耗时的任务之一。

- 特征重要性

查看单个决策树，重要的特征更可能出现在靠近根节点的位置，而不重要的特征通常出现在靠近叶节点的位置(甚至根本不出现)。因此，通过计算一个特征在森林中的所有树上的平均深度，可以估算出一个特征的重要程度。

## sklearn

### 随机森林

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

X, y = make_moons(n_samples=500, noise=0.3, random_state=666)

plt.scatter(X[y == 0, 0], X[y == 0, 1])
plt.scatter(X[y == 1, 0], X[y == 1, 1])
plt.show()

# 方法一：bagging
bag = BaggingClassifier(
    DecisionTreeClassifier(splitter="random"),
    n_estimators=500,
    bootstrap=True,
    oob_score=True,
    n_jobs=-1,
    random_state=42
)
bag.fit(X, y)
print(bag.oob_score_)

# 方法二：RandomForestClassifier
rf_clf = RandomForestClassifier(
    n_estimators=500,  # 森林中树木的数量
    bootstrap=True,  # 可放回的抽样
    oob_score=True,
    n_jobs=-1,
    random_state=42,
)
rf_clf.fit(X, y)
print(rf_clf.oob_score_)

# 修改参数
rf_clf2 = RandomForestClassifier(
    n_estimators=500,
    bootstrap=True, 
    max_leaf_nodes=16,
    oob_score=True,
    n_jobs=-1,
    random_state=42,
)
rf_clf2.fit(X, y)
print(rf_clf2.oob_score_)
```

### Extra-Tree

scikit-learn提供了一个方差来增加选择最佳阈值的随机性。`ExtraTreesClassifier`类可以实现随机计算阈值并选择最佳阈值的模型。这使我们能够进一步减小方差，通常可以获得更好的最终验证准确性。

api

```python
from sklearn.ensemble import ExtraTreesClassifier

et_clf = ExtraTreesClassifier(
	n_estimators=500,
  	bootstrap=True,
  	oob_score=True,
  	random_state=42,
)
et_clf.fit(X, y)
print(et_clf.oob_score_)
```

实例

```python
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import load_digits
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.model_selection import cross_val_score


# For reproducibility
np.random.seed(1000)

nb_classifications = 100


if __name__ == '__main__':
    # Load dataset
    digits = load_digits()

    # Collect accuracies
    rf_accuracy = []
    et_accuracy = []

    for i in range(1, nb_classifications):
        a = cross_val_score(RandomForestClassifier(n_estimators=i), digits.data, digits.target, scoring='accuracy',
                            cv=10).mean()
        rf_accuracy.append(a)

        b = cross_val_score(ExtraTreesClassifier(n_estimators=i), digits.data, digits.target, scoring='accuracy',
                            cv=10).mean()
        et_accuracy.append(b)

    # Show results
    plt.figure(figsize=(30, 25))
    plt.xlabel('Number of trees')
    plt.ylabel('Accuracy')
    plt.grid(True)
    plt.plot(rf_accuracy, color='blue', label='Random Forest')
    plt.plot(et_accuracy, color='red', label='Extra Random Forest')
    plt.legend(loc="lower right")
    plt.show()
```

### 特征重要性

通过变量`feature_importances_`在训练结束后自动计算每个特征的重要性。

```python
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier

iris = load_iris()
rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)
rnd_clf.fit(iris["data"], iris["target"])
for name, score in zip(iris["feature_names"], rnd_clf.feature_importances_):
    print(name, score)

print(rnd_clf.feature_importances_)
"""
sepal length (cm) 0.11249225099876375
sepal width (cm) 0.02311928828251033
petal length (cm) 0.4410304643639577
petal width (cm) 0.4233579963547682
[0.11249225 0.02311929 0.44103046 0.423358  ]
"""
```

### 回归问题

```python
from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreeRegressor
```

示例

```python
import numpy as np
import pandas as pd
import matplotlib as mpl
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.ensemble import RandomForestRegressor


# 快慢震荡组合数据
def model(x, sigma=0.3):
    fast_oscillation = np.sin(5 * x)
    slow_oscillation = np.sin(0.5 * x)
    noise = sigma * rng.randn(len(x))
    return slow_oscillation + fast_oscillation + noise


rng = np.random.RandomState(42)
x = 10 * rng.rand(200)

y = model(x)
# plt.errorbar(x, y, 0.3, fmt='o')
# plt.show()

# 使用随机森林回归
forest = RandomForestRegressor(200)
forest.fit(x[:, None], y)

xfit = np.linspace(0, 10, 1000)
yfit = forest.predict(xfit[:, None])
ytrue = model(xfit, sigma=0)

plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)
plt.plot(xfit, yfit, '-g')
plt.plot(xfit, ytrue, '-k', alpha=0.5)
plt.show()

```

### 随机森林训练

示例1

```python
import numpy as np

import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier


# 对分类器结果进行可视化
def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):
    ax = ax or plt.gca()
    # 画出训练数据
    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap, clim=(y.min(), y.max()), zorder=3)
    ax.axis('tight')
    ax.axis('off')
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    # 用评估器拟合数据
    model.fit(X, y)
    xx, yy = np.meshgrid(np.linspace(*xlim, num=200), np.linspace(*ylim, num=200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    # 为结果生成彩色图
    n_classes = len(np.unique(y))
    contours = ax.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5,
                           cmap=cmap, zorder=1)
    ax.set(xlim=xlim, ylim=ylim)


# 创建一颗决策树
X, y = make_blobs(n_samples=300, centers=4, random_state=0, cluster_std=1.0)
# plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
# plt.show()


# 集成袋装分类器
# tree = DecisionTreeClassifier()
# bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8, random_state=1)
# visualize_classifier(bag, X, y)
# plt.show()

# 随机森林
model = RandomForestClassifier(n_estimators=100, random_state=0)
visualize_classifier(model, X, y)
plt.show()
```

示例2

```python
# 1.导入合适的包
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier,export_graphviz
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier

# 2.加载数据
data = pd.read_csv("http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt")
# 3.数据处理
# 填补缺失值age
data["age"].fillna(data["age"].mean(), inplace=True)
# 雷彪数据进行One-Hot编码
data = pd.get_dummies(data, columns=["pclass", "sex"]) 
print(data.head(2))
# 4.特征选择和数据集分割
# 特征值
x = data[["age", "pclass_1st", "pclass_2nd", "pclass_3rd", "sex_female", "sex_male"]]
# 目标值
y = data["survived"]
# 数据集分割
x_train, x_test, y_train, y_test = train_test_split(x, y)
# 5.随机森林估计器流程
rfc = RandomForestClassifier(n_estimators=5, criterion="entropy", max_depth=4)
rfc.fit(x_train, y_train)
# 5.预测
predict = rfc.predict(x_test)
# 6.准确率
score = rfc.score(x_test, y_test)
print(score)
```

## tensorflow

使用回归树来预测波士顿房屋价格

```python
"""
Using a Random Forest
---------------------

This script will illustrate how to use TensorFlow's Boosted Random Forest algorithm.


For illustrative purposes we will show how to do this with the boston housing data.

Attribute Information:

    1. CRIM      per capita crime rate by town
    2. ZN        proportion of residential land zoned for lots over
                 25,000 sq.ft.
    3. INDUS     proportion of non-retail business acres per town
    4. CHAS      Charles River dummy variable (= 1 if tract bounds
                 river; 0 otherwise)
    5. NOX       nitric oxides concentration (parts per 10 million)
    6. RM        average number of rooms per dwelling
    7. AGE       proportion of owner-occupied units built prior to 1940
    8. DIS       weighted distances to five Boston employment centres
    9. RAD       index of accessibility to radial highways
    10. TAX      full-value property-tax rate per $10,000
    11. PTRATIO  pupil-teacher ratio by town
    12. B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks
                 by town
    13. LSTAT    % lower status of the population
    14. y_target Median value of owner-occupied homes in $1000's.
"""

import os
import numpy as np
import tensorflow as tf
from keras.datasets import boston_housing
from tensorflow.python.framework import ops
ops.reset_default_graph()

# For using the boosted trees classifier (binary classification) in TF:
# Note: target labels have to be 0 and 1.
boosted_classifier = tf.estimator.BoostedTreesClassifier

# For using a boosted trees regression classifier (binary classification) in TF:
regression_classifier = tf.estimator.BoostedTreesRegressor

# Load data
(x_train, y_train), (x_test, y_test) = boston_housing.load_data()

# Set model parameters
# Batch size
batch_size = 32
# Number of training steps
train_steps = 500
# Number of trees in our 'forest'
n_trees = 100
# Maximum depth of any tree in forest
max_depth = 6

# Data ETL
binary_split_cols = ['CHAS', 'RAD']
col_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']
X_dtrain = {col: x_train[:, ix] for ix, col in enumerate(col_names)}
X_dtest = {col: x_test[:, ix] for ix, col in enumerate(col_names)}

# Create feature columns!
feature_cols = []
for ix, column in enumerate(x_train.T):
    col_name = col_names[ix]

    # Create binary split feature
    if col_name in binary_split_cols:
        # To create 2 buckets, need 1 boundary - the mean
        bucket_boundaries = [column.mean()]
        numeric_feature = tf.feature_column.numeric_column(col_name)
        final_feature = tf.feature_column.bucketized_column(source_column=numeric_feature, boundaries=bucket_boundaries)
    # Create bucketed feature
    else:
        # To create 5 buckets, need 4 boundaries
        bucket_boundaries = list(np.linspace(column.min() * 1.1, column.max() * 0.9, 4))
        numeric_feature = tf.feature_column.numeric_column(col_name)
        final_feature = tf.feature_column.bucketized_column(source_column=numeric_feature, boundaries=bucket_boundaries)

    # Add feature to feature_col list
    feature_cols.append(final_feature)


# Create an input function
input_fun = tf.estimator.inputs.numpy_input_fn(X_dtrain, y=y_train, batch_size=batch_size, num_epochs=10, shuffle=True)

# Training
model = regression_classifier(feature_columns=feature_cols,
                              n_trees=n_trees,
                              max_depth=max_depth,
                              learning_rate=0.25,
                              n_batches_per_layer=batch_size)
model.train(input_fn=input_fun, steps=train_steps)

# Evaluation on test set
# Do not shuffle when predicting
p_input_fun = tf.estimator.inputs.numpy_input_fn(X_dtest, y=y_test, batch_size=batch_size, num_epochs=1, shuffle=False)
# Get predictions
predictions = list(model.predict(input_fn=p_input_fun))
final_preds = [pred['predictions'][0] for pred in predictions]

# Get accuracy (mean absolute error, MAE)
mae = np.mean([np.abs((actual - predicted) / predicted) for actual, predicted in zip(y_test, final_preds)])
print('Mean Abs Err on test set: {}'.format(acc))

```

