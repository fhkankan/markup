# 被动攻击算法

其他算法的一个问题是，在收集新样本时，必须重新训练整个模型。Grammer等人提出主要思想是逐步训练模型，仅允许需要时系乖参数，同时丢弃不改变均衡的所有更新。在初始论文中，提出了三种变体，此处考虑PA-II的变体。

## 回归

```python
from __future__ import print_function

import matplotlib.pyplot as plt
import numpy as np

from sklearn.datasets import make_regression
from sklearn.linear_model import PassiveAggressiveRegressor


# For reproducibility
np.random.seed(1000)

nb_samples_1 = 300
nb_samples_2 = 500


if __name__ == '__main__':
    # Create the dataset
    X, Y = make_regression(n_samples=nb_samples_1, n_features=5, random_state=1000)

    # Create the model
    par = PassiveAggressiveRegressor(C=0.01, loss='squared_epsilon_insensitive', epsilon=0.001, max_iter=2000,
                                     random_state=1000)

    # Fit the model incrementally and collect the squared errors
    squared_errors = []

    for (x, y) in zip(X, Y):
        par.partial_fit(x.reshape(1, -1), y.ravel())
        y_pred = par.predict(x.reshape(1, -1))
        squared_errors.append(np.power(y_pred - y, 2))

    # Show the error plot
    fig, ax = plt.subplots(figsize=(18, 8))

    ax.plot(squared_errors)
    ax.set_xlabel('Sample')
    ax.set_ylabel('Squared error')
    ax.grid()

    plt.show()

    # Repeat the example with a discontinuous dataset
    X1, Y1 = make_regression(n_samples=nb_samples_2, n_features=5, random_state=1000)
    X2, Y2 = make_regression(n_samples=nb_samples_2, n_features=5, random_state=1000)

    X2 += np.max(X1)
    Y2 += 0.5

    X = np.concatenate((X1, X2))
    Y = np.concatenate((Y1, Y2))

    par = PassiveAggressiveRegressor(C=0.01, loss='squared_epsilon_insensitive', epsilon=0.001, max_iter=2000,
                                     random_state=1000)

    # Fit the model incrementally and collect the squared errors
    squared_errors = []

    for (x, y) in zip(X, Y):
        par.partial_fit(x.reshape(1, -1), y.ravel())
        y_pred = par.predict(x.reshape(1, -1))
        squared_errors.append(np.power(y_pred - y, 2))

    # Show the error plot
    fig, ax = plt.subplots(figsize=(18, 8))

    ax.plot(squared_errors)
    ax.set_xlabel('Sample')
    ax.set_ylabel('Squared error')
    ax.grid()

    plt.show()
```

## 分类

```python
from __future__ import print_function

import matplotlib.pyplot as plt
import numpy as np

from sklearn.datasets import load_iris
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split


# For reproducibility
np.random.seed(1000)


if __name__ == '__main__':
    # Load and scale the dataset
    iris = load_iris()

    ss = StandardScaler()

    X = ss.fit_transform(iris['data'])
    Y = iris['target']

    # Split the dataset
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1000)

    # Create the model
    pac = PassiveAggressiveClassifier(C=0.05, loss='squared_hinge', max_iter=2000, random_state=1000)

    # Train with the start-up samples
    nb_initial_samples = int(X_train.shape[0] / 1.5)
    pac.fit(X_train[0:nb_initial_samples], Y_train[0:nb_initial_samples])

    # Continue with the incremental samples
    validation_accuracies = []

    for (x, y) in zip(X_train[nb_initial_samples:], Y_train[nb_initial_samples:]):
        pac.partial_fit(x.reshape(1, -1), y.ravel(), classes=np.unique(iris['target']))
        validation_accuracies.append(pac.score(X_test, Y_test))

    # Show the validation plot
    fig, ax = plt.subplots(figsize=(18, 8))

    ax.plot(validation_accuracies)
    ax.set_xlabel('Online sample')
    ax.set_ylabel('Validation accuracy')
    ax.grid()

    plt.show()

```

