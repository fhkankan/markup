[TOC]
# 聚类-其他

## 层次聚类

层次聚类时将聚类看成是一个按层次进行数据划分的过程，算法结束后将形成一棵聚类树。层次聚类按照执行顺序不同，可分为自顶向上的合并方法，以及自顶向下的分裂方法

如果两个簇就是两个点，则他们的距离可以用距离度量的任意一个距离指标来度量，当两个簇中包含多个数据点事，衡量两个簇 $C_i,C_j$。的距离的常用方法有如下

- 最小距离

对于簇$C_i$中任意一点$p_i\in C_i$，以及簇$C_j$中任意一个点$p_j\in C_j$，最小距离由两个簇中最近的两个样本点决定
$$
d_{i,j} = \min d(p_i,p_j)
$$

- 最大距离

对于簇$C_i$中任意一点$p_i\in C_i$，以及簇$C_j$中任意一个点$p_j\in C_j$，最小距离由两个簇中最远的两个样本点决定
$$
d_{i,j} = \max d(p_i,p_j)
$$


- 平均距离

对于簇$C_i$中任意一点$p_i\in C_i$，以及簇$C_j$中任意一个点$p_j\in C_j$，最小距离由两个簇中所有样本点决定
$$
d_{i,j} = \frac{1}{|C_i||C_j|}\sum_{p_i\in C_i}\sum_{p_j\in C_j} d(p_i,p_j)
$$

### 自底向上

AGNES算法流程

```
输入：包含m个数据点{x^1,x^2,\cdots, x^m}的数据集D
输出：k个簇{C_1,C_2,\cdots,C_k}
1.将每一个数据点单独初始化为一个簇，初始化C_i = x^i，并记当前的簇数为cur = m
2.当 cur > k时，执行下面的操作，否则跳出循环
	2.1 找出距离最短的两个簇C_i和C_j(i<j)，将簇C_i和C_j的数据点合并，合并后的簇重新标记为C_i
	2.2 将簇C_{j+1},C_{j+2},\cdots,C_{cur}分别重新编号为C_j,C_{j+1},\cdots,C_{cur}
	2.3 修改cur值：cur = cur - 1
3.输出k个簇{C_1, C_2,\cdots, C_k}
```

### 自顶向下

DIANA算法流程

```
输入：包含m个数据点{x^1,x^2,\cdots, x^m}的数据集D
输出：k个簇{C_1,C_2,\cdots,C_k}
1.将全部数据点都初始化为一个簇，初始化C_1 = {x^1,x^2,\cdots,x^m}，并记当前的簇数为cur = 1
2.当 cur < k时，执行下面的操作，否则跳出循环
	2.1 找出直径最大的簇进行分裂操作
	2.2 找出簇中与其他点平均距离最大的点p,将p放入split_group中，其余点放在old_part中 
	2.3 重复下面的操作
		在old_part中找出到slit_group最近的点的距离不大于到old_part中最近点的距离的点q，将q添加进split_group中；	
		若没有找到这样的点则推出循环
	2.4 将split_group与old_part分别作为两个新的簇，编号分别为C_i和C_{cur+1}
	2.5 cur = cur + 1
3.输出k个簇{C_1, C_2,\cdots, C_k}
```

### 树形图

为了更好理解聚集过程，引入一种称为**树形图**的图形化方法，该方法以静态方式显示从底部（所有样本都被分离）到顶部（连接完成的位置）的聚集过程。

```python
from __future__ import print_function

import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_blobs

from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage
from scipy.cluster.hierarchy import dendrogram

# For reproducibility
np.random.seed(1000)

nb_samples = 25

if __name__ == '__main__':
    # Create the dataset
    X, Y = make_blobs(n_samples=nb_samples, n_features=2, centers=3, cluster_std=1.5)

    # Show the dataset
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))

    ax.grid()
    ax.set_xlabel('X')
    ax.set_ylabel('Y')

    ax.scatter(X[:, 0], X[:, 1], marker='o', color='b')
    plt.show()

    # Compute the distance matrix
    Xdist = pdist(X, metric='euclidean')

    # Compute the linkage
    Xl = linkage(Xdist, method='ward')

    # Compute and show the dendrogram
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))
    Xd = dendrogram(Xl)
    plt.show()
```

### 凝聚聚类

层次聚类(hierarchical clustering)是一组聚类算法，通过不断地分解或合并集群来构建树状集群(tree-like clusters)。层次聚类的结构可以用一棵树表示。

层次聚类算法可以是自下而上的，也可以是自上而下的。在自下而上的算法中，每个数据点都被看作是一个单独的集群。这些集群不断地合并，直到所有的集群都合并成一个巨型集群。这被称为**凝聚层次聚类**。与之相反，自上而下的算法是从一个巨大的集群开始，不断地分解，直到所有的集群变成一个单独的数据点。

示例1

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import kneighbors_graph

# 添加一些噪声
def add_noise(x, y, amplitude):
    X = np.concatenate((x, y))
    X += amplitude * np.random.randn(2, X.shape[1])
    return X.T

# 一组呈螺旋状的数据点
def get_spiral(t, noise_amplitude=0.5):
    r = t
    x = r * np.cos(t)
    y = r * np.sin(t)

    return add_noise(x, y, noise_amplitude)

# 位于玫瑰曲线上的数据点
def get_rose(t, noise_amplitude=0.02):
    # 设置玫瑰曲线方程：若变量k是奇数，那么曲线有k朵花瓣，若k是偶数，则有2k朵花瓣
    k = 5
    r = np.cos(k * t) + 0.25
    x = r * np.cos(t)
    y = r * np.sin(t)

    return add_noise(x, y, noise_amplitude)

# 增加多样性
def get_hypotrochoid(t, noise_amplitude=0):
    a, b, h = 10.0, 2.0, 4.0
    x = (a - b) * np.cos(t) + h * np.cos((a - b) / b * t)
    y = (a - b) * np.sin(t) - h * np.sin((a - b) / b * t)

    return add_noise(x, y, 0)


def perform_clustering(X, connectivity, title, num_clusters=3, linkage='ward'):
    """实现凝聚层次聚类"""
    plt.figure()
    model = AgglomerativeClustering(linkage=linkage, connectivity=connectivity, n_clusters=num_clusters)
    model.fit(X)

    # 提取标记
    labels = model.labels_

    # 为每种集群设置不同的标记
    markers = '.vx'

    for i, marker in zip(range(num_clusters), markers):
        # 画出属于某个集群中心的数据点
        plt.scatter(X[labels == i, 0], X[labels == i, 1], s=50,
                    marker=marker, color='k', facecolors='none')

    plt.title(title)


if __name__ == '__main__':
    # 生成样本数据
    n_samples = 500
    np.random.seed(2)
    t = 2.5 * np.pi * (1 + 2 * np.random.rand(1, n_samples))
    X = get_spiral(t)

    # 不考虑螺旋形的数据连接性
    connectivity = None
    perform_clustering(X, connectivity, 'No connectivity')

    # 根据数据连接线创建k个临近点的图形
    connectivity = kneighbors_graph(X, 10, include_self=False)
    perform_clustering(X, connectivity, 'K-Neighbors connectivity')

    plt.show()

```

示例2

```python
from __future__ import print_function

import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_blobs
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score, adjusted_rand_score

# For reproducibility
np.random.seed(1000)

nb_samples = 3000


def plot_clustered_dataset(X, Y):
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))

    ax.grid()
    ax.set_xlabel('X')
    ax.set_ylabel('Y')

    markers = ['o', 'd', '^', 'x', '1', '2', '3', 's']
    colors = ['r', 'b', 'g', 'c', 'm', 'k', 'y', '#cccfff']

    for i in range(nb_samples):
        ax.scatter(X[i, 0], X[i, 1], marker=markers[Y[i]], color=colors[Y[i]])

    plt.show()


if __name__ == '__main__':
    # Create the dataset
    X, Y = make_blobs(n_samples=nb_samples, n_features=2, centers=8, cluster_std=2.0)

    # Show the dataset
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))

    ax.grid()
    ax.set_xlabel('X')
    ax.set_ylabel('Y')

    ax.scatter(X[:, 0], X[:, 1], marker='o', color='b')
    plt.show()

    # Complete linkage
    print('Complete linkage')
    ac = AgglomerativeClustering(n_clusters=8, linkage='complete')
    Y_pred = ac.fit_predict(X)

    print('Silhouette score (Complete): %.3f' % silhouette_score(X, Y_pred))
    print('Adjusted Rand score (Complete): %.3f' % adjusted_rand_score(Y, Y_pred))

    # Show the clustered dataset
    plot_clustered_dataset(X, Y)

    # Average linkage
    print('Average linkage')
    ac = AgglomerativeClustering(n_clusters=8, linkage='average')
    Y_pred = ac.fit_predict(X)

    print('Silhouette score (Average): %.3f' % silhouette_score(X, Y_pred))
    print('Adjusted Rand score (Average): %.3f' % adjusted_rand_score(Y, Y_pred))

    # Show the clustered dataset
    plot_clustered_dataset(X, Y)

    # Ward linkage
    print('Ward linkage')
    ac = AgglomerativeClustering(n_clusters=8)
    Y_pred = ac.fit_predict(X)

    print('Silhouette score (Ward): %.3f' % silhouette_score(X, Y_pred))
    print('Adjusted Rand score (Ward): %.3f' % adjusted_rand_score(Y, Y_pred))

    # Show the clustered dataset
    plot_clustered_dataset(X, Y)

```

### 连接限制

scikit-learn允许指定连接矩阵，可以在找到要合并的聚类时用作约束。以这种方式，跳过彼此远离（在连接矩阵中不相邻）的类。创建这样的矩阵常见的方法是使用k-NN图函数(`kneighbors_graph()`)，该方法基于根据特定的度量下样本的相邻点的数量。

```python
from __future__ import print_function

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm

from sklearn.datasets import make_circles
from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import kneighbors_graph
from sklearn.metrics import silhouette_score

# For reproducibility
np.random.seed(1000)


nb_samples = 3000


if __name__ == '__main__':
    # Create the dataset
    X, Y = make_circles(n_samples=nb_samples, noise=0.05)

    # Show the dataset
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))

    ax.grid()
    ax.set_xlabel('X')
    ax.set_ylabel('Y')

    ax.scatter(X[:, 0], X[:, 1], marker='o', color='b')
    plt.show()

    # Unstructured clustering with average linkage
    print('Unstructured clustering with average linkage')
    ac = AgglomerativeClustering(n_clusters=20, linkage='average')
    Y_pred = ac.fit_predict(X)

    print('Silhouette score: %.3f' % silhouette_score(X, Y_pred))

    # Plot the clustered dataset
    fig, ax = plt.subplots(1, 1, figsize=(12, 10))

    ax.grid()
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.scatter(X[:, 0], X[:, 1], marker='o', cmap=cm.spectral, c=ac.labels_)
    plt.show()

    # Connectivity constraints
    print('Imposing connectivity constraints')

    acc = []
    k = [50, 100, 200, 500]

    ac = AgglomerativeClustering(n_clusters=20, connectivity=None, linkage='average')
    ac.fit(X)

    for i in k:
        kng = kneighbors_graph(X, i)
        ac1 = AgglomerativeClustering(n_clusters=20, connectivity=kng, linkage='average')
        Y_pred = ac1.fit_predict(X)
        print('Silhouette score (k=%d): %.3f' % (i, silhouette_score(X, Y_pred)))
        acc.append(ac1)

    # Show the four plots
    fig, ax = plt.subplots(2, 2, figsize=(14, 10))

    ax[0, 0].grid()
    ax[0, 0].set_title('K = 50')
    ax[0, 0].set_xlabel('X')
    ax[0, 0].set_ylabel('Y')
    ax[0, 0].scatter(X[:, 0], X[:, 1], marker='o', cmap=cm.spectral, c=acc[0].labels_)

    ax[0, 1].grid()
    ax[0, 1].set_title('K = 100')
    ax[0, 1].set_xlabel('X')
    ax[0, 1].set_ylabel('Y')
    ax[0, 1].scatter(X[:, 0], X[:, 1], marker='o', cmap=cm.spectral, c=acc[1].labels_)

    ax[1, 0].grid()
    ax[1, 0].set_title('K = 200')
    ax[1, 0].set_xlabel('X')
    ax[1, 0].set_ylabel('Y')
    ax[1, 0].scatter(X[:, 0], X[:, 1], marker='o', cmap=cm.spectral, c=acc[2].labels_)

    ax[1, 1].grid()
    ax[1, 1].set_title('K = 500')
    ax[1, 1].set_xlabel('X')
    ax[1, 1].set_ylabel('Y')
    ax[1, 1].scatter(X[:, 0], X[:, 1], marker='o', cmap=cm.spectral, c=acc[3].labels_)
    plt.show()
    
```

## DBSCAN

DBSCAN即具有噪声的基于密度的空间聚类方法(density-based spatial clustering of applications with noise)。

### 原理

该算法利用基于密度的聚类的概念，即要求聚类空间中的一定区域内所包含的对象（点或其他空间对象）的数目不小于某一给定阈值。

DBSCAN算法有2个输入参数：一个是半径（Eps），表示以给定 $P$ 为中心的圆形邻域的范围；另一个是以点 $P$ 为中心的邻域内最少点的数量（MinPts），这2个参数的计算都来自经验。如果满足：以点 $P$ 为中心、半径为 $Eps$ 的邻域内的点的个数不少于 $MinPts$，则称点 $P$ 为核心点。

- 其主要基本概念如下：

$\varepsilon$ 邻域：给定对象半径 $\varepsilon$ 内的区域称为该对象的 $\varepsilon$ 邻域。

核心对象：如果给定对象 $\varepsilon$ 邻域内的样本点数大于等于 $MinPts$，则称该对象为核心对象。

直接密度可达：给定一个对象集合 $D$，如果 $p$ 在 $q$ 的 $\varepsilon$ 邻域内，且$q$ 是一个核心对象，则对象 $p$ 从对象 $q$ 出发是直接密度可达的（directly density-reachable）

密度可达：对于样本集合 $D$，如果存在一个对象链 $p_1,p_2,\cdots,p_n;p_1=q,p_n=p$，对于 $p_1\in D(1\leq i \leq n)$，$p_{i+1}$ 是从 $p_i$ 关于 $\varepsilon$ 和 $MinPts$ 直接密度可达，则对象 $p$ 是从对象$q$ 关于$\varepsilon$ 和 $MinPts$ 密度可达的（density-reachable）。

密度相连：如果存在对象 $o \in D$，使对象 $p,q$ 都是从 $o$ 关于$\varepsilon$ 和 $MinPts$ 密度可达的，那么对象 $p$ 到 $q$ 是于$\varepsilon$ 和 $MinPts$ 密度相连的（density-connected）。

可以发现，密度可达是直接密度可达的传递闭包，并且这种关系是非对称的。只有核心对象之间相互密度可达。密度相连是对称关系。DBSCAN的目的就是要找到密度相连对象的最大集合。

- 具体聚类过程

扫描整个数据集，找到任意一个核心点，对该核心点进行扩充。扩充的方法是寻找从该核心点出发的所有密度相连的数据点。遍历该核心点的 $\varepsilon$ 邻域内的所有核心点（边界点无法扩充），寻找与这些数据点密度相连的点，直到没有可以扩充的数据点为止。最后聚类成簇（即类）的边界节点都是非核心数据点。之后就是重新扫描数据集（不包括之前寻找到的簇中的任何数据点），寻找没有被聚类的核心点，再重复上面的步骤对该核心点进行扩充，直到数据集中没有新的核心点为止。数据集中没有包含在任何簇中的数据点就构成异常点。

- 优缺点

> 优点

不需要指定簇个数
可以发现任意形状的簇
擅长找到离群点(检测任务)
两个参数就enough

> 缺点

高维数据有些困难(可以降维)
参数难以选择(参数对结果的影响非常大)
sklearn中效率很慢(数据消减策略)

### sklearn

示例1

```python
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

db = DBSCAN(eps=10, min_samples=2).fix(X)
db_scaled = DBSCAN(eps=10, min_samples=2).fix(X_scaled)

lables = db.labels_
lables_scaled = db_scaled.labels_

score = silhouette_score(X, labels)
score_scaled = silhouette_score(X, labels_scaled)
print(score, score_scaled)
```

示例2

```python
import numpy as np
import os
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.cluster import DBSCAN
from sklearn.neighbors import KNeighborsClassifier

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training"


def save_fig(fig_id, tight_layout=True):
    path = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id + ".png")
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format='png', dpi=300)


X, y = make_moons(n_samples=1000, noise=0.05, random_state=42)

dbscan = DBSCAN(eps=0.05, min_samples=5)
dbscan.fit(X)
print(dbscan.labels_[:10])
print(len(dbscan.core_sample_indices_))
print(dbscan.core_sample_indices_[:10])
print(dbscan.components_[:3])
print(np.unique(dbscan.labels_))
"""
[ 0  2 -1 -1  1  0  0  0  2  5]
808
[ 0  4  5  6  7  8 10 11 12 13]
[[-0.02137124  0.40618608]
 [-0.84192557  0.53058695]
 [ 0.58930337 -0.32137599]]
[-1  0  1  2  3  4  5  6]
"""

dbscan2 = DBSCAN(eps=0.2)
dbscan2.fit(X)

def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):
    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)
    core_mask[dbscan.core_sample_indices_] = True
    anomalies_mask = dbscan.labels_ == -1
    non_core_mask = ~(core_mask | anomalies_mask)

    cores = dbscan.components_
    anomalies = X[anomalies_mask]
    non_cores = X[non_core_mask]

    plt.scatter(cores[:, 0], cores[:, 1],
                c=dbscan.labels_[core_mask], marker='o', s=size, cmap="Paired")
    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20, c=dbscan.labels_[core_mask])
    plt.scatter(anomalies[:, 0], anomalies[:, 1],
                c="r", marker="x", s=100)
    plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker=".")
    if show_xlabels:
        plt.xlabel("$x_1$", fontsize=14)
    else:
        plt.tick_params(labelbottom=False)
    if show_ylabels:
        plt.ylabel("$x_2$", fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft=False)
    plt.title("eps={:.2f}, min_samples={}".format(dbscan.eps, dbscan.min_samples), fontsize=14)


# 决策边界
def plot_data(X):
    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)


def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):
    if weights is not None:
        centroids = centroids[weights > weights.max() / 10]
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='o', s=30, linewidths=8,
                color=circle_color, zorder=10, alpha=0.9)
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='x', s=50, linewidths=50,
                color=cross_color, zorder=11, alpha=1)


def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,
                             show_xlabels=True, show_ylabels=True):
    mins = X.min(axis=0) - 0.1
    maxs = X.max(axis=0) + 0.1
    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),
                         np.linspace(mins[1], maxs[1], resolution))
    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                 cmap="Pastel2")
    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                linewidths=1, colors='k')
    plot_data(X)
    if show_centroids:
        plot_centroids(clusterer.cluster_centers_)

    if show_xlabels:
        plt.xlabel("$x_1$", fontsize=14)
    else:
        plt.tick_params(labelbottom=False)
    if show_ylabels:
        plt.ylabel("$x_2$", fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft=False)


plt.figure(figsize=(9, 3.2))
plt.subplot(121)
plot_dbscan(dbscan, X, size=100)
plt.subplot(122)
plot_dbscan(dbscan2, X, size=600, show_ylabels=False)
save_fig("dbscan_diagram")
plt.show()

dbscan = dbscan2
knn = KNeighborsClassifier(n_neighbors=50)
knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])
X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])
knn.predict(X_new)
print(knn.predict_proba(X_new))
"""
[[0.18 0.82]
 [1.   0.  ]
 [0.12 0.88]
 [1.   0.  ]]
"""

plt.figure(figsize=(6, 3))
plot_decision_boundaries(knn, X, show_centroids=False)
plt.scatter(X_new[:, 0], X_new[:, 1], c="b", marker="+", s=200, zorder=10)
save_fig("cluster_classification_diagram")
plt.show()

y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)
y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]
y_pred[y_dist > 0.2] = -1
res = y_pred.ravel()
print(res)
"""
[-1  0  1 -1]
"""
```

## 其他聚类

### 均值漂移

均值漂移是一种非常欠打的无监督学习算法，用于集群数据点。该算法把数据点的分布堪称是概率密度函数(probability-density function)，希望在特征空间中根据函数分布特征找出数据点的模式。这些模式就对应于一群群局部最密集分布的点。均值漂移算法的优点是它不需要事先确定集群的数量。

假设有一组输入点，在不知道要寻找多少集群的情况下找到它们。均值漂移算法就可以把这些点看成是服从某个概率密度函数的样本。如果这些数据点有集群，那么它们对应于概率密度函数的峰值。该算法从一个随机点开始，逐渐收敛于各个峰值。

```python
import numpy as np
from sklearn.cluster import MeanShift, estimate_bandwidth

# 加载数据
test = []
with open('data.txt', 'r') as f:
    for line in f.readlines():
        data = [float(x) for x in line.split(',')]
        test.append(data)

X = np.array(test)

# 创建均值漂移模型
# 设置宽带参数bandwidth
bandwidth = estimate_bandwidth(X, quantile=0.1, n_samples=len(X))
# 使用MeanShift计算聚类
meanshift_estimator = MeanShift(bandwidth=bandwidth, bin_seeding=True)

# 训练模型
meanshift_estimator.fit(X)
# 提取标记
labels = meanshift_estimator.labels_

# 提取集群的额中心点
centroids = meanshift_estimator.cluster_centers_
num_clusters = len(np.unique(labels))
print("Number of clusters in input data =", num_clusters)

# 集群可视化
import matplotlib.pyplot as plt
from itertools import cycle

plt.figure()

# 为每种集群设置不同的标记
markers = '.*xv'

for i, marker in zip(range(num_clusters), markers):
    # 画出属于某个集群中心的数据点
    plt.scatter(X[labels == i, 0], X[labels == i, 1], marker=marker, color='k')
    # 画出集群中心
    centroid = centroids[i]
    plt.plot(centroid[0], centroid[1], marker='o', markerfacecolor='k',
             markeredgecolor='k', markersize=15)

plt.title('Clusters and their centroids')
plt.show()

```

### 光谱聚类

示例1

```python
import numpy as np
import os
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.cluster import SpectralClustering

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training"


def save_fig(fig_id, tight_layout=True):
    path = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id + ".png")
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format='png', dpi=300)


X, y = make_moons(n_samples=1000, noise=0.05, random_state=42)

def plot_spectral_clustering(sc, X, size, alpha, show_xlabels=True, show_ylabels=True):
    plt.scatter(X[:, 0], X[:, 1], marker='o', s=size, c='gray', cmap="Paired", alpha=alpha)
    plt.scatter(X[:, 0], X[:, 1], marker='o', s=30, c='w')
    plt.scatter(X[:, 0], X[:, 1], marker='.', s=10, c=sc.labels_, cmap="Paired")

    if show_xlabels:
        plt.xlabel("$x_1$", fontsize=14)
    else:
        plt.tick_params(labelbottom=False)
    if show_ylabels:
        plt.ylabel("$x_2$", fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft=False)
    plt.title("RBF gamma={}".format(sc.gamma), fontsize=14)

sc1 = SpectralClustering(n_clusters=2, gamma=100, random_state=42)
sc1.fit(X)

sc2 = SpectralClustering(n_clusters=2, gamma=1, random_state=42)
sc2.fit(X)

res = np.percentile(sc1.affinity_matrix_, 95)
print(res)

plt.figure(figsize=(9, 3.2))
plt.subplot(121)
plot_spectral_clustering(sc1, X, size=500, alpha=0.1)
plt.subplot(122)
plot_spectral_clustering(sc2, X, size=4000, alpha=0.01, show_ylabels=False)
plt.show()

```
示例2

```python
from __future__ import print_function

import numpy as np
import matplotlib.pyplot as plt
import warnings

from sklearn.datasets import make_moons
from sklearn.cluster import SpectralClustering


# For reproducibility
np.random.seed(1000)

nb_samples = 1000


def show_dataset(X, Y):
    fig, ax = plt.subplots(1, 1, figsize=(30, 25))

    ax.grid()
    ax.set_xlabel('X')
    ax.set_ylabel('Y')

    for i in range(nb_samples):
        if Y[i] == 0:
            ax.scatter(X[i, 0], X[i, 1], marker='o', color='r')
        else:
            ax.scatter(X[i, 0], X[i, 1], marker='^', color='b')

    plt.show()


def show_clustered_dataset(X, Y):
    fig, ax = plt.subplots(1, 1, figsize=(30, 25))

    ax.grid()
    ax.set_xlabel('X')
    ax.set_ylabel('Y')

    ax.scatter(X[Y == 1, 0], X[Y == 1, 1], marker='o', color='r')
    ax.scatter(X[Y == 0, 0], X[Y == 0, 1], marker='^', color='b')

    plt.show()


if __name__ == '__main__':
    warnings.simplefilter("ignore")

    # Create dataset
    X, Y = make_moons(n_samples=nb_samples, noise=0.05)

    # Show dataset
    show_dataset(X, Y)

    # Cluster the dataset for different gamma values
    Yss = []
    gammas = np.linspace(0, 12, 4)

    for gamma in gammas:
        sc = SpectralClustering(n_clusters=2, affinity='rbf', gamma=gamma)
        Yss.append(sc.fit_predict(X))

    # Show the result
    # The colors can be inverted with respect to the figure in the book
    fig, ax = plt.subplots(1, 4, figsize=(18, 8))

    for i in range(4):
        ax[i].scatter(X[Yss[i] == 1, 0], X[Yss[i] == 1, 1], marker='o', color='r')
        ax[i].scatter(X[Yss[i] == 0, 0], X[Yss[i] == 0, 1], marker='^', color='b')
        ax[i].grid()
        ax[i].set_xlabel('X')
        ax[i].set_ylabel('Y')
        ax[i].set_title('Gamma = {}'.format(i * 4))

    plt.show()

    # Create and train Spectral Clustering
    sc = SpectralClustering(n_clusters=2, affinity='nearest_neighbors')
    Ys = sc.fit_predict(X)

    # Show clustered dataset
    show_clustered_dataset(X, Y)

```

### 在线聚类

- mini-batch k-means

```python
from __future__ import print_function

import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans, MiniBatchKMeans
from sklearn.metrics import adjusted_rand_score


# Set random seed for reproducibility
np.random.seed(1000)


nb_samples = 2000
batch_size = 80


if __name__ == '__main__':
    # Create the dataset
    X, Y = make_blobs(n_samples=nb_samples, n_features=2, centers=5, cluster_std=1.5, random_state=1000)

    # Create an instance of Mini-Batch k-Means
    mbkm = MiniBatchKMeans(n_clusters=5, max_iter=1000, batch_size=batch_size, random_state=1000)

    # Train the model
    X_batch = []
    Y_preds = []

    for i in range(0, nb_samples, batch_size):
        mbkm.partial_fit(X[i:i + batch_size])

        X_batch.append(X[:i + batch_size])
        Y_preds.append(mbkm.predict(X[:i + batch_size]))

    # Show the training steps
    fig, ax = plt.subplots(5, 5, figsize=(20, 12))

    for i in range(5):
        for j in range(5):
            idx = (i * 5) + j

            for k in range(5):
                ax[i][j].scatter(X_batch[idx][Y_preds[idx] == k, 0], X_batch[idx][Y_preds[idx] == k, 1], s=3)

            ax[i][j].set_xticks([])
            ax[i][j].set_yticks([])
            ax[i][j].set_title('{} samples'.format(batch_size * (idx + 1)))

    plt.show()

    # Compute the Adjusted-Rand score and compare it with a standard K-Means
    print(adjusted_rand_score(mbkm.predict(X), Y))

    km = KMeans(n_clusters=5, max_iter=1000, random_state=1000)
    km.fit(X)

    print(adjusted_rand_score(km.predict(X), Y))


```

- BIRCH

```python
from __future__ import print_function

import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_blobs
from sklearn.cluster import Birch
from sklearn.metrics import adjusted_rand_score


# Set random seed for reproducibility
np.random.seed(1000)


nb_samples = 2000
batch_size = 80


if __name__ == '__main__':
    # Create the dataset
    X, Y = make_blobs(n_samples=nb_samples, n_features=2, centers=5, cluster_std=1.5, random_state=1000)

    # Create an instance of BIRCH
    birch = Birch(n_clusters=5, threshold=0.15, branching_factor=100)

    # Train the model
    X_batch = []
    Y_preds = []

    for i in range(0, nb_samples, batch_size):
        birch.partial_fit(X[i:i + batch_size])
        X_batch.append(X[:i + batch_size])
        Y_preds.append(birch.predict(X[:i + batch_size]))

    print(adjusted_rand_score(birch.predict(X), Y))

    # Show the training steps
    fig, ax = plt.subplots(5, 5, figsize=(20, 12))

    for i in range(5):
        for j in range(5):
            idx = (i * 5) + j

            for k in range(5):
                ax[i][j].scatter(X_batch[idx][Y_preds[idx] == k, 0], X_batch[idx][Y_preds[idx] == k, 1], s=3)

            ax[i][j].set_xticks([])
            ax[i][j].set_yticks([])
            ax[i][j].set_title('{} samples'.format(batch_size * (idx + 1)))

    plt.show()

```

### 双聚类

```python
from __future__ import print_function

import numpy as np
import matplotlib.pyplot as plt

from sklearn.cluster.bicluster import SpectralBiclustering


# Set random seed for reproducibility
np.random.seed(1000)


nb_users = 100
nb_products = 150
max_rating = 10


if __name__ == '__main__':
    # Create the user-product matrix
    up_matrix = np.random.randint(0, max_rating + 1, size=(nb_users, nb_products))
    mask_matrix = np.random.randint(0, 2, size=(nb_users, nb_products))
    up_matrix *= mask_matrix

    # Show the matrix
    fig, ax = plt.subplots(figsize=(12, 6))

    matx = ax.matshow(up_matrix)
    fig.colorbar(matx)

    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_xlabel('Products')
    ax.set_ylabel('Users')

    plt.show()

    # Perform a Spectral Biclustering
    sbc = SpectralBiclustering(n_clusters=10, random_state=1000)
    sbc.fit(up_matrix)

    # Show the clustered matrix
    up_clustered = np.outer(np.sort(sbc.row_labels_) + 1, np.sort(sbc.column_labels_) + 1)

    fig, ax = plt.subplots(figsize=(12, 6))

    matx = ax.matshow(up_clustered)

    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_xlabel('Products')
    ax.set_ylabel('Users')

    plt.show()

    # Show some examples of users and products associated with ranking 6
    print(np.where(sbc.rows_[6, :] == True))
    print(np.where(sbc.columns_[6, :] == True))
```






