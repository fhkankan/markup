# 降维

许多机器学习问题涉及训练实例的几千甚至上百万个特征。这不仅导致训练非常缓慢，也让让我们更加难以找到更好的解决方案。这个问题通常被称为**维度的诅咒**。

一般可以大量减少特征的数量，将棘手的问题转化为容易解决的问题。这样可以加快训练速度，同时降维对于数据可视化也非常有用。

数据降维确实会丢失一些信息，所欲，它虽然能够加速训练，但是也会轻微降低系统性能。同时也让流水线更为复杂，维护难度上升。所以，如果训练太慢，首先应该尝试的还是继续使用原始数据，然后再考虑数据降维。不过再某些情况下，降维可能会过滤掉一些不必要的噪声和细节，从而导致性能更好（通常 来说不会，它只会加速训练）。

## 方法

降维有两种主要方法：投影和流形学习。

- 投影

在大多数现实世界的问题里，训练实例在所有维度上并不是均匀分布的。许多特征几乎是不变的，也有许多特征是高度相关联的。因此，高维空间的所有训练实例实际上（或近似于）受一个低得多的低维子空间所影响。我们可以将每个训练实例投影到这个子空间中，这样就将数据集维度进行了降维护。

不过投影并不总是降维的最佳方法。在许多情况下，子空间可能会弯曲或扭动。

主要处理线性降维，常用的算法有：主成分分析(PCA)、核主成分分析(Kernel PCA)

- 流形学习

瑞士卷就是二维流形的一个例子。简单地说，2D流形就是一个能够在更高维空间里面弯曲和扭转的2D形状。更概括地说，d维流形就是n（d<n）维空间的一部分，局部类似于一个d维超平面。在瑞士卷的例子中，d=2,n=3：它局部类似于一个2D平面，但是在第三个维度上卷起。

许多降维算法是通过对训练实例进行流形 建模来实现的。这被称为**流形学习**。它依赖于**流形假设** ，也称为**流形假说**，认为大多数现实世界的高维度数据集存在一个低维度的流形来重新表示。这个假设通常是凭经验观察的。

流形假设通常还伴随一个隐含的假设：如果能用低维空间的流形表示，手头的任务（分类或回归）将变得更简单。但是，这个假设并不总是成立。

简而言之，在训练模型之前降低训练集的维度，肯定可以 加快训练速度，但这并不总是会导致更好或更 简单的解决方案，它取决于数据集。

主要处理非线性降维，常用的算法有：局部线性嵌入(LLE)、t-分步随机近邻嵌入(t-SNE)、Isomap算法

## PCA

主成分分析是最流行的降维算法。它先是识别出最接近数据的超平面，然后将数据投影其上。

### 保留差异性

将训练集投影到低维超平面之前，需要选择正确的超平面。数据投影到超平面上，需要保留最大的差异性，因为保留最大的差异性，可能丢失的信息更少。证明这一选择的方法为比较原始数据集与其轴上的投影之间的均方距离，使这个均方距离最小的轴是最合理的选择。
$$
Var(x) = \frac{1}{m}\sum_{i=1}^m(x_i-\bar{x})^2
$$
找到一个轴，使样本空间的所有点映射到这个轴后，方差最大

将样例的均值归0（demean），则方差公式简化为
$$
Var(x) = \frac{1}{m}\sum_{i=1}^mx_i^2
$$

### 主成分

- 目标函数

对于主成分分析：

> 对所有的样本进行demean处理，得到$X^{(i)}=(X_1^{(i)}, X_2^{(i)})$

> 求一个轴的方向$w = (w_1, w_2)$使得所有的样本，映射到w后，得到$X_{project}^{(i)}=(X_{pr1}^{(i)}, X_{pr2}^{(i)})$，有$Var(X_{project}) = \frac{1}{m}\sum_{i=1}^m{(X_{project}^{(i)}-\bar{X}_{project})^2}$最大，即$Var(X_{project}) = \frac{1}{m}\sum_{i=1}^m{\Arrowvert{X_{project}^{(i)}-\bar{X}_{project}\Arrowvert}^2}$最大值，最终即$Var(X_{project}) = \frac{1}{m}\sum_{i=1}^m{\Arrowvert{X_{project}^{(i)}\Arrowvert}^2}$最大

由于
$$
X^{(i)}\cdot{w} = \Arrowvert{X^{(i)}}\Arrowvert\cdot\Arrowvert{w}\Arrowvert\cdot\cos\theta
$$

$$
\Arrowvert{w}\Arrowvert= 1
$$

故
$$
X^{(i)}\cdot{w} = \Arrowvert{X^{(i)}}\Arrowvert\cdot\cos\theta=\Arrowvert{X_{project}^{(i)}}\Arrowvert
$$
则
$$
Var(X_{project}) = \frac{1}{m}\sum_{i=1}^m{\Arrowvert{X_{project}^{(i)}\Arrowvert}^2} = \frac{1}{m}\sum_{i=1}^m{({X^{(i)}\cdot{w})}^2}
$$
对于多维
$$
Var(X_{project}) =  \frac{1}{m}\sum_{i=1}^m{({X_1^{(i)}\cdot{w_1} + X_2^{(i)}\cdot{w_2} +\ldots + X_n^{(i)}\cdot{w_n})}^2}= \frac{1}{m}\sum_{i=1}^{m}(\sum_{j=1}^m{X_j^{(i)}w_j})^2
$$
求其最大值

- 求解

求$w$使得$f(X) = \frac{1}{m}\sum_{i=1}^m{({X_1^{(i)}\cdot{w_1} + X_2^{(i)}\cdot{w_2} +\ldots + X_n^{(i)}\cdot{w_n})}^2}$最大

> 方法一：数学求解

标准矩阵分解技术，叫做**奇异值分解(SVD)**。它可以精训练集矩阵 $X$ 分解为三个矩阵的点积 $U\cdot \sum \cdot V^T$，其中 $V^T$ 正包含我们想要的所有主成分。

> 方法二：梯度上升法

$$
\nabla{f} = \left( \begin{array}{ccc} {\partial{f}}/{\partial{w_1}} \\ {\partial{f}}/{\partial{w_2}} \\ \ldots\\ {\partial{f}}/{\partial{w_n}} \end{array} \right)= \frac{2}{m}\left( \begin{array}{ccc} {\sum_{i=1}^{m}{(X_1^{(i)}w_1 + \ldots+X_n^{(i)}w_n )X_1^{(i)}}} \\ {\sum_{i=1}^{m}{(X_2^{(i)}w_2 + \ldots+X_n^{(i)}w_n )X_2^{(i)}}} \\ \ldots\\ {\sum_{i=1}^{m}{(X_1^{(i)}w_1 + \ldots+X_n^{(i)}w_n )X_n^{(i)}}} \end{array} \right)
$$

简化
$$
\nabla{f} = \frac{2}{m}\left( \begin{array}{ccc} {\sum_{i=1}^{m}{(X^{(i)}w )X_1^{(i)}}} \\ {\sum_{i=1}^{m}{(X^{(i)}w )X_2^{(i)}}} \\ \ldots\\ {\sum_{i=1}^{m}{(X^{(i)}w )X_n^{(i)}}} \end{array} \right)
$$
向量化
$$
\frac{2}{m}\cdot(X^{(1)}w,X^{(2)}w,\ldots, X^{(m)}w )\cdot\left( \begin{array}{ccc}  X_1^{(1)} & X_2^{(1)} & \ldots & X_n^{(1)} \\ X_1^{(2)} & X_2^{(2)} & \ldots & X_n^{(2)} \\ \cdots &&& \cdots \\ X_1^{(m)} & X_2^{(m)} &\ldots & X_n^{(m)}\end{array} \right)
$$
则
$$
\frac{2}{m}\cdot(Xw)^T\cdot{X}
$$
转换行列
$$
\nabla{f} = \frac{2}{m}\left( \begin{array}{ccc} {\sum_{i=1}^{m}{(X^{(i)}w )X_1^{(i)}}} \\ {\sum_{i=1}^{m}{(X^{(i)}w )X_2^{(i)}}} \\ \ldots\\ {\sum_{i=1}^{m}{(X^{(i)}w )X_n^{(i)}}} \end{array} \right) = \frac{2}{m}\cdot{X^T}\cdot{(Xw)}
$$

- 下一主成分

数据进行改变，将数据在第一个主成分上的分量去掉

由于
$$
X^{(i)}\cdot{w} = \Arrowvert{X^{(i)}}\Arrowvert\cdot\cos\theta=\Arrowvert{X_{project}^{(i)}}\Arrowvert
$$

$$
X_{project}^{(i)} =\Arrowvert{X_{project}^{(i)}}\Arrowvert\cdot{w}
$$

则
$$
X^{'(i)} = X^{(i)}-X_{project}^{(i)}
$$
对新的$X^{'(i)}$求第一主成分，则是$X^{(i)}$的第二主成分，依次类推

- 主成分的方向不稳定

如果稍微打乱训练集，然后重新运行PCA，部分新的主成分可能指向跟原来的主成分相反的方向。但是，他们通常还是在同一条轴上。在某些情况下，两条主成分可能会旋转甚至交换，但是他们定义的平面还是不变。

### 投影与转换

一旦确定了所有主成分，就可以将数据集投影到前k个主成分定义的超平面 ，从而将数据集的维度降低到k维。这个超平面的选择，能确保投影尽可能多的差异性。
$$
X = \left( \begin{array}{ccc}  X_1^{(1)} & X_2^{(1)} & \ldots & X_n^{(1)} \\ X_1^{(2)} & X_2^{(2)} & \ldots & X_n^{(2)} \\ \cdots &&& \cdots \\ X_1^{(m)} & X_2^{(m)} &\ldots & X_n^{(m)}\end{array} \right)
$$

$$
W_k = \left( \begin{array}{ccc}  W_1^{(1)} & W_2^{(1)} & \ldots & W_n^{(1)} \\ W_1^{(2)} & W_2^{(2)} & \ldots & W_n^{(2)} \\ \cdots &&& \cdots \\ W_1^{(k)} & W_2^{(k)} &\ldots & W_n^{(k)}\end{array} \right)
$$

由$X$的`m*n`矩阵乘以`n*k`的$W_k$矩阵(前`k`个主成分)，形成`m*k`的矩阵$X_k$
$$
X\cdot{W_k^T} = X_k
$$
反向恢复，虽然丢失信息(与 $X$ 不同了)，但是还是可以由`m*k`转换为`m*n`维矩阵
$$
X_k\cdot{W_k} = X_m
$$

原始数据与重建数据之间的均方距离，称为**重建误差**。

### 手动实现

- 数学解法

```python
import numpy as np

m = 60
w1, w2 = 0.1, 0.3
noise = 0.1

angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5
X = np.empty((m, 3))
X[:, 0] = np.cos(angles) + np.sin(angles) / 2 + noise * np.random.randn(m) / 2
X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2
X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)

# 获取训练集中所有主成分，并提取前两个
X_centered = X - X.mean(axis=0)
U, s, Vt = np.linalg.svd(X_centered)
c1 = Vt.T[:, 0]
c2 = Vt.T[:, 1]

m, n = X.shape

S = np.zeros(X_centered.shape)
S[:n, :n] = np.diag(s)

# 比较是否相等
res = np.allclose(X_centered, U.dot(S).dot(Vt))
print(res)

# 将训练集投影到由前两个主成分定义的平面上
W2 = Vt.T[:, :2]
X2D = X_centered.dot(W2)
X2D_using_svd = X2D
print(X2D_using_svd)

```

- 梯度上升

第一主成分

```python
import numpy as np
import matplotlib.pyplot as plt

X = np.empty((100, 2))
X[:, 0] = np.random.uniform(0., 100., size=100)
X[:, 1] = 0.75 * X[:, 0] + 3. + np.random.normal(0, 10., size=100)


def demean(X):
    """均值归零"""
    return X - np.mean(X, axis=0)


def f(w, X):
    """目标函数"""
    return np.sum((X.dot(w) ** 2)) / len(X)


def df_math(w, X):
    """目标函数梯度"""
    return X.T.dot(X.dot(w)) * 2. / len(X)


def df_debug(w, X, epsilon=0.0001):
    """近似求导，通用"""
    res = np.empty(len(w))

    for i in range(len(w)):
        w_1 = w.copy()
        w_1[i] += epsilon
        w_2 = w.copy()
        w_2[i] -= epsilon
        res[i] = (f(w_1, X) - f(w_2, X)) / (2 * epsilon)
    return res


def direction(w):
    """w为单位向量"""
    return w / np.linalg.norm(w)


def gradient_ascent(df, X, initial_w, eta, n_iters=1e4, epsilon=1e-8):
    """批量梯度下降法"""
    w = direction(initial_w)
    cur_iter = 0
    while cur_iter < n_iters:
        gradient = df(w, X)
        last_w = w
        w = w + eta * gradient
        w = direction(w)  # 注意：每次求一个单位方向
        if (abs(f(w, X) - f(last_w, X)) < epsilon):
            break
        cur_iter += 1
    return w


initial_w = np.random.random(X.shape[1])  # 注意：不能用0向量开始
eta = 0.001
X_demean = demean(X)
# 注意：不能使用StandarScaler标准化数据
# debug测试
w = gradient_ascent(df_debug, X_demean, initial_w, eta)
print(w)
w = gradient_ascent(df_math, X_demean, initial_w, eta)
print(w)

# 绘图
plt.scatter(X_demean[:, 0], X_demean[:, 1])
plt.plot([0, w[0] * 30], [0, w[1] * 30], color='r')  # 由于w比较小，为可视化，扩大30倍
plt.show()

```

前n个主成分

```python
import numpy as np
import matplotlib.pyplot as plt

X = np.empty((100, 2))
X[:, 0] = np.random.uniform(0., 100., size=100)
X[:, 1] = 0.75 * X[:, 0] + 3. + np.random.normal(0, 10., size=100)


def demean(X):
    return X - np.mean(X, axis=0)


def f(w, X):
    return np.sum((X.dot(w) ** 2)) / len(X)


def df(w, X):
    return X.T.dot(X.dot(w)) * 2. / len(X)


def direction(w):
    """w为单位向量"""
    return w / np.linalg.norm(w)


def first_component(X, initial_w, eta, n_iters=1e4, epsilon=1e-8):
    """批量梯度下降法"""
    w = direction(initial_w)
    cur_iter = 0
    while cur_iter < n_iters:
        gradient = df(w, X)
        last_w = w
        w = w + eta * gradient
        w = direction(w)  # 注意：每次求一个单位方向
        if (abs(f(w, X) - f(last_w, X)) < epsilon):
            break
        cur_iter += 1
    return w


initial_w = np.random.random(X.shape[1])  # 注意：不能用0向量开始
eta = 0.001
X = demean(X)
# 求第一主成分的轴
w = first_component(X, initial_w, eta)
print(w)

# X2 = np.empty(X.shape)
# for i in range(len(X)):  # 循环求解
#   	X2[i] = X[i] - X[i].dot(w) * w

X2 = X - X.dot(w).reshape(-1, 1) * w  # 向量化

# 绘图
plt.scatter(X2[:, 0], X2[:, 1])
plt.show()

# 求第二主成分的轴
w2 = first_component(X2, initial_w, eta)
print(w2)


# 前n项主成分求解方法
def first_n_components(n, X, eta=0.01, n_iters=1e4, epsilon=1e-8):
    X_pca = X.copy()
    X_pca = demean(X_pca)
    res = []
    for i in range(n):
        initial_w = np.random.random(X_pca.shape[1])
        w = first_component(X_pca, initial_w, eta)
        res.append(w)
        X_pca = X_pca - X_pca.dot(w).reshape(-1, 1) * w
    return res


res = first_n_components(2, X)
print(res)

```

类创建

```python
import numpy as np


class PCA:

    def __init__(self, n_components):
        """初始化PCA"""
        assert n_components >= 1, "n_components must be valid"
        self.n_components = n_components  # n维主成分
        self.components_ = None

    def fit(self, X, eta=0.01, n_iters=1e4):
        """获得数据集X的前n个主成分"""
        assert self.n_components <= X.shape[1], \
            "n_components must not be greater than the feature number of X"

        def demean(X):
            """均值归零"""
            return X - np.mean(X, axis=0)

        def f(w, X):
            """目标函数"""
            return np.sum((X.dot(w) ** 2)) / len(X)

        def df(w, X):
            """目标函数梯度"""
            return X.T.dot(X.dot(w)) * 2. / len(X)

        def direction(w):
            """单位向量方向"""
            return w / np.linalg.norm(w)

        def first_component(X, initial_w, eta=0.01, n_iters=1e4, epsilon=1e-8):
            """梯度上升法求第一主成分"""
            w = direction(initial_w)
            cur_iter = 0

            while cur_iter < n_iters:
                gradient = df(w, X)
                last_w = w
                w = w + eta * gradient
                w = direction(w)
                if (abs(f(w, X) - f(last_w, X)) < epsilon):
                    break

                cur_iter += 1

            return w

        X_pca = demean(X)
        self.components_ = np.empty(shape=(self.n_components, X.shape[1]))
        for i in range(self.n_components):
            initial_w = np.random.random(X_pca.shape[1])
            w = first_component(X_pca, initial_w, eta, n_iters)
            self.components_[i, :] = w

            X_pca = X_pca - X_pca.dot(w).reshape(-1, 1) * w

        return self

    def transform(self, X):
        """将给定的X，映射到各个主成分分量中"""
        assert X.shape[1] == self.components_.shape[1]

        return X.dot(self.components_.T)

    def inverse_transform(self, X):
        """将给定的X，反向映射回原来的特征空间"""
        assert X.shape[1] == self.components_.shape[0]

        return X.dot(self.components_)

    def __repr__(self):
        return "PCA(n_components=%d)" % self.n_components


if __name__ == '__main__':
    import numpy as np
    import matplotlib.pyplot as plt

    X = np.empty((100, 2))
    X[:, 0] = np.random.uniform(0., 100., size=100)
    X[:, 1] = 0.75 * X[:, 0] + 3. + np.random.normal(0, 10., size=100)

    pca = PCA(n_components=2)  # 2维的数据
    pca.fit(X)
    print(pca.components_)

    pca = PCA(n_components=1)
    pca.fit(X)

    # 降维
    X_reduction = pca.transform(X)
    print(X_reduction.shape)

    # 恢复
    X_restore = pca.inverse_transform(X_reduction)
    print(X_restore.shape)

    # 绘图
    plt.scatter(X[:, 0], X[:, 1], color='b', alpha=0.5)
    plt.scatter(X_restore[:, 0], X_restore[:, 1], color='r', alpha=0.5)
    plt.show()

```

### Sklearn

#### 实现

scklearn的PCA类使用SVD分解来实现主成分分析。

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA

# 1.PCA概述
rng = np.random.RandomState(1)
X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T
plt.scatter(X[:, 0], X[:, 1])
plt.axis('equal')
plt.title("data")
plt.show()

pca = PCA(n_components=2)
# 主成分
pca.fit(X)
print(pca.components_)
"""
[[-0.94446029 -0.32862557]
 [-0.32862557  0.94446029]]
"""
pca.fit_transform(X)
print(pca.components_.T)
"""
[[-0.94446029 -0.32862557]
 [-0.32862557  0.94446029]]
"""
# 解释性
print(pca.explained_variance_)  # 可解释性差异
print(pca.explained_variance_ratio_)  # 方差解释率，表示每个主成分轴对整个数据集的方差的贡献度
"""
[0.7625315 0.0184779]
[0.97634101 0.02365899]
"""


# 画出指标
def draw_vector(v0, v1, ax=None):
    ax = ax or plt.gca()
    arrowprops = dict(arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0)
    ax.annotate('', v1, v0, arrowprops=arrowprops)


plt.scatter(X[:, 0], X[:, 1], alpha=0.2)
for length, vector in zip(pca.explained_variance_, pca.components_):
    v = vector * 3 * np.sqrt(length)
    draw_vector(pca.mean_, pca.mean_ + v)
plt.axis('equal')
plt.title("pca axis")
plt.show()

# 2.用PCA降维
pca = PCA(n_components=1)
pca.fit(X)
X_pca = pca.transform(X)
# X_pca = pca.fit_transform(X)
print('original shape: ', X.shape)
print('transformed shape: ', X_pca.shape)
"""
original shape:  (200, 2)
transformed shape:  (200, 1)
"""

# 数据降维逆变换后，与原始数据比较
X_new = pca.inverse_transform(X_pca)  # 逆转换
res = np.allclose(X, X_new)
print(res)  # False,不相等
plt.scatter(X[:, 0], X[:, 1], alpha=0.2)
plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)
plt.axis('equal')
plt.title("compare")
plt.show()

# 3.用PCA数据可视化
digits = load_digits()
print(digits.data.shape)
"""
(1797, 64)
图像数据是8px*8px
"""
# 可视化为2维
pca = PCA(2)
projected = pca.fit_transform(digits.data)
print(digits.data.shape)
print(projected.shape)
"""
(1797, 64)
(1797, 2)
"""

# 画出图
plt.scatter(projected[:, 0], projected[:, 1], c=digits.target, edgecolors='none',
            alpha=0.5, cmap=plt.cm.get_cmap('Spectral', 10))
plt.xlabel('component 1')
plt.ylabel('component 2')
plt.colorbar()
plt.show()

# 确定主成分的数量：累积方差贡献率
pca = PCA().fit(digits.data)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.title('cumulative')
plt.show()
```

#### 合适的维度

对于降低到合适的维度才能在效率和准确度上保持平衡的方法

```
1. 网格搜索，将维数信息作为超参数
2. 使用pca.explained_variance_ratio_
```

示例

```python
# 方法一：计算explained_variance_ratio_，选取n_components
pca = PCA(n_components=X_train.shape[1])
pca.fit(X_train)
pca.explained_variance_ratio_  # 查看各个维度所能代表信息的比率

# 可视化方法一
plt.plot([i for i in range(X_train.shape[1])],
        [np.sum(pca.explained_variance_ratio_[: i+1]) for i in range(X_train.shape[1])]
        )
# 可视化方法二
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.show()
# 非可视化方法三
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >=0.95) + 1


# 方法二：输入准确度，自动计算n_components
pca = PCA(0.95)
pca.fit(X_train)
pca.n_components_
```

#### 降噪

降低维度，丢失了信息，信息中也包含了噪声，故保留下来的信息，丢失需要的信息的同时也去除了部分噪声

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA


digits = load_digits()
print(digits.data.shape)

def plot_digits(data):
    fig, axes = plt.subplots(4, 10, figsize=(10, 4), subplot_kw={'xticks': [], 'yticks': []},
                             gridspec_kw=dict(hspace=0.1, wspace=0.1))
    for i, ax in enumerate(axes.flat):
        ax.imshow(data[i].reshape(8, 8), cmap='binary', interpolation='nearest', clim=(0, 16))


# 无噪音输入数据
plot_digits(digits.data)
plt.show()
# 有噪音输入数据
np.random.seed(42)
noisy = np.random.normal(digits.data, 4)
plot_digits(noisy)
plt.show()

# 用噪音数据训练一个PCA，要求保留50%的方差
pca = PCA(0.5).fit(noisy)
print(pca.n_components_)
components = pca.transform(noisy)
filtered = pca.inverse_transform(components)
plot_digits(filtered)
plt.show()
```

#### 压缩

```python
import numpy as np
import os

np.random.seed(42)

import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training"


def save_fig(fig_id, tight_layout=True):
    path = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id + ".png")
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format='png', dpi=300)


# Ignore useless warnings (see SciPy issue #5998)
import warnings

warnings.filterwarnings(action="ignore", message="^internal gelsd")

from sklearn.decomposition import PCA

try:
    from sklearn.datasets import fetch_openml

    mnist = fetch_openml('mnist_784', version=1)
    mnist.target = mnist.target.astype(np.int64)
except ImportError:
    from sklearn.datasets import fetch_mldata

    mnist = fetch_mldata('MNIST original')

from sklearn.model_selection import train_test_split

X = mnist["data"]
y = mnist["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y)
print(X_train.shape)  # (52500, 784)

pca = PCA()
pca.fit(X_train)
# 确定维度
# 方法一
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >= 0.95) + 1
print(d)  # 154
# 方法二
pca = PCA(n_components=0.95)
X_reduced = pca.fit_transform(X_train)
print(pca.n_components_)  # 154
print(np.sum(pca.explained_variance_ratio_))  # 0.95

# 逆转换
pca = PCA(n_components=154)
X_reduced = pca.fit_transform(X_train)
X_recovered = pca.inverse_transform(X_reduced)


def plot_digits(instances, images_per_row=5, **options):
    size = 28
    images_per_row = min(len(instances), images_per_row)
    images = [instance.reshape(size, size) for instance in instances]
    n_rows = (len(instances) - 1) // images_per_row + 1
    row_images = []
    n_empty = n_rows * images_per_row - len(instances)
    images.append(np.zeros((size, size * n_empty)))
    for row in range(n_rows):
        rimages = images[row * images_per_row: (row + 1) * images_per_row]
        row_images.append(np.concatenate(rimages, axis=1))
    image = np.concatenate(row_images, axis=0)
    plt.imshow(image, cmap=mpl.cm.binary, **options)
    plt.axis("off")


plt.figure(figsize=(7, 4))
plt.subplot(121)
plot_digits(X_train[::2100])
plt.title("Original", fontsize=16)
plt.subplot(122)
plot_digits(X_recovered[::2100])
plt.title("Compressed", fontsize=16)
plt.show()

save_fig("mnist_compression_plot")

```

#### 特征脸

$$
X = \left( \begin{array}{ccc}  X_1^{(1)} & X_2^{(1)} & \ldots & X_n^{(1)} \\ X_1^{(2)} & X_2^{(2)} & \ldots & X_n^{(2)} \\ \cdots &&& \cdots \\ X_1^{(m)} & X_2^{(m)} &\ldots & X_n^{(m)}\end{array} \right)
$$

$$
W_k = \left( \begin{array}{ccc}  W_1^{(1)} & W_2^{(1)} & \ldots & W_n^{(1)} \\ W_1^{(2)} & W_2^{(2)} & \ldots & W_n^{(2)} \\ \cdots &&& \cdots \\ W_1^{(k)} & W_2^{(k)} &\ldots & W_n^{(k)}\end{array} \right)
$$



由$X$的`m*n`矩阵乘以`n*k`的$W_k$矩阵(前`k`个主成分)，形成`m*k`的矩阵$X_k$
$$
X\cdot{W_k^T} = X_k
$$
其中$W_k$的每一行都可以视为$X$的n维特征转换后的重要度依次降低的k维特征

示例

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.decomposition import PCA

faces = datasets.fetch_lfw_people()
print(faces.keys())
print(faces.shape)
print(faces.images.shape)

random_indexes = np.random.permutation(len(faces.data))
X = faces.data[random_indexes]
example_faces = X[:36, :]
print(example_faces.shape)

def plot_faces(faces):
  	ig, axes = plt.subplots(
      6,6, figsize=(10, 10),
      subplot_kw={'xticks':[],'yticks':[]},
      gridspec_kw=dict(hspace=0.1,wspace=0.1)
      )
    for i, ax in enumerate(axes.flat):
      	ax.imshow(
        	data[i].reshape(62, 47),
          cmap='bone'
        )
    plt.show()
    
plot_faces(example_faces)


# 特征脸
pca = PCA(svd_solver='randomized')
pca.fit(X)
print(pca.components_.shape)
plot_faces(pca.components_[:36, :])


# 人脸识别数据库
faces2 = datasets.fetch_lfw_people(min_faces_per_person=60)
print(faces2.data.shape)
print(faces2.target_names)
print(len(faces2.target_names))
```

示例

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA

faces = fetch_lfw_people(min_faces_per_person=60)
print(faces.target_names, faces.images.shape)
"""
['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'
 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair'] 
 (1348, 62, 47)
"""
# 数据集比较大，使用一个随机方法来估计前N个主成分
pca = PCA(n_components=150, svd_solver='randomized')
pca.fit(faces.data)

# 画出特征脸
# 主成分被称作特征向量，这些图像被称作特征脸
# fig, axes = plt.subplots(3, 8, figsize=(9, 4), subplot_kw={'xticks':[], 'yticks':[]},
#                           gridspec_kw=dict(hspace=0.1, wspace=0.1))
# for i, ax in enumerate(axes.flat):
#     ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')
# plt.show()

# 查看累积方差
# plt.plot(np.cumsum(pca.explained_variance_ratio_))
# plt.xlabel('number of components')
# plt.ylabel('cumulative explained variance')
# plt.show()

# 计算成分和投影的人脸
pca = PCA(n_components=150, svd_solver='randomized').fit(faces.data)
components = pca.transform(faces.data)
projected = pca.inverse_transform(components)

fig, ax = plt.subplots(2, 10, figsize=(10, 2.5), subplot_kw={'xticks':[], 'yticks': []},
                       gridspec_kw=dict(hspace=0.1, wspace=0.1))
for i in range(10):
    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')
    ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')
ax[0, 0].set_ylabel('full-dim\ninput')
ax[1, 0].set_ylabel('150-dim\nreconstruction')
plt.show()

```

#### 增量PCA

前面的主成分分析的实现需要整个训练集都进入内存，才能运行SVD算法。有**增量主成分分析(IPCA)**算法：可以将训练集分成一个个小批量，一次给IPCA算法喂一个。对于大型训练集来说，这个方法很有用，并且还可以在线应用PCA。

```python
import numpy as np
import os

np.random.seed(42)

import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)


try:
    from sklearn.datasets import fetch_openml

    mnist = fetch_openml('mnist_784', version=1)
    mnist.target = mnist.target.astype(np.int64)
except ImportError:
    from sklearn.datasets import fetch_mldata

    mnist = fetch_mldata('MNIST original')

from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.decomposition import IncrementalPCA

X = mnist["data"]
y = mnist["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y)
print(X_train.shape)  # (52500, 784)

# 将数据集分为100个小批量
n_batches = 100
# 方法一：array_split+IncrementalPCA
inc_pca = IncrementalPCA(n_components=154)
# 分批喂数据
for X_batch in np.array_split(X_train, n_batches):
    print(".", end="")  # not shown in the book
    inc_pca.partial_fit(X_batch)

# 方法二：memmap+IncrementalPCA
# 使用memmap，允许操控一个存储在磁盘二进制文件里的大型数组，就好似它也完全在内存中一样，而memmap仅在需要时加载内存中需要的数据。
# 由于IncrementalPCA在任何时间都只是使用数组的一部分，因此内存的使用情况仍然受控，这时可以调用fit()
# 1.存储文件
filename = "my_mnist.data"
m, n = X_train.shape

X_mm = np.memmap(filename, dtype='float32', mode='write', shape=(m, n))
X_mm[:] = X_train
del X_mm  # 现在删除memmap()对象将触发其Python终结器，这将确保数据被保存到磁盘。
# 2.加载文件
X_mm = np.memmap(filename, dtype="float32", mode="readonly", shape=(m, n))

batch_size = m // n_batches
inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)
inc_pca.fit(X_mm)

# 绘制原始数据与PCA逆转数据对比
X_reduced = inc_pca.transform(X_train)
X_recovered_inc_pca = inc_pca.inverse_transform(X_reduced)


def plot_digits(instances, images_per_row=5, **options):
    size = 28
    images_per_row = min(len(instances), images_per_row)
    images = [instance.reshape(size, size) for instance in instances]
    n_rows = (len(instances) - 1) // images_per_row + 1
    row_images = []
    n_empty = n_rows * images_per_row - len(instances)
    images.append(np.zeros((size, size * n_empty)))
    for row in range(n_rows):
        rimages = images[row * images_per_row: (row + 1) * images_per_row]
        row_images.append(np.concatenate(rimages, axis=1))
    image = np.concatenate(row_images, axis=0)
    plt.imshow(image, cmap=mpl.cm.binary, **options)
    plt.axis("off")


plt.figure(figsize=(7, 4))
plt.subplot(121)
plot_digits(X_train[::2100])
plt.subplot(122)
plot_digits(X_recovered_inc_pca[::2100])
plt.tight_layout()
plt.show()

X_reduced_inc_pca = X_reduced

```

#### 随机PCA

随机PCA是一个随机算法，可以快速找到前d个主成分的近似值。它的计算复杂度是 $O(m\times d^2)+O(d^3)$，而不是 $O(m\times n^2)+O(n^3)$，所以当d远小于n时，它比前面的算法要快得多。

```python
import numpy as np
import os
import time

np.random.seed(42)

import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)


try:
    from sklearn.datasets import fetch_openml

    mnist = fetch_openml('mnist_784', version=1)
    mnist.target = mnist.target.astype(np.int64)
except ImportError:
    from sklearn.datasets import fetch_mldata

    mnist = fetch_mldata('MNIST original')

from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.decomposition import IncrementalPCA

X = mnist["data"]
y = mnist["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y)
print(X_train.shape)  # (52500, 784)

# 不同主成分下不同算法的比较
for n_components in (2, 10, 154):
    print("n_components =", n_components)
    regular_pca = PCA(n_components=n_components)  # PCA
    inc_pca = IncrementalPCA(n_components=n_components, batch_size=500)  # 增量PCA
    rnd_pca = PCA(n_components=n_components, random_state=42, svd_solver="randomized")  # 随机PCA

    for pca in (regular_pca, inc_pca, rnd_pca):
        t1 = time.time()
        pca.fit(X_train)
        t2 = time.time()
        print("    {}: {:.1f} seconds".format(pca.__class__.__name__, t2 - t1))
"""
n_components = 2
    PCA: 1.1 seconds
    IncrementalPCA: 7.5 seconds
    PCA: 1.1 seconds
n_components = 10
    PCA: 1.2 seconds
    IncrementalPCA: 9.0 seconds
    PCA: 1.3 seconds
n_components = 154
    PCA: 3.1 seconds
    IncrementalPCA: 11.9 seconds
    PCA: 3.3 seconds
"""
# 不同数据集下不同算法的比较
times_rpca = []
times_pca = []
sizes = [1000, 10000, 20000, 30000, 40000, 50000, 70000, 100000, 200000, 500000]
for n_samples in sizes:
    X = np.random.randn(n_samples, 5)
    pca = PCA(n_components=2, svd_solver="randomized", random_state=42)  # 随机PCA
    t1 = time.time()
    pca.fit(X)
    t2 = time.time()
    times_rpca.append(t2 - t1)
    pca = PCA(n_components=2)  # PCA
    t1 = time.time()
    pca.fit(X)
    t2 = time.time()
    times_pca.append(t2 - t1)

plt.plot(sizes, times_rpca, "b-o", label="RPCA")
plt.plot(sizes, times_pca, "r-s", label="PCA")
plt.xlabel("n_samples")
plt.ylabel("Training time")
plt.legend(loc="upper left")
plt.title("PCA and Randomized PCA time complexity ")
plt.show()

# 相同数据集不同特征下不同算法的比较
times_rpca = []
times_pca = []
sizes = [1000, 2000, 3000, 4000, 5000, 6000]
for n_features in sizes:
    X = np.random.randn(2000, n_features)
    pca = PCA(n_components=2, random_state=42, svd_solver="randomized")  # 随机PCA 
    t1 = time.time()
    pca.fit(X)
    t2 = time.time()
    times_rpca.append(t2 - t1)
    pca = PCA(n_components=2)  # PCA
    t1 = time.time()
    pca.fit(X)
    t2 = time.time()
    times_pca.append(t2 - t1)

plt.plot(sizes, times_rpca, "b-o", label="RPCA")
plt.plot(sizes, times_pca, "r-s", label="PCA")
plt.xlabel("n_features")
plt.ylabel("Training time")
plt.legend(loc="upper left")
plt.title("PCA and Randomized PCA time complexity ")
plt.show()
```

## 线性判别分析

线性判别分析（Linear Discriminant Analysis, LDA）是一种有监督的线性分类器，其原理与PCA非常类似，也是将原市特征数据映射到新的维度，按照各新维度上方差从大到小排序，删除其中较小方差的部分以达到降维的目的。LDA与PCA不同的地方在于其在生成新维度时多考虑了每种标签样本数据集内的分布情况。

- 双重标准

因为务监督环境中所有样本数据没有分类标签，所以PCA在为样本集定义全新的特征维度时不得不通盘考虑所有的样本数据。而LDA可以堪称在有数据标签时的PCA样本，它将寻找新的特征映射正交基时的目标从“最大化所有样本在新维度上的方差”改为了双重标准------“最大化类间样本的方差、最小化类内样本的方差”。

LDA实现过程与PCA非常类似，设样本数据由n种标签的m维数据构成，可通过如下几步完成LDA计算：

1. 对每一类标签数据计算其在原始各维度上的均值，得到n个m维均值向量，记为$U{1\cdots}U_{n}$;
2. 计算类内原始各维度的协方差矩阵$S_{w}=\sum_{i=1}^n{S_i}$，其中$S_i$是每种标签样本的协方差矩阵，$S_w$是一个形如$m\times m$的矩阵。
3. 计算类间协方差矩阵$S_{B}=\sum_{i=1}^n{N_{i}(U_i-U)(U_i-U)^T}$，其中$N_i$是各标签样本总数，$U_i$是各标签均值向量，$U$是所有样本的均值向量，$S_{B}$是一个$n\times n$的矩阵。
4. 计算矩阵$S_{W}^{-1}S_{B}$的特征值与特征向量
5. 将特征值从大到小排列，取特征值最大的$m$个特征向量组成样本线性映射矩阵$\tilde{U}^T$。
6. 降维后的样本向量矩阵$Y=\tilde{U}^TX$

由于$S_W$是M阶方阵，$S_n$是n阶方阵，因此用$S_W^{-1}S_{B}$进行降维后的最大维数既受N的约束，也受M的约束；而PCA降维后的最大维数只受M影响。

- 实现

API

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

# 参数
Solver
# 计算新正价基坐标系的方法，可选svd, lsqr,eigen即奇异值分解、最小二乘法、特征值分解
n_components
# 降维后的维度数，必须小于“标签总数-1”
```

示例

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

iris = datasets.load_iris()
X = iris.data
y = iris.target

# PCA
plt.subplot(121)
pca = PCA(n_components=2).fit(X)
X_pca = pca.transform(X)
print(pca.explained_variance_radio_)  # 降维后各维度上的方差比

for name , label ,m in [('Setosa', 0, "<"),("Versicolor", 1, "o"), ("Virginica", 2, ">")]:
    plt.scatter(X_pca[y==label, 0], X_pca[y==label, 1],label=name, marker=m)

plt.legend()
plt.title("PCA")

# LDA
plt.subplot(122)
lda = LDA(n_components=2).fit(X, y)
X_lda = lda.transform(X)
print(lda.explained_variance_radio_)  # 降维后各维度上的方差比

for name , label ,m in [('Setosa', 0, "<"),("Versicolor", 1, "o"), ("Virginica", 2, ">")]:
    plt.scatter(X_pca[y==label, 0], X_pca[y==label, 1],label=name, marker=m)

plt.legend()
plt.title("LDA")
plt.show()


```

## 核主成分分析

PCA能很好地降低维度，但PCA是以线性方式工作的，如果数据集不是以线性方式组织的，则PCA不能实现其功嗯那个。但是核主成分分析可以很好地解决这个问题。

核技巧，隐性地将实例映射到非常高维的空间。事实证明，同样的核技巧也可应用于PCA，使复杂的非线性投影降维称为可能。这就是**核主成分分析(kPCA)**。它擅长在投影后保留实例的集群，有时甚至也能展开近似于一个扭曲流形的数据集。

### sklearn

```python
import numpy as np
import os
import time

from sklearn.datasets import make_swiss_roll

np.random.seed(42)

import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training"


def save_fig(fig_id, tight_layout=True):
    path = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id + ".png")
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format='png', dpi=300)


# Ignore useless warnings (see SciPy issue #5998)
import warnings

warnings.filterwarnings(action="ignore", message="^internal gelsd")


X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)

# 原数据瑞士卷
ax = plt.subplot(111, projection='3d')
ax.view_init(10, -70)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.hot, marker="x")
ax.set_xlabel("")
ax.set_ylabel("")
ax.set_zlabel("")
ax.set_xticklabels([])
ax.set_yticklabels([])
ax.set_zticklabels([])
ax.set_title("ori")

save_fig("oriimage_plot", tight_layout=False)
plt.show()

from sklearn.decomposition import KernelPCA

# 使用了不同核函数降维到二维的瑞士卷
lin_pca = KernelPCA(n_components=2, kernel="linear", fit_inverse_transform=True)
rbf_pca = KernelPCA(n_components=2, kernel="rbf", gamma=0.0433, fit_inverse_transform=True)
sig_pca = KernelPCA(n_components=2, kernel="sigmoid", gamma=0.001, coef0=1, fit_inverse_transform=True)

y = t > 6.9

plt.figure(figsize=(11, 4))
for subplot, pca, title in ((131, lin_pca, "Linear kernel"), (132, rbf_pca, "RBF kernel, $\gamma=0.04$"),
                            (133, sig_pca, "Sigmoid kernel, $\gamma=10^{-3}, r=1$")):
    X_reduced = pca.fit_transform(X)
    if subplot == 132:
        X_reduced_rbf = X_reduced

    plt.subplot(subplot)
    # plt.plot(X_reduced[y, 0], X_reduced[y, 1], "gs")
    # plt.plot(X_reduced[~y, 0], X_reduced[~y, 1], "y^")
    plt.title(title, fontsize=14)
    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)
    plt.xlabel("$z_1$", fontsize=18)
    if subplot == 131:
        plt.ylabel("$z_2$", fontsize=18, rotation=0)
    plt.grid(True)

save_fig("kernel_pca_plot")
plt.show()

# 降维逆转后的瑞士卷
plt.figure(figsize=(6, 5))

X_inverse = rbf_pca.inverse_transform(X_reduced_rbf)

ax = plt.subplot(111, projection='3d')
ax.view_init(10, -70)
ax.scatter(X_inverse[:, 0], X_inverse[:, 1], X_inverse[:, 2], c=t, cmap=plt.cm.hot, marker="x")
ax.set_xlabel("")
ax.set_ylabel("")
ax.set_zlabel("")
ax.set_xticklabels([])
ax.set_yticklabels([])
ax.set_zticklabels([])
ax.set_title("inverse")
save_fig("preimage_plot", tight_layout=False)
plt.show()
```

### 超参数

由于kPCA是一种无监督的学习算法，因此没有明显的性能指标来选择最佳的和函数和超参数值。而降维通常是监督式学习任务的准备步骤，所以可以使用网格搜索，来找到使任务性能最佳的核和超参数。

还有一种完全不受监督方法，就是选择重建误差最低的核和超参数。由于特征空间是无限维度，无法计算出重建点，因此也无法计算出真实的重建误差。但是幸好，可以在原始空间中找到一个点，使其映射接近于重建点。这称为**重建原像**。一旦有了这个原像，就可以测量它到原始实例的平方距离。最后，便可以选择使这个重建原像误差最小化的核和超参数。执行重建的一个方法是：训练一个监督式回归模型，以投影后的实例作为训练集，并以原始实例作为目标。

```python
import numpy as np
import os

np.random.seed(42)

import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training"


def save_fig(fig_id, tight_layout=True):
    path = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id + ".png")
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format='png', dpi=300)


# Ignore useless warnings (see SciPy issue #5998)
import warnings

warnings.filterwarnings(action="ignore", message="^internal gelsd")

from sklearn.datasets import make_swiss_roll
from sklearn.decomposition import KernelPCA
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)
y = t > 6.9

ax = plt.subplot(111, projection='3d')
ax.view_init(10, -70)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.hot, marker="x")
ax.set_xlabel("")
ax.set_ylabel("")
ax.set_zlabel("")
ax.set_xticklabels([])
ax.set_yticklabels([])
ax.set_zticklabels([])
ax.set_title("ori")

save_fig("oriimage_plot", tight_layout=False)
plt.show()

# 方法一：监督式
clf = Pipeline([
    ("kpca", KernelPCA(n_components=2)),
    ("log_reg", LogisticRegression(solver="liblinear"))
])

param_grid = [{
    "kpca__gamma": np.linspace(0.03, 0.05, 10),
    "kpca__kernel": ["rbf", "sigmoid"]
}]

grid_search = GridSearchCV(clf, param_grid, cv=3)
grid_search.fit(X, y)

print(grid_search.best_params_)
"""
{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}
"""

# 方法二：非监督式
# 设置fit_inverse_transform=True会自动执行重建
rbf_pca = KernelPCA(n_components=2, kernel="rbf", gamma=0.0433,
                    fit_inverse_transform=True)
X_reduced = rbf_pca.fit_transform(X)
X_preimage = rbf_pca.inverse_transform(X_reduced)  # 只有fit_inverse_transform=True时有inverse_transform方法
# 计算重建原像误差
res = mean_squared_error(X, X_preimage)
print(res)
"""
1.0538494967480354e-26
"""
```

## 流形学习

PCA虽然是一个灵活、快速且容易解释的算法，但是它对存在**非线性**关系的数据集的处理效果并不太好。

流形学习弥补了这个缺陷。流形学习是一种无监督评估器，试图将一个低维度流形嵌入到一个高纬度空间来描述数据集。

- 与PCA对比

```
1.在流形学习中，并没有好的框架来处理缺失值。相比之下，PCA算法中有一个用于处理缺失值的迭代方法
2.在流形学习中，数据中噪音的出现将造成流形短路，并且严重影响嵌入结果。相比之下，PCA可以自然地从最重要的成分中滤除噪音
3.流形嵌入的结果通常高度依赖所选取的邻节点的个数，并且通常没有确定的定量方式来选择最优的邻节点个数。相比之下，PCA算法中并不存在这样的问题
4.在流形学习中，全局最优的输出维度数很难确定。相比之下，PCA可以基于解释方差来确定输出的维度数
5.在流形学习中，嵌入维度的含义并不总是很清楚。在PCA算法中，主成分有非常明确的含义
6.在流形学习中，流形方法的计算复杂度为O[N^2]或O[N^3]。而PCA可以选择随机方法，通常速度更快
```

虽然以上为流形学习相比PCA算法的缺点，但是流形学习还有一个明显的优点：它具有保留数据中的非线性关系的能力。通常做法：先用PCA探索数据的线性特征，再用流形方法探索数据的非线性特征。

- 常见sklearn-API

MDS

```python
from sklearn.manifold import MDS
# 多维缩放算法，保持实例之间的距离，降低维度
```

LLE

```python
from sklearn.manifold import LocallyLinearEmbedding
# 对于简单问题，如S曲线、局部线性嵌入及其变体学习效果非常好
```

Isomap

```python
from sklearn.manifold import Isomap
# 对现实世界的高纬度数据源学习效果较好

# 初始化参数
n_neightbors  # 虚招样本近邻时"最近的k个邻居"中k的值
n_components	# 降维后的维度数量
eigen_solver  # 计算矩阵特征值的算法，arpack使用Arnoldi迭代算法，适用于稀疏矩阵，dense使用直接接发，适用于稠密矩阵
tol,max_iter  # 控制Arnoldi算法的迭代参数
path_method		# 测地线最短路径的生成算法，FW使用Floyd-Warshall,时间复杂度是O(N^3).D使用Dijkstra算法，时间复杂度是O(N^2(k+log(N)))，其中N是样本数量，k是原始维度数
neighbors_algorithm  # 近邻搜索算法，可选brute,kd_tree, ball_tree

# 模型属性
embedding_  # 样本降维后的结果
kenel_pca_	# 使用KernelPCA作为降维算法，可以通过本属性读取KernelPCA对象
nbrs_				# 以sklearn.neighbors.NearestNeighbors对象白哦大的样本近邻结构。
dist_matrix_	# 测地线距离矩阵
```

t-SNE

```python
from sklearn.manifold import TSNE
# 用于高度聚类的数据效果较好，但是学习速度较慢
```

### 线性嵌入

线性嵌入：将数据旋转、平移和缩放到一个高维空间的操作。

多维标度法（MDS）
虽然从(x,y)坐标计算这个距离矩阵很简单，但是从距离矩阵转换回x坐标和y坐标值却比较困难
多维标度法可以解决这个问题：可以将一个数据集的距离矩阵还原称一个D维坐标来表示数据集

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.image import imread
import seaborn as sns
from sklearn.manifold import MDS
from sklearn.metrics import pairwise_distances
from mpl_toolkits import mplot3d


# 定义一个流形
def make_hello(N=1000, rseed=42):
    #  画出"HELLO"文字形状的图像，并保存为PNG
    fig, ax = plt.subplots(figsize=(4, 1))
    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)
    ax.axis('off')
    ax.text(0.5, 0.4, 'HELLO', va='center', ha='center', weight='bold', size=85)
    fig.savefig('hello.png')
    plt.close(fig)

    # 打开png，并将一些随机点画进去
    data = imread('hello.png')[::-1, :, 0].T
    rng = np.random.RandomState(rseed)
    X = rng.rand(4 * N, 2)
    i, j = (X * data.shape).astype(int).T
    mask = (data[i, j] < 1)
    X = X[mask]
    X[:, 0] *= (data.shape[0] / data.shape[1])
    X = X[:N]
    return X[np.argsort(X[:, 0])]


X = make_hello(1000)
colorize = dict(c=X[:, 0], cmap=plt.cm.get_cmap('rainbow', 5))


# plt.scatter(X[:, 0], X[:, 1], **colorize)
# plt.axis('equal')
# plt.show()

# 用旋转矩阵旋转数据，x和y的值会改变，但是数据形状基本还是一样
def rotate(X, angle):
    theta = np.deg2rad(angle)
    R = [[np.cos(theta), np.sin(theta)], [-np.sin(theta), np.cos(theta)]]
    return np.dot(X, R)


X2 = rotate(X, 20) + 5
# plt.scatter(X2[:, 0], X2[:, 1], **colorize)
# plt.axis('equal')
# plt.show()
# 说明x和y的值并不是数据间关系的必要基础特征。真正的基础特征是每个点与数据集中其他点的距离
# 表示这种关系的常用方法是关系（距离）矩阵：对于N个点，构建一个N*N的矩阵，元素(i,j)是点i和点j之间的距离

# 计算关系矩阵
D = pairwise_distances(X)
print(D.shape)  # (1000, 1000)
# N=1000个点，获得一个1000*1000的矩阵
# 画出矩阵
# plt.imshow(D, zorder=2, cmap='Blues', interpolation='nearest')
# plt.colorbar()
# plt.show()
# 为做过旋转和平移的数据构建一个距离矩阵
D2 = pairwise_distances(X2)
print(np.allclose(D, D2))  # True
# 这个距离矩阵给出了一个数据集内部关系的表现形式，这种形式与数据集的旋转和投影无关。
# 距离矩阵的可视化效果不够直观

# 多维标度法（MDS）
# 虽然从(x,y)坐标计算这个距离矩阵很简单，但是从距离矩阵转换回x坐标和y坐标值却比较困难
# 多维标度法可以解决这个问题：可以将一个数据集的距离矩阵还原称一个D维坐标来表示数据集
# model = MDS(n_components=2, dissimilarity='precomputed', random_state=1)
# out = model.fit_transform(D)
# plt.scatter(out[:, 0], out[:, 1], **colorize)
# plt.axis('equal')
# plt.show()
# 仅仅依靠描述数据点间关系的N*N距离矩阵 ，MDS算法就可以为数据还原出一种可行二维坐标

# 将MSD用于流形学习
# 三维空间
def random_project(X, dimension=3, rseed=42):
    assert dimension >= X.shape[1]
    rng = np.random.RandomState(rseed)
    C = rng.randn(dimension, dimension)
    e, v = np.linalg.eigh(np.dot(C, C.T))
    return np.dot(X, v[:X.shape[1]])


X3 = random_project(X, 3)
print(X3.shape)  # (1000, 3)

# 画出图
# ax = plt.axes(projection='3d')
# ax.scatter3D(X3[:, 0], X3[:, 1], X3[:, 2], **colorize)
# ax.view_init(azim=70, elev=50)
# plt.show()

# 计算三维数据的距离矩阵，得出距离矩阵的最优二维嵌入结果
model = MDS(n_components=2, random_state=1)
out3 = model.fit_transform(X3)
# plt.scatter(out3[:, 0], out3[:, 1], **colorize)
# plt.axis('equal')
# plt.show()

```

### 非线形嵌入

当嵌入是非线性时，即超越简单的操作集合时，MDS算法失效。

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.image import imread
import seaborn as sns
from mpl_toolkits import mplot3d
from sklearn.manifold import MDS


# 定义一个流形
def make_hello(N=1000, rseed=42):
    #  画出"HELLO"文字形状的图像，并保存为PNG
    fig, ax = plt.subplots(figsize=(4, 1))
    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)
    ax.axis('off')
    ax.text(0.5, 0.4, 'HELLO', va='center', ha='center', weight='bold', size=85)
    fig.savefig('hello.png')
    plt.close(fig)

    # 打开png，并将一些随机点画进去
    data = imread('hello.png')[::-1, :, 0].T
    rng = np.random.RandomState(rseed)
    X = rng.rand(4 * N, 2)
    i, j = (X * data.shape).astype(int).T
    mask = (data[i, j] < 1)
    X = X[mask]
    X[:, 0] *= (data.shape[0] / data.shape[1])
    X = X[:N]
    return X[np.argsort(X[:, 0])]


X = make_hello(1000)
colorize = dict(c=X[:, 0], cmap=plt.cm.get_cmap('rainbow', 5))


# plt.scatter(X[:, 0], X[:, 1], **colorize)
# plt.axis('equal')
# plt.show()

# 定义一个三维空间中扭曲成S的形状
def make_hello_s_curve(X):
    t = (X[:, 0] - 2) * 0.75 * np.pi
    x = np.sin(t)
    y = X[:, 1]
    z = np.sign(t) * (np.cos(t) - 1)
    return np.vstack((x, y, z)).T


XS = make_hello_s_curve(X)

# 画图
# ax = plt.axes(projection='3d')
# ax.scatter3D(XS[:, 0], XS[:, 1], XS[:, 2], **colorize)
# plt.show()
# 虽然数据点间基本的关系仍然存在，但是这次数据以非线性的方式进行了变换：被包裹成了S形
# MSD
model = MDS(n_components=2, random_state=2)
outS = model.fit_transform(XS)
plt.scatter(outS[:, 0], outS[:, 1], **colorize)
plt.axis('equal')
plt.show()
# 尝试使用MDS处理这个数据，则无法展示数据非线性嵌入的特征，进而导致丢失了这个嵌入式流形的内部基本关系特性
```

### 局部线性嵌入

MSD生成的嵌入模型，会试图保留数据集中每对数据点间的距离。局部性嵌入(LLE)生成的嵌入模型，该方法不保留所有的距离，仅保留邻节点的距离。

LLE算法通过某种方式将卷曲的数据展开，并且线段的长度基本保持不变。通过对成本函数的全局优化来反映这个逻辑。

LLE首先测量每个算法如何与其最近的邻居线性相关，然后为训练集寻找一个能最大程度保留这些局部关系的低维表示。这使得它特别擅长展开弯曲的流形，特别是没有太多噪声时。

```python
# 局部线性嵌入
from sklearn.manifold import LocallyLinearEmbedding
model = LocallyLinearEmbedding(n_neighbors=100, n_components=2, method='modified', eigen_solver='dense')
out = model.fit_transform(XS)
fig, ax = plt.subplots()
ax.scatter(out[:, 0], out[:, 1], **colorize)
ax.set_ylim(0.15, -0.15)
plt.show()
# 虽然有一定变形，但是保留了数据的基本关系特性
```

示例2

```python
import numpy as np
import os

np.random.seed(42)

import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training"


def save_fig(fig_id, tight_layout=True):
    path = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id + ".png")
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format='png', dpi=300)


# Ignore useless warnings (see SciPy issue #5998)
import warnings

warnings.filterwarnings(action="ignore", message="^internal gelsd")

from sklearn.datasets import make_swiss_roll
from sklearn.manifold import LocallyLinearEmbedding

X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)
y = t > 6.9

ax = plt.subplot(111, projection='3d')
ax.view_init(10, -70)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.hot, marker="x")
ax.set_xlabel("")
ax.set_ylabel("")
ax.set_zlabel("")
ax.set_xticklabels([])
ax.set_yticklabels([])
ax.set_zticklabels([])
ax.set_title("ori")

save_fig("oriimage_plot", tight_layout=False)
plt.show()

# 局部线性嵌入
lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)
X_reduced = lle.fit_transform(X)

plt.title("Unrolled swiss roll using LLE", fontsize=14)
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)
plt.xlabel("$z_1$", fontsize=18)
plt.ylabel("$z_2$", fontsize=18)
plt.axis([-0.065, 0.055, -0.1, 0.12])
plt.grid(True)

save_fig("lle_unrolling_plot")
plt.show()

```

### 各方法对比

```python
import numpy as np
import os

np.random.seed(42)

import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training"


def save_fig(fig_id, tight_layout=True):
    path = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id + ".png")
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format='png', dpi=300)


# Ignore useless warnings (see SciPy issue #5998)
import warnings

warnings.filterwarnings(action="ignore", message="^internal gelsd")

from sklearn.datasets import make_swiss_roll

X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)
y = t > 6.9

ax = plt.subplot(111, projection='3d')
ax.view_init(10, -70)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.hot, marker="x")
ax.set_xlabel("")
ax.set_ylabel("")
ax.set_zlabel("")
ax.set_xticklabels([])
ax.set_yticklabels([])
ax.set_zticklabels([])
ax.set_title("ori")

save_fig("oriimage_plot", tight_layout=False)
plt.show()

# 使用不同的技术将瑞士卷降为2D
from sklearn.manifold import LocallyLinearEmbedding

lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)
X_reduced_lle = lle.fit_transform(X)

from sklearn.manifold import MDS

mds = MDS(n_components=2, random_state=42)
X_reduced_mds = mds.fit_transform(X)

from sklearn.manifold import Isomap

isomap = Isomap(n_components=2)
X_reduced_isomap = isomap.fit_transform(X)

from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, random_state=42)
X_reduced_tsne = tsne.fit_transform(X)

titles = ['LLE', "MDS", "Isomap", "t-SNE"]

plt.figure(figsize=(15, 4))

for subplot, title, X_reduced in zip((141, 142, 143, 144), titles,
                                     (X_reduced_lle, X_reduced_mds, X_reduced_isomap, X_reduced_tsne)):
    plt.subplot(subplot)
    plt.title(title, fontsize=14)
    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)
    plt.xlabel("$z_1$", fontsize=18)
    if subplot == 131:
        plt.ylabel("$z_2$", fontsize=18, rotation=0)
    plt.grid(True)

save_fig("other_dim_reduction_plot")
plt.show()

```

## 盲源分离

**盲源分离**是指将信号从混合体中分离出来的过程。假设一组不同的信号发生器生成了不同的信号，而一个公共接收机接收了所有这些信号。盲源分析就是用这些信号的性质将这些信号从混合体中分离出来，使用**独立成分分析**(Independent Components Analysis，ICA)算法来实现。

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal

from sklearn.decomposition import PCA, FastICA

# 加载数据，每行类似:6.469001940617275181e+00 8.076526596732451324e+00 9.426766749221311414e+00 5.838134436994008247e+00
input_file = 'mixture_of_signals.txt'
X = np.loadtxt(input_file)

# 创建ICA对象
ica = FastICA(n_components=4)

# 重构信号
signals_ica = ica.fit_transform(X)

# 提取混合矩阵
mixing_mat = ica.mixing_  

# 执行PCA 
pca = PCA(n_components=4)
signals_pca = pca.fit_transform(X)  # 基于正交成分重构信号

# 定义画图参数 
models = [X, signals_ica, signals_pca]
colors = ['blue', 'red', 'black', 'green']

# 画出输入信号
plt.figure()
plt.title('Input signal (mixture)')
for i, (sig, color) in enumerate(zip(X.T, colors), 1):
    plt.plot(sig, color=color)

# 画出用ICA分离的信号
plt.figure()
plt.title('ICA separated signals')
plt.subplots_adjust(left=0.1, bottom=0.05, right=0.94, 
        top=0.94, wspace=0.25, hspace=0.45)
for i, (sig, color) in enumerate(zip(signals_ica.T, colors), 1):
    plt.subplot(4, 1, i)
    plt.title('Signal ' + str(i))
    plt.plot(sig, color=color)

# 画出用PCA分离的信号 
plt.figure()
plt.title('PCA separated signals')
plt.subplots_adjust(left=0.1, bottom=0.05, right=0.94, 
        top=0.94, wspace=0.25, hspace=0.45)
for i, (sig, color) in enumerate(zip(signals_pca.T, colors), 1):
    plt.subplot(4, 1, i)
    plt.title('Signal ' + str(i))
    plt.plot(sig, color=color)

plt.show()
```

















