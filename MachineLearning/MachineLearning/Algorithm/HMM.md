# 隐马尔可夫

Hidden Markov Model

隐马尔可夫模型是一个带有隐含状态的基于统计方法的马尔可夫模型，其在有状态的智能系统中有丰富的应用空间，如金融统计、语音输入识别、汉语输入法等。隐马尔可夫模型以贝叶斯网络作为数据框架，用哪个Baum-Welch算法进行务监督训练，通过Viterbi算法计算隐含状态。

## 场景建模

### 两种状态链

隐马尔可夫模型围绕两组状态序列进行建模，它们分别是现实状态链和隐藏状态链。

- 马尔可夫模型

马尔可夫模型最初是离散随机过程链，后来一般化到可数的无限状态空间的状态表示。

1. 它表示的是一个有顺序的状态转换模型，可以用$t_1,t_2,\cdots,t_n$的方式表示这个序列。
2. 序列中的每个结点可以取值为若干状态之一（或是连续区间中的一个值），但一个结点不可以有多个状态。
3. 这个状态序列可以是无限长的。
4. 该模型可以用来沿着这个状态链“以概率的形式根据历史状态预测未来状态”。
5. 每次状态的预测只以其最近的一个状态作为依据，比如预测$t_5$的状态时只用$t_4$的状态作为依据，而不去考虑$t_1，t_2，t_3$的状态。

常见场景：天气状态链、股票状态链、文本链

- 隐马尔可夫模型

马尔可夫模型进行扩充，建立一个由隐藏层和可见层组成的两层模型，底层隐藏层由马尔可夫状态链组成，可见层由普通状态结点组成。因此，该模型被命名为隐马尔可夫模型，训练该模型的算法是B aum-Welch算法。

理解隐马尔可夫的关键点：1.显示层的结点状态是可以直接获取的，2.作为隐藏层的马尔可夫链的各结点状态不直接可见，需要用可见层的状态去推测。

### 两种概率

HMM中的状态链中的可能、一定程度、非绝对等最终是通过模型中的两种概率表来起具体作用的。

- 输出概率

输出概率是隐含层的每一种状态的结点都以一定的概率表现为可见层的结点状态。

| 序号 | 隐藏层状态 | 可见层状态 | 输出概率 |
| ---- | ---------- | ---------- | -------- |
| 1    | 晴         | 打球       | 0.4      |
| 2    | 晴         | 读书       | 0.3      |
| 3    | 晴         | 访友       | 0.3      |

可以把隐藏层状态和可见层状态看成一组因果关系：隐藏层是因，可见层是果。此表有如下特点：

1. 每一种隐藏层状态对各可见层状态的输出概率总和为1.
2. 可见层每种状态的“被输出概率”总和可以是任意值。

- 转换概率

转换概率是隐藏层内部各状态之间的固有转换规律。

| 序号 | T时刻隐藏层状态 | T+1时刻隐藏层状态 | 转换概率 |
| ---- | --------------- | ----------------- | -------- |
| 1    | 晴              | 晴                | 0.7      |
| 2    | 晴              | 阴                | 0.2      |
| 3    | 晴              | 雨                | 0.1      |

### 三种问题

在模型的隐藏层状态、乐见层状态、输出概率、转换概率等几种数据中，通常有一些是未知的，而HMM的目的就是找出这些未知数据。通常可以把这些问题归结为三这个类型：

- 状态问题：已知模型的所有参数（隐藏层状态类型、可见层状态了行、输出概率表、转换概率表），求任意一个可见层状态链产生的概率。
- 解码问题：隐藏层状态猜测。已知模型的所有参数，并且知道某一组可见层状态链，求这些可见状态下对应的隐藏状态链是什么。
- 训练模型参数。已知隐藏层状态数量、可见层状态数量、足够长的可见状态链，求模型中的输出概率表和转换状态表。自然，已知的可见层状态链越长，模型参数的推测也越准确。

问题一、二可以和问题三结合起来解决现实问题：1.已知一系列的可见层状态，求某一段可见层的概率。2.已知一系列的可见层状态，求它们下面对应的隐藏层状态。

### hmmLearn

安装

```
pip install hmmlearn
```

根据从隐藏层状态到可见层状态输出概率的不通形式，hmmLearn提供了如下三种HMM模型：

1. MultinomialHMM：适合用于可见层状态是离散类型的情况。所谓离散类型可以理解为编程中的枚举类型，数量有限，值与值之间没有数量上的多与少、大与小的关系。
2. GaussianHMM：适合用于可见层状态是连续类型且假设输出概率符合Gaussain分布的情况。所谓连续类型是指状态值较多，且值之间有大小关系。
3. GMMHMM：适合用于隐藏状态是连续类型且假设输出概率符合GMM分布的情况。混合高斯分布可以想象为有多个峰值的分布。

代码：通过学生活动（可见层）预测当时可能的天气情况（隐藏层）

```python
import numpy as np
from hmmlearn import hmm


# 输出概率表
emission_probability = np.array(
    [[0.4, 0.3, 0.3], [0.2, 0.3, 0.5], [0.1, 0.8, 0.1]])  # 晴的隐藏状态时概率：打球0.4读书0.3访友0.3；阴的隐藏状态时概率：打球0.2读书0.3访友0.5；雨的隐藏状态时概率：打球0.1读书0.8访友0.1
# 转换概率表
transition_probability = np.array(
    [[0.7, 0.2, 0.1], [0.3, 0.5, 0.2], [0.3, 0.4, 0.3]])  # 晴->晴0.7->阴0.2->雨0.1；阴->晴0.3->阴0.5->雨0.2；雨->晴0.3->阴0.4->雨0.3
# 隐藏层各状态的初始概率
start_probability = np.array([0.5, 0.3, 0.2])  # 晴0.5阴0.3雨0.2

# 训练数据
model = hmm.MultinomialHMM(n_components=3)  # 定义有3中可见层状态
model.startprob_ = start_probability
model.transmat_ = transition_probability
model.emissionprob_ = emission_probability

observe_chain = np.array(
    [0, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0])  # 0打球1读书2访友
data = observe_chain.reshape(-1, 1)
res = model.predict(data) # 预测值  # 0晴1阴2雨
print(observe_chain, res, sep='\n')
"""
[0 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 0]
[0 0 2 2 2 2 2 2 1 1 1 1 1 1 1 1 0 0]
"""
```

## 离散分布算法

离散可见层输出概率模型建模，这种模型使用的是多项分布，即hmmLearn中的MultinomialNB，相对于连续型可见层状态分布的特点就是输出概率可以通过输出概率矩阵直接表示。

### 前向和后向算法

解决可见层状态链可能性评估的算法可以是暴力求解、前向算法或后向算法，后两者也合称为forward-backward algorithm.

- 暴力破解

所谓可能性问题就是求解一个固定可见状态链出现的概率，这个概率最直接的求解方法步骤如下

1. 计算每一种隐藏状态链的出现概率；
2. 针对每一种隐藏状态链，计算在其出现的情况下给定可见状态链的出现概率；
3. 所有隐藏状态链下计算的给定可见状态链概率求和即得到题解

- 前向算法

经过前面的分析，暴力求解的时间复杂度是$O(T\cdot N^T)$，它是指数级的。所谓前向算法，就是用动态规划的方法改造计算过程，使其最终的时间复杂度能够过达到$O(T\cdot N^2)$，是多项式级别的算法。

前向算法的步骤可以总结为：

1. 通过初试概率读取第一列隐藏层所有可能状态的概率值
2. 根据第一列所有状态概率值计算第二列隐藏层所有的可能状态概率值
3. 用第二列的概率值估计第三列的概率值
4. 以此类推直至最后一列
5. 然后计算这个过程中产生的所有路径的输出概率，并求和。

- 后向算法

后向算法与前向算法类似，也是用动态对话的方法按照层次分别计算每一列隐藏结点可能状态的概率值。两者去比在于前向算法从第一个结点开始向前推导，后向算法从最后一个结点向后推到。后向算法的过程为：

1. 计算隐藏层最后一个结点(设为t时刻)所有可能状态的后向概率值；
2. 根据所有t的状态后向概率值计算t-1时刻隐藏层所有可能状态的后向概率值；
3. 用t-1时刻的概率值计算t-2层的概率值
4. 以此类推直至第一列
5. 然后计算这个过程中产生的所有路径的输出概率，并求和。

其时间复杂度也是$O(T\cdot N^T)$。其中出现了后向概率的概念，其实就是从当前结点开始向世界轴方向观测的概率值。

### Viterbi算法

维特比算法用来在给定HMM模型参数和一个可见状态链时，找出最可能的隐藏状态链。

算法的过程可以分为两步：

1. 正方向推算，用动态规划的方法从第一个隐藏结点开始逐步计算每个可能状态：
   1. 计算本状态出现的概率，设为$\delta$。
   2. 最可能达到本状态的前一个结点的状态，设为$\psi$.

2. 反方向推理，计算到最后一个节点时，概率最大的那个状态就是最后一个隐藏结点的被推测状态。然后逐步向时间轴反防线推到，逐个找出最可能的隐藏结点。

### EM算法

EM是最大期望算法的简称，它是一种迭代改进的算法。

- EM算法过程

EM算法通过不断地迭代来找出模型最佳参数$\theta$，原理如下

1. 定义系统评价方法
2. 随机初始化模型参数$\theta$
3. 不断重复迭代如下两步
   1. E(expectation)步：用当前参数$\theta$ 评价所有输入数据。
   2. M(maximization)步：用评价结果重新计算$\theta$参数，使得新的$\theta$对输入数据获得更好的评价。

4. 当M步对$\theta$ 的改进不再显著时停止上述迭代，认为系统已经找到了最佳参数$\theta$.

注意：参数$\theta$ 可以不只是单一参数，大多数的场景中时指一组变量$\theta_1,\theta_2\cdots$

- EM算法的局限性

EM算法和梯度下降有类似的局限性：改算法并不保证能找到全局最优参数，只能保证找到局部最优解。

但是这个特点并没有影响EM算法的广泛应用，其原因有二：

1. EM算法是对复杂问题在全局最优解与实践复杂度之间的一个权衡，寻找全局最优解的算法在高维特征情况下通常超出了多项式界别的时间复杂度。
2. 在高维环境中，算法陷入局部最优解的概率较小。

因此，EM算法是一种非常有性价比的算法。

### Baunm-Welch算法

HMM前两类问题都假定HMM参数已经给出，但是模型参数本身从何而来呢？这个问题主要有两种思路来解决。

1. 由足够多已有的隐藏层状态链表生成转换概率矩阵；由足够多的"隐藏状态<->可见状态"链表生成输出概率矩阵。
2. 给出足够多的可见状态链表，根据其猜测输出概率和转换概率矩阵。

第一种方式本质上是一种手动整理海量数据的方法，需要对大量的数据进行人工标记，这在很多领域是不可能或成本过高的。

第二种当时就是Baum-Welch方法，Baum正式隐马尔科夫模型的提出者。由于这种方式对隐藏层一无所知，通常不同的初始参数会导致不同的结果，所以在训练之后仍然需要对模型概率数据进行人工调研。但总体来说，这种方法相对第一种方法仍然更加经济高效。

- 总体流程

Baum-Welch采用的就是上述的EM算法，但是在其主线的年代，EM算法还没有正式被提出，所以在HMM领域其只是被成为Baum-Welch算法。如下将Baum-Welch对应到EM过程中。

1. 定义评估方法，最大话可见状态链的前向概率$\alpha$和后向概率$\beta$.
2. 初始化模型参数：随机定义一套初始概率$\pi$、转换概率矩阵$A$、输出概率矩阵$B$.
3. 不断重复迭代。
   1. E步：用当前的$\pi,A,B$ 计算出$\alpha,\beta$.
   2. M步：用$\alpha,\beta$更新$\pi,A,B$.

4. 重复上述步骤，当$\pi,A,B$的更新趋于静止时认为已经找到了模型最佳参数，停止迭代。

### hmmLearn

- MultinomialNB求估计

在`hmmLearn`中所有的HMM模型类都有一个`score()`成员函数，用于为给定模型的某个可见状态计算可能性，其函数为

```python
score(X, lengths=None)

# 参数
X是一个可见层状态的Numpy数组
# 输出
输出一个log概率值。所谓log概率值是对原始概率进行ln计算后获得的值
```

`hmmLearn`不直接返回概率而是使用log概率值的原因是：计算机对服点数的有效位数有限制，而log概率能更好地利用有限的浮点数有效位。

- MultinomialNB求解码问题

在`hmmLearn`中可以直接调用几个HMM模型的`predict()`函数用Viterbi算法进行隐藏状态推测，函数为

````python
predict(X, lengths=None)

# 参数
X是一个可见层状态的Numpy数组，lengths用来说明X中每条可见链的长度。
# 输出
预测可见链的隐藏层。
````

实现

```python
# X由1个可见链构成，每个可见结点有1个特征
X = np.array([0,2,1,1,1,1,1,1,2,2,2,]).reshape(-1, 1)
model.predict(X, [11,])

# X由1个可见链购层呢，每个可见结点有多个特征
X = np.array([0,2,1,1,1,1,1,1,2,2,2,]).reshape(-1, 3)
model.predict(X, [4,])

# X由多个可见链构成
X = np.array([0,2,1,1,1,1,1,1,2,2,2,]).reshape(-1, 1)
model.predict(X, [4,7])

# 查看解码返回
observe_chain = np.array([0,2,1,1,1,1,1,1,2,2,2]).reshape(-1, 1)
model.predict(observe_chain)  # 返回数组的所有11歌元素构成1个隐藏链
model.predict(observe_chain, [5, 6])  # 返回数组的前5个元素是一个隐藏链，后6个元素是另一个隐藏链
```

- 训练数据

```python
fit(X, length=None)

# 参数
X是一个可见层状态的Numpy数组，lengths用来说明X中每条可见链的长度。
可以通过如下三个参数控制：可见链结点的特征维数、可见链的数量、每条可见链的长度
```

## 连续型概率分布

在众多场景中表现层状态需要建模成连续型概率分布，如金融分析、语音识别等领域。这种情况下由于颗鸡蛋状态无法穷举，所以无法以定长定宽的矩阵形式表达输出概率：同时由于连续型状态的值之间有大小关系，使其能够用正态分布表征，所以在hmmLearn中使用高斯分布和混合高斯分布来对其建模。

### 多元高斯分布

由于HMM中每个可见层结点的状态可以由多个特征一起构成，其中每个特征都用高斯分布拟合，所以对可见层结点整体来说其服从的是多元高斯分布。

多元高斯分布是高斯分布在多特征情况下的扩展。

- 普通高斯分布

$$
f(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{(x-\mu)^2}{2\sigma^2}}
$$

其中有如下两个方程参数：

$\mu$：即均值，高斯分布期望值，即概率分布中心点。

$\sigma^2$：即方差，描述分布的聚合程度，方差越小概率分布越集中

- 多元高斯分布

与普通高斯分布相比，不同点：

1. 输入参数由标量$x$变为向量$<x_1,x_2,\cdots>$.
2. 均值由标量$\mu$变为向量$<\mu_1,\mu_2,\cdots>$，分别表示对应输入参数$<x_1,x_2,\cdots>$的均值。
3. 方差由标量$\sigma^2$ 变为$N \times N$的协方差矩阵，其中N是输入向量的维数。

- 协方差矩阵

理解协方差矩阵从理解协方差开始。协方差定义:
$$
cov(X,Y) = E[(X-\mu_X)(Y-\mu_Y)]
$$
协方差的意义是用来衡量两个特征之间的相关性，有如下特点：

1. 协方差为正时两个特征正相关，即$X$变大时$Y$也变大，$X$变小时$Y$也变小
2. 协方差为负时两个特征负相关，即$X$变大时$Y$变小，$X$变小时$Y$变大
3. 协方擦好绝对值越大，两个特征相关性越大

协方差仅能表现两个特征之间的相关性，协方差矩阵表征多个特征的相关性。
$$
\Sigma = \left[ \begin{array}{ccc} E[(X_1-\mu_1)(X_1-\mu_1)] & E[(X_1-\mu_1)(X_2-\mu_2)] & \ldots & E[(X_1-\mu_1)(X_n-\mu_n)]\\ E[(X_2-\mu_2)(X_1-\mu_1)] & E[(X_2-\mu_2)(X_2-\mu_2)] & \ldots & E[(X_2-\mu_2)(X_n-\mu_n)] & \ldots \\ \vdots & \vdots & \ddots \\E[(X_n-\mu_n)(X_1-\mu_1)] & E[(X_n-\mu_n)(X_2-\mu_2)] & \ldots & E[(X_n-\mu_n)(X_n-\mu_n)]\end{array} \right]
$$
协方差矩阵必定是一个方阵，其元素就是横纵坐标对应特征之间的协方差。有了协方差矩阵，就可以结合矩阵向量计算多特征结点的整体概率分布了。

### hmmLearn

- GaussianHMM

GaussianHMM是两种高斯分布中较简单的形式，它假定：

1. 结点中的每个维度都符合高斯分布，因此输出概率是一个多元高斯分布。
2. 隐藏层状态是对每个可见层特征分别识别出的均值与方差。
3. 状态维度之间可以有关联关系，通过输出概率协方差矩阵来体现。

```python
from hmmlearn.hmm import GaussianHMM

model = GaussianHMM(...)

# 构造参数
n_components		# 隐藏层结点的状态数量。
covariance_type # 输出概率的协方差矩阵类型，可选值：spherical球面协方差矩阵，即矩阵对角线上的元素都相等，且其他元素为零；diag对角协方差矩阵，即对角线元素可以是任意值，其他元素为零；full完全矩阵，即任意元素可以是任意值，但需要更多的训练数据。
min_covar				# 协方擦好矩阵中对角线上的最小数值，该值设置得越小模型对数据的拟合就越好，但更容易出现过度拟合
startprob_prior		# 第一个隐藏结点的处事概率矩阵
transmat_prior		# 转换概率矩阵
means_prior,means_weight # 先验隐藏层均值矩阵
covars_prior,covars_weight # 先验隐藏层协方差矩阵
algorithm		# 解码算法，可选viterbi,map更快速但非全局最优解
random_state	# 随机种子，用于在Baum-Welch算法中初始化模型参数
n_iter	# Baum-Welch算法最大迭代次数。该值越大，训练模型对数据的拟合度越高，但训练耗时越长。
tol		# Baum-Welch算法停止迭代的容忍阈值。该值越小，训练模型对书局的拟合度越到，但训练耗时越长
verbose	# 是否打印Baum-Welch每次迭代的调试信息
params	# 在训练过程中更新的HMM参数，可以是四个字母中的任意几个组合的字符串，s指初始概率，t是转换概率，m是高斯均值矩阵，c是高斯协方差矩阵
init_params		# 在寻来呢开始之前使用已有概率矩阵初始化模型，可以是四个字母中的任意几个组合的字符串


# 训练
model.fit(X)
# 均值矩阵
model.means_
# 协方差矩阵
model.covars_
# 转换概率矩阵
model.transmat_

# 预测
hiiden_states = model.predict(X)
```

- GMMHMM

GMMHMM适用于隐藏层状态是连续类型且假设输出概率符合GMM分布的情况。它与GaussionHMM不同点仅在于它假设输出概率使用GMM而非简单的Gaussian分布。

```python
from hmmearn.hmm import GMMHMM

model = GMMHMM(...)

# 构造参数（大部分与GausinaHMM一样）
n_mix		# 混合高斯概率分布中高斯分布的数量，如果n_min=1,则退化为GaussinaHMM
means_prior,means_weight,covars_prior,covars_weight		# 维数随着n_mix的设置而不同
```

由于理论上任何密度函数都以由混合高斯模型拟合，所以GMMHMM有非常大的适用场景。

