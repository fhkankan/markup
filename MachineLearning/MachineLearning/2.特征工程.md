# 特征工程

数据预处理涉及的策略和技术非常广泛，主要技术是属性选择技术和主成分分析技术。

属性选择

```
指从数据集中选择最具代表性的属性子集，删除冗余或不相关的属性，从而提高数据处理的效率，使模型更容易理解
```

主成分分析

```
利用降维的思想，把给定的一组相关属性通过线性变换转换成另一组不相关的属性，这些新属性按照方差依次递减的顺序进行排列
主成分分析将很多个复杂属性归结为少数几个主成分，将复杂问题简单化，便于分析和处理
```

离散化

```
将连续的数值型数据切分为若干个称为分箱(bin)的小段，是数据分析中常用的手段
离散化的实质是将无限空间中有限的个体映射到有限的空间中去，其作用是提高算法的时空效率，有时也为了能够使用特定的机器学习算法而将数据离散化
```

## 特征预处理

### 缺失值

- 丢弃
```
- 比例较低，缺失值没有明显的数据分布规律或特征时直接删除特征，
- 删除整行样本
```
- 补全：

```
- 统计法：使用均值、加权均值、中位数等补全
- 模型法：进行预测，从而得到最为可能的补全值
- 专家法：对于少量且具有重要意义的数据记录专家人工补全
- 其他法：随机法、特殊值法、多重填补等
```

- 真值转换

```
承认缺失值的存在，并把数据缺失也作为数据分布规律的一部分
```

- 不处理

```
有些模型可以自动处理缺失值：
KNN，决策树、随机森林、神经网络、朴素贝叶斯、DBSCAN
```

### 异常值

不要轻易抛弃异常数据

- 伪异常值

有些异常时由于业务特定运营动作产生，其实是正常反映业务状态，如果抛弃异常值将导致无法正确反馈业务结果

- 真异常值

异常数据本身是目标数据，如果被处理掉将损失关键信息：如客户异常识别、信用卡欺诈等

- 异常检测

- 不处理

数据算法和模型对异常值不敏感时可不处理

### 重复值

- 不去重

**重复的记录用于不均衡处理**：对于少数样本类别做简单过采样，通过随机过采样采取简单复制样本的策略来增加少数类样本。

**重复的记录用于检测业务规则问题**：对于事务型的数据而言，重复数据可能意味着重大运营规则问题。

### 二值化

二值化用于将数值特征向量转换为布尔类型向量

### 均值移除

把每个特征的平均值移除，以保证特征均值为0（标准化处理）。这样可以消除特征彼此之间的偏差。

### 归一化

如果输入数据属性具有非常大的比例差异，往往导致算法的性能表现不佳，可以采用归一化来同比例缩放所有属性，可以加快梯度下降的收敛过程。

- 最值归一化(Normalization)

将说有数据归一化到0～1的分布中
$$
x_{scale} =\frac{x-x_{min}}{x_{max}-x_{min}}
$$
适用有明显边界的情况， 不适合有极端值的情况



- 均值方差归一化(Standardization)

将所有数据归一化到均值为0，方差为1的分布中
$$
x_{scale} = \frac{x-x_{mean}}{S}
$$
适用没有明显边界，有可能存在极端值

- 范数

将特征向量应用max、L1和L2范数
$$
||X||_{max} = \frac{X}{\max_{i}{X}} \\
||X||_{L1} = \frac{X}{\sum_{i}|x_i|} \\
||X||_{L2} = \frac{X}{\sqrt{\sum_{i}|x_i|^2}}
$$

## 样本不均衡

样本类别不均衡场景

```
1.异常检测场景：黄牛订单、信用卡欺诈、设备故障
2.罕见事件的分析
3.客户流失场景
4.虽然事件是预期或计划性十斤啊，但是发生频率低
```

对于样本不均衡问题，可采取以下方法处理

```
1. 增加或减少样本
2. 调整权重惩罚
3. 使用其他舍弃的特征产生更好的特征
4. 尝试多种模型算法,如随机森林、逻辑回归之外的模型
5. 使用集成学习，采用多个模型来提高准确预测
6. 调整模型的超参数
```

### 数据采样

对于样本不均衡状况，可以采用下采样或过采样进行处理

- 下采样

按照样本较少的类别，将样本较多的类别中选取同样的样本数

缺点：由于样本过少，易发生过拟合

- 过采样

对于样本较少的类别，采用一定的算法，自动生成与样本较多类别个数的样本数据数据

SMOTE算法

```
1. 对于少数类中的每一个样本x，以欧式距离为标准计算它到少数类样本集中所有样本的距离，得到其k近邻
2. 根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本x，从其k近邻中随机选择若干个样本，假设选择的近邻为x(n)
3. 对于每一个随机选出的近邻x(n)，分别与原样本按照如下公式构建新的样本
```

$$
x_{new} = x + rand(0, 1)*(\bar{x}-x)
$$

使用

```python
import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

credit_cards = pd.read_csv('creditcard.csv')
columns = credit_cards.columns
features_columns = columns.delete(len(columns) - 1)
features = credit_cards[features_columns]
labels = credit_cards['Class']
features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2, random_state=0)

oversample = SMOTE(random_state=0)
os_features, os_labels = oversample.fit_sample(features_train, lables_train)

print(len(os_labels[os_labels==1]))
os_features = pd.DataFrame(os_features)
os_labels = pd.DataFrame(os_labels)
```

### 权重调整

通过对较多数据样本的权重降低，较少的样本的权重提高，来平衡样本不均衡

```python
from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import cross_val_predict, KFold

# 模型训练
lr = LogisticRegression()
cols = loans.columns
train_cols = cols.drop("loan_status")
features = loans[train_cols]
target = loans["loan_status"]
lr.fit(features, target)
predictions = lr.predict(features)

# 未调整权重
# lr = LogisticRegression()
# 调整权重
# 采用默认调整权重
lr = LogisticRegression(clas_weight="balanced")
# 自定义调整权重
lr = LogisticRegression(clas_weight={0:10, 1:1})
kf = KFold(features.shape[0], random_state=1)
predictions = cross_val_predict(lr, features, target, cv=kf)
predictions = pd.Series(predictions)

# 评价指标
fp_filter = (predictions == 1) & (loans["loan_status"] == 0)
fp = len(predictions[fp_filter])
tp_filter = (predictions == 1) & (loans["loan_status"] == 1)
tp = len(predictions[tp_filter])
fn_filter = (predictions == 0) & (loans["loan_status"] == 1)
fn = len(predictions[fn_filter])
tn_filter = (predictions == 0) & (loans["loan_status"] == 0)
tn = len(predictions[tn_filter])

fpr = fp / float((fp + tn))
tpr = tp / float((tp + fn))
print(fpr, tpr)
```

## 数据源冲突

原因
```
- 类型冲突：日期、时间戳类型不同
- 结构冲突：数据主体的描述结构有冲突
- 记录粒度不同：如订单的id、商品的id
- 值域与制不同：如订单状态不同
```
解决
```
- 全局性的汇总统计：消除冲突并形成一份唯一数据
- 整体的流程行统计分析：不消除冲突但是使用全部冲突数据
- 数据用于数据建模：不消除也不作任何处理
```
## 特征抽取

### 分类特征

- one-hot

通常，需要处理的数据都是稀疏地、散乱地分布在空间中，然而，我们并不需要存储这些大数值，这时就需要使用one-hot编码。可以将其看作是一种收紧特征向量的工具，它把特征向量的每个特征与特征的非重复总数相对应，通过one-of-k的形式对每个值进行编码。特征向量的每个特征值都按照这种方式编码，这样可以更加有效地表示空间。

```
5种方式：
1. np.eye
2. pd.get_dummies
3. OneHotEncoder
4. LabelBinarizer
5. tf.keras.utils.to_categorical
```

实现

```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelBinarizer, OneHotEncoder
import tensorflow as tf

np_data = np.array([0, 1,2,3,4,5])

# np.eye
res = np.eye(6)[np_data.reshape(-1)]
# pd.get_dummies
res = pd.get_dummies(np_data).values
# OneHotEncoder
encoder = OneHotEncoder()
res = encoder.fit_transform(np_data.reshape(-1, 1)).toarray()
# LabelEncoder
encoder = LabelBinarizer()
res = encoder.fit_transform(np_data)
# tf.keras.utils.to_categorical
res = tf.keras.utils.to_categorical(np_data)
print(res)
```

### 文本特征

- 单词统计

```python
from sklearn.feature_extraction.text import CountVectorizer

# 实例化
vec = CountVectorizer()

vec.fit_transform(X)       
# 参数X:文本或者包含文本字符串的可迭代对象
# 返回：词频矩阵
# fit_transform = fit + transform

vev.get_feature_names()
# 返回值:单词列表
```

实现

```python
from sklearn.feature_extraction.text import CountVectorizer

data = ["life is short, i like python", "lisfe is too long, i disliake python"]

# 特征抽取，抽取词频矩阵
vec = CountVectorizer()
# fit提取特征名
name = vec.fit(data)
# transform根据提取出来的特征词，统计个数
result = vec.transform(data)
# data是文本或包含文本字符串的可迭代对象，返回词频矩阵
# result = vec.fit_transform(data)  # fit_transform = fit + transform
print(vec.get_feature_names())  # 返回单词列表
# ['disliake', 'is', 'life', 'like', 'lisfe', 'long', 'python', 'short', 'too']
print(result)  # 稀疏矩阵
#  (0, 1)	1
#   (0, 2)	1
#   (0, 3)	1
#   (0, 6)	1
#   (0, 7)	1
#   (1, 0)	1
#   (1, 1)	1
#   (1, 4)	1
#   (1, 5)	1
#   (1, 6)	1
#   (1, 8)	1

print(result.toarray())  # sparse矩阵转换为array数组
# [[0 1 1 1 0 0 1 1 0]
#  [1 1 0 0 1 1 1 0 1]]

```

中文文本

```python
import jieba
from sklearn.feature_extraction.text import CountVectorizer

data = "生活很短，我喜欢python, 生活太久了，我不喜欢python"

# 分词,返回值是generator
cut_ge = jieba.cut(data)
# 方法一：生成器转列表
# content = []
# for word in cut_ge:
#     content.append(word)
# data = [" ".join(content)]
# 方法二，join(可迭代)
data = " ".join(cut_ge)
cv = CountVectorizer()
result = cv.fit_transform(data)
print(cv.get_feature_names())
print(result)
print(result.toarray())
```

- TF-IDF

原始的单词统计会让一些常用词聚集过高的权重，不利于分类算法。

TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

```python
from sklearn.feature_extraction.text import TfidfVectorizer
# 返回词的权重矩阵

vec = TfidfVectorizer(stop_words=None,…)

vec.fit_transform(X,y)       
# 参数X:文本或者包含文本字符串的可迭代对象
# 返回值：返回sparse矩阵

vec.inverse_transform(X)
# 参数X:array数组或者sparse矩阵
# 返回值:转换之前数据格式

vec.get_feature_names()
# 返回值:单词列表
```

实现

```python
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

data = ["life is short, i like python", "lisfe is too long, i disliake python"]

vec = TfidfVectorizer()
X = vec.fit_transform(data)
print(X)
#   (0, 6)	0.35520008546852583
#   (0, 3)	0.4992213265230509
#   (0, 7)	0.4992213265230509
#   (0, 1)	0.35520008546852583
#   (0, 2)	0.4992213265230509
#   (1, 0)	0.4466561618018052
#   (1, 5)	0.4466561618018052
#   (1, 8)	0.4466561618018052
#   (1, 4)	0.4466561618018052
#   (1, 6)	0.31779953783628945
#   (1, 1)	0.31779953783628945
print(X.toarray())
# [[0.         0.35520009 0.49922133 0.49922133 0.         0.         0.35520009 0.49922133 0.]
#  [0.44665616 0.31779954 0.         0.         0.44665616 0.44665616 0.31779954 0.         0.44665616]]
res = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())
print(res)
#    disliake      is      life      like  ...      long  python     short       too
# 0  0.000000  0.3552  0.499221  0.499221  ...  0.000000  0.3552  0.499221  0.000000
# 1  0.446656  0.3178  0.000000  0.000000  ...  0.446656  0.3178  0.000000  0.446656

```

中文

```python
import jieba 
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

def cut_words():
    s1 = "今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。"
    s2 = "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。"
    s3 = "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"   
    s1_ge = jieba.cut(s1)
    s2_ge = jieba.cut(s2)
    s3_ge = jieba.cut(s3)
    return " ".join(s1_ge), " ".join(s2_ge), " ".join(s3_ge)

words1, words2, words3 = cut_words()
# 使用TFIDF特征抽取
tfidf = TfidfVectorizer(stop_words=["一种", "每个"])
# 输入：文本或包含文本字符创的可迭代对象，返回值：saprse矩阵
result = tfidf.fit_transform([words1, words2, words3])
# 返回值：单词列表
print(tfidf.get_feature_names())
print(result.toarray())
# 输入：array数组或sparse矩阵，返回值：转换之前的数据格式
print(tfidf.inverse_transform(result))
```

### 图像特征

方法一：使用像素

## 特征选择

主要方法：

```
Filter:VarianceThreshold

Embedded:正则化、决策树

Wrapper
```

函数

```python
# 类
sklearn.feature_selection.VarianceThreshold
# 实例化
VarianceThreshold(threshold = 0.0)
删除所有低方差特征

Variance.fit_transform(X,y)       
X:numpy array格式的数据[n_samples,n_features]
返回值：训练集差异低于threshold的特征将被删除。
默认值是保留所有非零方差特征，即删除所有样本
中具有相同值的特征。
```

## 特征重要性

将某个特征对应的样本值加入干扰值之后获取error，与原本特征的样本获取的error，进行对比，若相差较大，则说明此特征比较重要

```python
import numpy as np
from sklearn.feature_selection import SelectKBest, f_classif
import matplotlib.pyplot as plt

predictors = ["Pclass", "Sex", "Age", "Fare"]

# 特征选择
selector = SelectKBest(f_classif, k=5)
selector.fit(titanic[predictors], titanic["Survived"])
# 获取每个特征对应的p-value，然后将其转换为score
scores = -np.log10(selector.pvalues_)

# 画图
plt.bar(range(len(predictors)), scores)
plt.xticks(range(len(predictors)), predictos, totation="vertical")
plt.show()
```

## 特征降维

分类：

```
主成成分分析(principalcomponent analysis,PCA)

因子分析(Factor Analysis)

独立成分分析(Independent Component Analysis，ICA)
```

本质：PCA是一种分析、简化数据集的技术。在PCA中，数据从原来的坐标系转换到新的坐标系。

目的：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。

**特征选择/降维**

相同点：

特征选择和降维都是降低数据维度

不同点：

特征选择筛选掉的特征不会对模型的训练产生任何影响

降维做了数据的映射，保留主要成分，所有的特征对模型训练有影响

## 使用流程

流程

```
1.导入需要的库
2.导入数据集
3.处理丢失数据
4.解析分类数据
5.拆分数据集为训练集合和测试集合
6.特征量化
```

data

| Country | Age  | Salary | Purchased |
| ------- | ---- | ------ | --------- |
| France  | 44   | 72000  | No        |
| Spain   | 27   | 48000  | Yes       |
| Germany | 30   | 54000  | No        |
| Spain   | 38   | 61000  | No        |
| Germany | 40   |        | Yes       |
| France  | 35   | 58000  | Yes       |
| Spain   |      | 52000  | No        |
| France  | 48   | 79000  | Yes       |
| Germany | 50   | 83000  | No        |
| France  | 37   | 67000  | Yes       |

code

```python
#Step 1: Importing the libraries
import numpy as np
import pandas as pd

#Step 2: Importing dataset
dataset = pd.read_csv('../datasets/Data.csv')
X = dataset.iloc[ : , :-1].values
Y = dataset.iloc[ : , 3].values
print("Step 2: Importing dataset")
print("X")
print(X)
print("Y")
print(Y)

#Step 3: Handling the missing data
from sklearn.preprocessing import Imputer
imputer = Imputer(missing_values = "NaN", strategy = "mean", axis = 0)
imputer = imputer.fit(X[ : , 1:3])
X[ : , 1:3] = imputer.transform(X[ : , 1:3])
print("---------------------")
print("Step 3: Handling the missing data")
print("step2")
print("X")
print(X)

#Step 4: Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X = LabelEncoder()
X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])
#Creating a dummy variable
onehotencoder = OneHotEncoder(categorical_features = [0])
X = onehotencoder.fit_transform(X).toarray()
labelencoder_Y = LabelEncoder()
Y =  labelencoder_Y.fit_transform(Y)
print("---------------------")
print("Step 4: Encoding categorical data")
print("X")
print(X)
print("Y")
print(Y)

#Step 5: Splitting the datasets into training sets and Test sets
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)
print("---------------------")
print("Step 5: Splitting the datasets into training sets and Test sets")
print("X_train")
print(X_train)
print("X_test")
print(X_test)
print("Y_train")
print(Y_train)
print("Y_test")
print(Y_test)

#Step 6: Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
print("---------------------")
print("Step 6: Feature Scaling")
print("X_train")
print(X_train)
print("X_test")
print(X_test)
```

