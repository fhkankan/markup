# 手写数字识别

## 数据集介绍

[数据集地址](http://yann.lecun.com/exdb/mnist/)

Minist数据获取API

```python
# TensorFlow框架自带了获取这个数据集的接口，所以不需要自行读取
# 旧版本tensorflow
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)
mnist.train.next_batch(100)	# 批量获取数据功能
mnist.train.images  # 训练数据图片
mnist.train.labels	# 训练数据图片对应的目标值(经过one-hot处理)
mnist.test.images	# 测试数据图片
mnist.test.labels	# 测试数据图片对应的目标值(经过one-hot处理)
# 新版本
import tensorflow as tf
data = tf.keras.datasets.mnist
(train_x, train_y), (test_x, test_y) = data.load_data()
train_label = tf.keras.utils.to_categorical(train_label, num_classes=10)  # one-hot处理
test_label = tf.keras.utils.to_categorical(test_label, num_classes=10)  # one-hot处理
```

数据集内容

```
数据集分为两部分：55000行训练数据集(mnist.tain);10000行测试数据集(mnist.test)
每个MINIST数据单元有两部分组成：一张包含手写数字的图片和一个对应的标签
图片为黑白图片，每张图片包含28像素*28像素
每个图片都有相应的标签，0～9之间的数字表示图像中绘制的内容。使用了one-hot编码
```

## 单层全连接

```python
import os

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data



# 1.利用数据，在训练的时候实时提供数据
# minist手写数字数据在运行时候实时提供给占位符

tf.app.flags.DEFINE_integer("is_train", 1, "指定是否是训练模型，还是拿数据去预测")
FLAGS = tf.app.flags.FLAGS


def full_connection():
    """
    用单层全连接来对手写数字进行识别
    特征值：[None, 784]
    目标值： one-hot [None, 10]
    """
    # 1.准备数据
    minist = input_data.read_data_sets("../minist_data", one_hot=True)
    with tf.variable_scope("minist_data"):
        x = tf.placeholder(dtype=tf.float32, shape=[None, 784])
        y_t = tf.placeholder(dtype=tf.float32, shape=[None, 10])
    # 2.构建模型
    # 类别：10个类别，全连接层：10个神经元
    # 全连接层神经网络计算公式：[None, 784] * [784,10] = [None,10]
    # 随机初始化权重偏置参数，这些是优化的参数，必须使用变量op去定义
    with tf.variable_scope("fc_model"):
        weights = tf.Variable(initial_value=tf.random_normal(shape=[784, 10]), name="w")
        bias = tf.Variable(initial_value=tf.random_normal(shape=[10]), name="b")
        # fc层的计算，[None, 10]输出结果，提供给softmax使用
        y_p = tf.matmul(x, weights) + bias
    # 3.构造损失函数
    # softmax回归及交叉熵损失计算
    with tf.variable_scope("softmax_cross_entropy"):
        # labels：真实值，[None, 10]
        # logits: 全连接层的输出 [None, 10]
        # 返回每个样本的损失组成的列表
        error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_t, logits=y_p))
    # 4.优化损失
    # 梯度下降优化损失
    with tf.variable_scope("optimizer"):
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(error)
    # 5.准确率计算
    with tf.variable_scope("accuracy"):
        # 1)比较输出的结果最大值所在位置和真实值的最大值所在位置
        equal_list = tf.equal(tf.argmax(y_t, 1), tf.argmax(y_p, 1))
        # 2)求平均
        accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))

    # （2）收集要显示的变量
    tf.summary.scalar("error", error)  # 损失
    tf.summary.scalar("accuracy", accuracy)  # 准确率
    tf.summary.histogram("weights", weights)  # 权重
    tf.summary.histogram("bias", bias)  # 偏置
    
    # （3）合并所有变量op
    merged = tf.summary.merge_all()

    # 创建模型保存和加载
    saver = tf.train.Saver()

    # 开启会话
    with tf.Session() as sess:
        # 初始化变量
    	init = tf.global_variables_initializer()
        sess.run(init)
        # tensorboard可视化
        # （1）创建一个events文件实例
        file_writer = tf.summary.FileWriter("./tmp/summary/", graph=sess.graph)
        # 加载模型
        if os.path.exists("./tmp/modelckpt/checkpoint"):
            saver.restore(sess, "./tmp/modelckpt/fc_nn_model")

        if FLAGS.is_train == 1:
            # 循环步数去训练
            for i in range(1000):
                # 获取数据，实时提供
                # 每步提供50个样本训练
                image, label = minist.train.next_batch(50)
                print("训练之前，损失为:{}".format(sess.run(error, feed_dict={x: image, y_t: label})))
                #  开始训练
                _, loss, accuracy_v = sess.run([optimizer, error, accuracy], feed_dict={x: image, y_t: label})
                print("第{}次训练，损失为:{}, 准确率:{}".format(i + 1, loss, accuracy_v))

                # （4）运行合并，写入事件文件当中
                summary = sess.run(merged, feed_dict={x: image, y_t: label})
                file_writer.add_summary(summary, i)
                # 保存模型文件
                if i % 100 == 0:
                    saver.save(sess, "./tmp/modeelckpt/fc_nn_model")

        else:
            # 若不是训练，则是预测测试集数据
            for i in range(100):
                # 每次拿一个样本
                image, label = minist.test.next_batch(1)
                print("第{}个样本的真实值是：{}, 模型的预测结果是：{}".format(
                    i + 1,
                    tf.argmax(sess.run(y_t, feed_dict={x: image, y_t: label}), 1).eval(),
                    tf.argmax(sess.run(y_p, feed_dict={x: image, y_t: label}), 1).eval()
                ))


def main(argv):
    full_connection()


if __name__ == "__main__":
    tf.app.run()

```

## 两层全连接

```python
import os

import numpy as np
import tensorflow as tf

"""
使用2层全连接神经网络对Mnist数据集进行多分类
1.网络设计
神经网络除了输入层之外采取两个层：1.隐层中64个神经元，2.输出层（全连接层）设置10个神经元(用于区分0～9)
2.确定网络结构及形状
1层：输入x[None, 784],权重[784, 64],偏置[64],输出[None, 64]
2层：输入[None, 64],权重[64, 10],偏置[10],输出[None, 10]
3.流程
获取数据、前向传播（网络结构定义）、损失计算、反向传播（梯度下降优化）
"""
# 定义一个是否训练、预测的标志
tf.app.flags.DEFINE_integer("is_train", 2, "训练or预测")

FLAGS = tf.app.flags.FLAGS


def full_connected_nn():
    # 1.获取数据
    # tensorflow新的加载Mnist数据集方式
    mnist = tf.keras.datasets.mnist
    (train_image, train_label), (test_image, test_label) = mnist.load_data()
    print(train_image.shape, train_label.shape, test_image.shape, test_label.shape)
    # (60000, 28, 28) (60000,) (10000, 28, 28) (10000,)
    # 对数据集的特征值做处理，对目标值做one-hot处理
    train_image = train_image.reshape(train_image.shape[0], 784).astype(np.float32) / 255
    test_image = test_image.reshape(test_image.shape[0], 784).astype(np.float32) / 255
    train_label = tf.keras.utils.to_categorical(train_label, num_classes=10)
    test_label = tf.keras.utils.to_categorical(test_label, num_classes=10)
    print(train_label.shape)  # (60000, 10)

    # 定义占位符，Mnist数据实时提供给placeholder
    # 输入[None, 784],输出[None, 10]
    with tf.variable_scope("mnist_data"):
        x = tf.placeholder(tf.float32, [None, 784], name="feature")
        y_true = tf.placeholder(tf.int32, [None, 10], name="label")

    # 2.前向传播
    # 1层：输入x[None, 784],权重[784, 64],偏置[64],输出[None, 64]
    # 2层：输入[None, 64],权重[64, 10],偏置[10],输出[None, 10]
    with tf.variable_scope("fc_model"):
        # 随机化初始权重和偏置参数，使用OP定义
        # 1层
        weight_1 = tf.Variable(tf.random_normal([784, 64], mean=0.0, stddev=1.0), name="weightes_1")
        biases_1 = tf.Variable(tf.random_normal([64], mean=0.0, stddev=1.0), name='biases_1')
        # 2层
        weight_2 = tf.Variable(tf.random_normal([64, 10], mean=0.0, stddev=1.0), name="weightes_2")
        biases_2 = tf.Variable(tf.random_normal([10], mean=0.0, stddev=1.0), name='biases_2')
        # 矩阵运算，全连接层输出结果
        y_1 = tf.matmul(x, weight_1) + biases_1
        y_predict = tf.matmul(y_1, weight_2) + biases_2

    # 3.softmax回归及交叉熵损失计算
    with tf.variable_scope("softmax_cross"):
        # 先进行网络输出的值的概率计算softmax,再进行交叉熵损失计算
        all_loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict)
        # 平均损失
        loss = tf.reduce_mean(all_loss)

    # 4.梯度下降优化损失
    with tf.variable_scope("optimizer"):
        train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

    # 5.得出每次训练的准确率
    with tf.variable_scope("accuracy"):
        # 求出每个样本是否相等的一个列表
        equal_list = tf.equal(tf.argmax(y_true, 1), tf.argmax(y_predict, 1))
        # 计算相等的样本的比例
        accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))

    # 1.收集要在tensorboard观察的张量值
    tf.summary.scalar("losses", loss)
    tf.summary.scalar("acc", accuracy)
    tf.summary.histogram("w1", weight_1)
    tf.summary.histogram("b1", biases_1)
    tf.summary.histogram("w2", weight_2)
    tf.summary.histogram("b2", biases_2)

    # 2.合并所有变量op
    merged = tf.summary.merge_all()

    # 创建模型保存和加载
    saver = tf.train.Saver()

    # 开启会话进行训练
    with tf.Session() as sess:
        # 初始化变量op
        init_op = tf.global_variables_initializer()
        sess.run(init_op)
        # tensorboard可视化
        # 创建一个events文件实例
        file_writer = tf.summary.FileWriter('./tmp/summary/', graph=sess.graph)
        # 加载本地模型继续训练或者拿来进行预测测试集
        if os.path.exists("./tmp/modelckpt/checkpoint"):
            saver.restore(sess, "./tmp/modelckpt/fc_nn_model")
        if FLAGS.is_train == 1:
            # 循环训练
            for i in range(1200):
                # 每批给50个样本
                mnist_x, mnist_y = train_image[50 * i:50 * (i + 1)], train_label[50 * i:50 * (i + 1)]
                _, loss_run, acc_run, summary = sess.run([train_op, loss, accuracy, merged],
                                                         feed_dict={x: mnist_x, y_true: mnist_y})
                print("第 %d 步的50个样本损失为：%f , 准确率为：%f" % (i, loss_run, acc_run))

                # 3.运行合变量op，写入事件文件当中
                file_writer.add_summary(summary, i)

                # 每隔100步保存一次模型
                if i % 100 == 0:
                    saver.save(sess, "./tmp/modelckpt/fc_nn_model")
        else:
            # 进行预测
            for i in range(100):
                image, label = test_image[1 * i:1 * (i + 1)], test_label[1 * i:1 * (i + 1)]
                # 直接运行网络的输出预测结果
                print("第 %d 样本，真实的图片数字为：%d, 神经网络预测的数字为：%d " % (
                    i,
                    tf.argmax(label, 1).eval(),
                    tf.argmax(sess.run(y_predict, feed_dict={x: image, y_true: label}), 1).eval()
                ))


if __name__ == '__main__':
    full_connected_nn()

```

## 卷积神经网络

```python
import os

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

# 1.利用数据，在训练的时候实时提供数据
# minist手写数字数据在运行时候实时提供给占位符

tf.app.flags.DEFINE_integer("is_train", 1, "指定是否是训练模型，还是拿数据去预测")
FLAGS = tf.app.flags.FLAGS


def create_weights(shape):
    return tf.Variable(initial_value=tf.random_normal(shape=shape))


def create_model(x):
    """
    构建卷积神经网络
    :return:
    """
    y_p = 0
    # 1.第一个卷积大层
    with tf.variable_scope("conv1"):
        # 卷积层
        # 将x[None, 784]形状进行修改
        input_x = tf.reshape(x, shape=[-1, 28, 28, 1])
        # 定义filter和偏置
        conv1_weights = create_weights(shape=[5, 5, 1, 32])
        conv1_bias = create_weights(shape=[32])
        conv1_x = tf.nn.conv2d(input=input_x, filter=conv1_weights, strides=[1, 1, 1, 1], padding="SAME") + conv1_bias
        # 激活层
        relu1_x = tf.nn.relu(conv1_x)
        # 池化层
        pool1_x = tf.nn.max_pool(value=relu1_x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME")
    # 2.第二个卷积大层
    with tf.variable_scope("conv2"):
        # 卷积层
        # 定义filter和偏置
        conv2_weights = create_weights(shape=[5, 5, 32, 64])
        conv2_bias = create_weights(shape=[64])
        conv2_x = tf.nn.conv2d(pool1_x, filter=conv2_weights, strides=[1, 1, 1, 1], padding="SAME") + conv2_bias
        # 激活层
        relu2_x = tf.nn.relu(conv2_x)
        # 池化层
        pool2_x = tf.nn.max_pool(value=relu2_x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME")
    # 3.全连接
    with tf.variable_scope("full_connection"):
        x_fc = tf.reshape(pool2_x, shape=[-1, 7 * 7 * 64])
        weights_fc = create_weights(shape=[7 * 7 * 64, 10])
        bias_fc = create_weights(shape=[10])
        y_p = tf.matmul(x_fc, weights_fc) + bias_fc
    return y_p


def full_connection():
    """
    用单层全连接来对手写数字进行识别
    特征值：[None, 784]
    目标值： one-hot [None, 10]
    """
    # 1.准备数据
    minist = input_data.read_data_sets("../minist_data", one_hot=True)
    with tf.variable_scope("minist_data"):
        x = tf.placeholder(dtype=tf.float32, shape=[None, 784])
        y_t = tf.placeholder(dtype=tf.float32, shape=[None, 10])
    # 2.构建模型
    # 卷积神经网络
    y_p = create_model(x)
    # 3.构造损失函数
    # softmax回归及交叉熵损失计算
    with tf.variable_scope("softmax_cross_entropy"):
        # labels：真实值，[None, 10]
        # logits: 全连接层的输出 [None, 10]
        # 返回每个样本的损失组成的列表
        error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_t, logits=y_p))
    # 4.优化损失
    # 梯度下降优化损失
    with tf.variable_scope("optimizer"):
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(error)
    # 5.准确率计算
    with tf.variable_scope("accuracy"):
        # 1)比较输出的结果最大值所在位置和真实值的最大值所在位置
        equal_list = tf.equal(tf.argmax(y_t, 1), tf.argmax(y_p, 1))
        # 2)求平均
        accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))

    # （2）收集要显示的变量
    tf.summary.scalar("error", error)  # 损失
    tf.summary.scalar("accuracy", accuracy)  # 准确率
    # 初始化变量
    init = tf.global_variables_initializer()
    # （3）合并所有变量op
    merged = tf.summary.merge_all()

    # 创建模型保存和加载
    saver = tf.train.Saver()

    # 开启会话
    with tf.Session() as sess:
        # 初始化变量
        sess.run(init)
        # tensorboard可视化
        # （1）创建一个events文件实例
        file_writer = tf.summary.FileWriter("./tmp/summary/", graph=sess.graph)
        # 加载模型
        if os.path.exists("./tmp/modelckpt/checkpoint"):
            saver.restore(sess, "./tmp/modelckpt/c_nn_model")

        if FLAGS.is_train == 1:
            # 循环步数去训练
            for i in range(1000):
                # 获取数据，实时提供
                # 每步提供50个样本训练
                image, label = minist.train.next_batch(50)
                print("训练之前，损失为:{}".format(sess.run(error, feed_dict={x: image, y_t: label})))
                #  开始训练
                _, loss, accuracy_v = sess.run([optimizer, error, accuracy], feed_dict={x: image, y_t: label})
                print("第{}次训练，损失为:{}, 准确率:{}".format(i + 1, loss, accuracy_v))

                # （4）运行合并，写入事件文件当中
                summary = sess.run(merged, feed_dict={x: image, y_t: label})
                file_writer.add_summary(summary, i)
                # 保存模型文件
                if i % 100 == 0:
                    saver.save(sess, "./tmp/modeelckpt/c_nn_model")

        else:
            # 若不是训练，则是预测测试集数据
            for i in range(100):
                # 每次拿一个样本
                image, label = minist.test.next_batch(1)
                y_t_t = tf.argmax(sess.run(y_t, feed_dict={x: image, y_t: label}), 1)
                y_t_p = tf.argmax(sess.run(y_p, feed_dict={x: image, y_t: label}), 1)
                y_t_a = tf.reduce_mean(tf.cast(tf.equal(y_t_t, y_t_p), tf.float32))
                print("第{}个样本的真实值是：{}, 模型的预测结果是：{}, 准确率为：{}".format(
                    i + 1,
                    y_t_t.eval(),
                    y_t_p.eval(),
                    y_t_a.eval()
                ))


def main(argv):
    full_connection()


if __name__ == "__main__":
    tf.app.run()

```

