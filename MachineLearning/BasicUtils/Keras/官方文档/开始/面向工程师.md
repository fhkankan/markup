# 面向工程师

```python
import numpy as np
import tensorflow as tf
from tensorflow import keras
```

## 概述

您是否是一位机器学习工程师，希望使用Keras在实际产品中提供深度学习功能？本指南将作为您首次介绍Keras API核心概念。

在本指南中，您将学习如何：

- 在训练模型之前准备数据（将其转换为NumPy数组或tf.data.Dataset对象）。
- 进行数据预处理，例如功能归一化或词汇索引。
- 使用Keras Functional API建立一个将您的数据转化为有用的预测的模型。
- 使用内置的Keras `fit()`方法训练模型，同时注意检查点，指标监视和容错能力。
- 根据测试数据评估模型，以及如何将其用于推断新数据。
- 自定义`fit()`的功能，例如构建GAN。
- 利用多个GPU加快培训速度。
- 通过超参数调整来优化模型。

在本指南的最后，您将获得指向端到端示例的指针，以巩固这些概念：

- 图片分类
- 文字分类
- 信用卡欺诈检测

## 数据处理

神经网络不处理原始数据，例如文本文件，编码的JPEG图像文件或CSV文件。他们处理矢量化和标准化表示。

- 文本文件需要读取为字符串张量，然后拆分为单词。最后，单词需要索引并转换为整数张量。
- 需要读取图像并将其解码为整数张量，然后将其转换为浮点并归一化为较小的值（通常在0到1之间）。
- 需要分析CSV数据，将数字特征转换为浮点张量，对分类特征进行索引并转换为整数张量。然后，通常需要将每个特征归一化为零均值和单位方差。

### 数据加载

Keras模型接受三种类型的输入：

- NumPy数组，就像Scikit-Learn和许多其他基于Python的库一样。如果您的数据适合内存，这是一个不错的选择。
- TensorFlow数据集对象。这是一个高性能的选项，它更适合于内存不足且从磁盘或分布式文件系统流式传输的数据集。
- 生成大量数据的Python生成器（例如`keras.utils.Sequence`类的自定义子类）。

在开始训练模型之前，您需要将数据以上格式提供。如果您有大型数据集并且正在使用GPU进行培训，请考虑使用Dataset对象，因为它们将处理对性能至关重要的细节，例如：

```
- 在GPU繁忙时，在CPU上异步预处理数据，并将其缓冲到队列中。
- 在GPU内存上预取数据，以便在GPU处理完前一批数据后立即可用，因此可以充分利用GPU的资源。
```

Keras具有一系列实用程序，可帮助您将磁盘上的原始数据转换为数据集：

```
- tf.keras.preprocessing.image_dataset_from_directory将分类到特定于类的文件夹中的图像文件转换为带标签的图像张量数据集。
- tf.keras.preprocessing.text_dataset_from_directory对文本文件执行相同的操作。
```

此外，TensorFlow `tf.data`包括其他类似的实用程序，例如`tf.data.experimental.make_csv_dataset`，用于从CSV文件加载结构化数据。

> 示例：从磁盘上的图像文件获取标记的数据集

假设您在不同的文件夹中按类对图像文件进行了排序，如下所示：

```
main_directory/
...class_a/
......a_image_1.jpg
......a_image_2.jpg
...class_b/
......b_image_1.jpg
......b_image_2.jpg
```

之后你可以做

```python
# 创建一个数据集
dataset = keras.preprocessing.image_dataset_from_directory(
  'path/to/main_directory', batch_size=64, image_size=(200, 200))

# 为了演示，迭代数据集产生的批次
for data, labels in dataset:
   print(data.shape)  # (64, 200, 200, 3)
   print(data.dtype)  # float32
   print(labels.shape)  # (64,)
   print(labels.dtype)  # int32
```

样品的标签是其文件夹按字母数字顺序排列的等级。自然地，这也可以通过例如传递来明确地配置。`class_names = ['class_a'，'class_b']`，在这种情况下，标签0将为`class_a`，而标签1将为`class_b`。

> 示例：从磁盘上的文本文件获取标记的数据集

对于文本也是如此：如果您将.txt文档按类放在不同的文件夹中，则可以执行以下操作：

```python
dataset = keras.preprocessing.text_dataset_from_directory(
  'path/to/main_directory', batch_size=64)

# 为了演示，迭代数据集产生的批次
for data, labels in dataset:
   print(data.shape)  # (64,)
   print(data.dtype)  # string
   print(labels.shape)  # (64,)
   print(labels.dtype)  # int32
```

### 数据预处理

一旦您的数据采用字符串/整数/浮点数NumpPy数组的形式，或者生成一批字符串/整数/浮点数张量的数据集对象（或Python生成器），就该对数据进行预处理了。这可能意味着：

```
- 字符串数据的标记化，然后进行标记索引。
- 特征标准化。
- 将数据重新缩放为较小的值（通常，神经网络的输入值应接近零-通常，我们期望均值和单位方差为零的数据或[0，1]范围内的数据。
```

- 理想的机器学习模型是端到端的

通常，您应该尽可能地将数据作为模型的一部分进行预处理，而不是通过外部数据预处理管道进行。这是因为外部数据预处理使您的模型在生产中使用时不那么便携。考虑一个处理文本的模型：它使用特定的标记化算法和特定的词汇索引。当您要将模型运送到移动应用程序或JavaScript应用程序时，您将需要使用目标语言重新创建完全相同的预处理设置。这可能会变得非常棘手：原始管道与您重新创建的管道之间的任何细微差异都有可能完全使您的模型无效，或者至少严重降低其性能。

能够简单地导出已经包含预处理的端到端模型会容易得多。**理想模型应该期望输入的内容尽可能接近原始数据：图像模型应该期望[0，255]范围内的RGB像素值，而文本模型应该接受utf-8字符的字符串。**这样，导出模型的使用者就不必了解预处理管道。

- 使用Keras预处理层

在Keras中，您可以通过**预处理层**进行模型内数据预处理。这包括：
```
- 通过`TextVectorization`层向量化文本的原始字符串
- 通过`Normalization`层进行特征归一化
- 图像缩放，裁剪或图像数据扩充
```
使用Keras预处理层的主要优点是，可以在训练过程中或训练后将**它们直接包含在模型中**，从而使模型可移植。

一些预处理层具有以下状态：
```
- TextVectorization拥有将单词或标记映射到整数索引的索引
- Normalization保存特征的均值和方差
```
预处理层的状态是通过在训练数据（或全部）样本上调用`layer.adapt(data)`来获得的。

> 示例：将字符串转换为整数单词索引的序列

```python
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

# string类型的示例训练数据
training_data = np.array([["This is the 1st sample."], ["And here's the 2nd sample."]])

# 创建一个TextVectorization图层实例。可以配置为返回整数令牌索引或密集令牌表示形式（例如，多热点或TF-IDF）。文本标准化和文本分割算法完全可配置的。
vectorizer = TextVectorization(output_mode="int")

# 在数组或数据集上调用adapt会使图层生成词汇表数据索引，在查看新数据时可以重新使用。
vectorizer.adapt(training_data)

# 调用Adapt之后，该层就可以在adapt()数据中对之前见过的任何n-gram进行编码
# 未知的n-gram通过out-of-vocabulary的令牌编码。
integer_data = vectorizer(training_data)
print(integer_data)
# tf.Tensor([[4 5 2 9 3] [7 6 2 8 3]], shape=(2, 5), dtype=int64)
```

> 示例：将字符串转换为一键编码双字母组的序列

```python
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

# string类型的示例训练数据
training_data = np.array([["This is the 1st sample."], ["And here's the 2nd sample."]])

# 创建一个TextVectorization图层实例。可以配置为返回整数令牌索引或密集令牌表示形式（例如，多热点或TF-IDF）。文本标准化和文本分割算法完全可配置的。
vectorizer = TextVectorization(output_mode="binary", ngrams=2)

# 在数组或数据集上调用adapt会使图层生成词汇表数据索引，在查看新数据时可以重新使用。
vectorizer.adapt(training_data)

# 调用Adapt之后，该层就可以在adapt()数据中对之前见过的任何n-gram进行编码
# 未知的n-gram通过out-of-vocabulary的令牌编码。
integer_data = vectorizer(training_data)
print(integer_data)
# tf.Tensor([[0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1.] [0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0.]], shape=(2, 17), dtype=float32)
```

> 示例：标准化特征

```python
from tensorflow.keras.layers.experimental.preprocessing import Normalization

# 示例图像数据，其值在[0，255]范围内
training_data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype("float32")

normalizer = Normalization(axis=-1)
normalizer.adapt(training_data)

normalized_data = normalizer(training_data)
print("var: %.4f" % np.var(normalized_data))
print("mean: %.4f" % np.mean(normalized_data))
# var: 1.0000  
# mean: 0.0000
```

> 示例：重新缩放和中心裁剪图像

`Rescaling`层和`CenterCrop`层都是无状态的，因此在这种情况下不必调用`Adapt()`。

```python
from tensorflow.keras.layers.experimental.preprocessing import CenterCrop
from tensorflow.keras.layers.experimental.preprocessing import Rescaling

# 示例图像数据，其值在[0，255]范围内
training_data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype("float32")

cropper = CenterCrop(height=150, width=150)
scaler = Rescaling(scale=1.0 / 255)

output_data = scaler(cropper(training_data))
print("shape:", output_data.shape)
print("min:", np.min(output_data))
print("max:", np.max(output_data))
# shape: (64, 150, 150, 3)
# min: 0.0
# max: 1.0
```

## 构建模型

“层”是简单的输入-输出转换（例如上面的缩放和中心裁剪转换）。例如，下面是一个线性投影层，它将其输入映射到16维特征空间：

```python
dense = keras.layers.Dense(units=16)
```

“模型”是层的有向无环图。您可以将模型视为包含多个子层的“更大层”，并且可以通过向数据暴露来对其进行训练。

构建Keras模型的最常见、最强大的方法是Functional API。要使用Functional API构建模型，首先要指定输入的形状（以及可选的dtype）。如果输入的任何维度可以变化，则可以将其指定为`None`。例如，用于200x200 RGB图像的输入将具有形状`(200,200,3)`，但是用于任何大小的RGB图像的输入将具有形状`(None,None,3)`。

```python
# 假设我们期望输入是任意大小的RGB图像
inputs = keras.Input(shape=(None, None, 3))
```

定义输入后，您可以在输入之上链接图层转换，直到最终输出：

```python
from tensorflow.keras import layers

# 中心裁剪图片到150x150
x = CenterCrop(height=150, width=150)(inputs)
# 重新缩放图片至[0, 1]
x = Rescaling(scale=1.0 / 255)(x)

# 应用一些卷积和池化层
x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation="relu")(x)
x = layers.MaxPooling2D(pool_size=(3, 3))(x)
x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation="relu")(x)
x = layers.MaxPooling2D(pool_size=(3, 3))(x)
x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation="relu")(x)

# 应用全局平均池化以获得展开特征向量
x = layers.GlobalAveragePooling2D()(x)

# 在顶部添加密集分类器
num_classes = 10
outputs = layers.Dense(num_classes, activation="softmax")(x)
```

一旦定义了有向非循环有向图，将您的输入转换为输出，就可以实例化Model对象：

```python
model = keras.Model(inputs=inputs, outputs=outputs)
```

该模型的行为基本上像一个更大的层。您可以按批量数据调用它，如下所示：

```python
data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype("float32")
processed_data = model(data)
print(processed_data.shape)
# (64,10)
```

您可以打印有关在模型的每个阶段如何转换数据的摘要。这对于调试很有用。

请注意，为每个图层显示的输出形状包括批次大小。此处的批次大小为`None`，这表明我们的模型可以处理任何大小的批次。

```python
model.summary()
```

Functional API还使构建具有多个输入（例如，图像及其元数据）或多个输出（例如，预测图像的类别以及用户点击图像的可能性）的模型变得容易。要更深入地了解您可以做什么，请参阅我们的[Functional API指南](https://keras.io/guides/functional_api/).。

## 训练模型

### 默认训练

至此，您知道了：
```
- 如何准备数据（例如，作为NumPy数组或tf.data.Dataset对象）
- 如何建立将处理您的数据的模型
```
下一步是根据数据训练模型。Model类具有内置的训练循环`fit()`方法。它接受`Dataset`对象，产生大量数据的Python生成器或NumPy数组。

在调用`fit()`之前，您需要指定优化器和损失函数（假设您已经熟悉这些概念）。这是`compile()`步骤：

```python
model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),
              loss=keras.losses.CategoricalCrossentropy())
```

可以通过其字符串标识符指定损失和优化器（在这种情况下，使用其默认构造函数参数值）：

```python
model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
```

编译模型后，即可开始将模型“拟合”到数据中。这是使用NumPy数据拟合模型的样子：

```python
model.fit(numpy_array_of_samples, numpy_array_of_labels,
          batch_size=32, epochs=10)
```

除了数据之外，您还必须指定两个关键参数：`batch_size`和`epochs`（数据上的迭代次数）。在这里，我们的数据将按32个样本的批次进行切片，并且模型将在训练期间对数据进行10次迭代。

这是将模型与数据集拟合的结果：

```python
model.fit(dataset_of_samples_and_labels, epochs=10)
```

由于预期数据集产生的数据已被批处理，因此您无需在此处指定批处理大小。

让我们用一个玩具示例模型在实践中进行研究，该模型学习对MNIST数字进行分类：

```python
# Get the data as Numpy arrays
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Build a simple model
inputs = keras.Input(shape=(28, 28))
x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(inputs)
x = layers.Flatten()(x)
x = layers.Dense(128, activation="relu")(x)
x = layers.Dense(128, activation="relu")(x)
outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs, outputs)
model.summary()

# Compile the model
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")

# Train the model for 1 epoch from Numpy data
batch_size = 64
print("Fit on NumPy data")
history = model.fit(x_train, y_train, batch_size=batch_size, epochs=1)

# Train the model for 1 epoch using a dataset
dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)
print("Fit on Dataset")
history = model.fit(dataset, epochs=1)
```

`fit()`调用返回一个“历史”对象，该对象记录了训练过程中发生的情况。`history.history`字典包含度量值的每个时期的时间序列（这里我们只有一个度量，一个损失和一个时期，因此我们只能得到一个标量）

```python
print(history.history)
# {'loss': [0.11615095287561417]}
```

有关`fit()`用法的详细概述，请参阅[内置Keras方法的培训和评估指南](https://keras.io/guides/training_with_built_in_methods/).。

- 跟踪性能指标

在训练模型时，您希望跟踪指标，例如分类准确性，精度，召回率，AUC等。此外，您不仅要在训练数据上而且要在验证集上监视这些指标。

> 监控指标

您可以将指标对象列表传递给`compile()`，如下所示：

```python
model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=[keras.metrics.SparseCategoricalAccuracy(name="acc")],
)
history = model.fit(dataset, epochs=1)
```

> 将验证数据传递给fit()

您可以将验证数据传递给`fit()`来监视您的验证损失和验证指标。验证指标在每个迭代结束时报告。

```python
val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)
history = model.fit(dataset, epochs=1, validation_data=val_dataset)
```

- 使用回调进行检查点（以及更多）

如果训练进行了几分钟以上，那么在训练期间定期保存模型非常重要。然后，您可以使用保存的模型重新开始训练，以防训练过程崩溃（这对于多工分布式训练非常重要，因为对于许多worker而言，其中至少有一个worker一定会失败）。

Keras的一个重要功能是在`fit()`中配置的**回调**。回调是模型在训练过程中的不同点被调用的对象，尤其是：

```
- 每批的开始和结束
- 在每个迭代的开始和结束
```

回调是使模型可训练的完全可编写脚本的一种方法。

您可以使用回调来定期保存模型。这是一个简单的示例：`ModelCheckpoint`回调配置为在每个迭代结束时保存模型。文件名将包含当前迭代。

```python
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath='path/to/my/model_{epoch}',
        save_freq='epoch')
]
model.fit(dataset, epochs=2, callbacks=callbacks)
```

您还可以使用回调来执行诸如定期更改优化器的学习，将指标流式传输到Slack机器人，在培训完成后向自己发送电子邮件通知之类的事情。

有关可用的回调以及如何编写自己的回调的详细概述，请参阅[回调API文档](https://keras.io/api/callbacks/) 和[编写自定义回调的指南](https://keras.io/guides/writing_your_own_callbacks/)。

- 使用TensorBoard监控培训进度

盯着Keras进度栏并不是最符合人体工程学的可以监控您的损失和指标随着时间的变化的方法。有一个更好的解决方案：`TensorBoard`，它是一个Web应用程序，可以显示您的指标（以及更多）的实时图形。

要将`TensorBoard`与`fit()`结合使用，只需传递`keras.callbacks.TensorBoard`回调，指定存储`TensorBoard`日志的目录：

```python
callbacks = [
    keras.callbacks.TensorBoard(log_dir='./logs')
]
model.fit(dataset, epochs=2, callbacks=callbacks)
```

然后，您可以启动一个TensorBoard实例，您可以在浏览器中打开该实例，以监视写入该位置的日志：

```python
tensorboard --logdir=./logs
```

此外，当在`Jupyter/Colab`笔记本中训练模型时，您可以启动一个嵌入式TensorBoard选项卡。这是[更多信息](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks)。

-  `fit()`之后：评估测试性能并根据新数据生成预测

拥有训练有素的模型后，您可以通过`fit()`评估其损失和新数据的指标：

```python
loss, acc = model.evaluate(val_dataset)  # 返回损失和指标
print("loss: %.2f" % loss)
print("acc: %.2f" % acc)
# loss: 0.12
# acc: 0.97
```

您还可以通过`predict()`生成预测值（模型中输出层的激活）的NumPy数组：

```python
predictions = model.predict(val_dataset)
print(predictions.shape)
# (10000, 10)
```

### 自定义训练

默认情况下，`fit()`配置为监督学习。如果您需要其他类型的训练循环（例如GAN训练循环），则可以提供自己的`Model.train_step()`方法的实现。这是在`fit()`中反复调用的方法。

指标，回调等将照常工作。

这是一个简单的示例，可重新实现`fit()`正常执行的操作

```python
class CustomModel(keras.Model):
  def train_step(self, data):
    # 解压缩数据。它的结构取决于您的模型以及传递给fit()的内容。
    x, y = data
    with tf.GradientTape() as tape:
      y_pred = self(x, training=True)  # Forward pass
      # 计算损失值(损失函数在compile()中配置)
      loss = self.compiled_loss(y, y_pred,
                                regularization_losses=self.losses)
    # 计算梯度
    trainable_vars = self.trainable_variables
    gradients = tape.gradient(loss, trainable_vars)
    # 更新权重
    self.optimizer.apply_gradients(zip(gradients, trainable_vars))
    # 更新指标 (包含跟踪损失的指标)
    self.compiled_metrics.update_state(y, y_pred)
    # 返回一个将指标名称映射到当前值的字典
    return {m.name: m.result() for m in self.metrics}

# 构造并编译CustomModel的实例
inputs = keras.Input(shape=(32,))
outputs = keras.layers.Dense(1)(inputs)
model = CustomModel(inputs, outputs)
model.compile(optimizer='adam', loss='mse', metrics=[...])

# 和常规使用fit一样
model.fit(dataset, epochs=3, callbacks=...)
```

有关如何自定义内置训练和评估循环的详细概述，请参见指南：["Customizing what happens in fit()"](https://keras.io/guides/customizing_what_happens_in_fit/)。

## 调试模型

如果编写自定义培训步骤或自定义图层，则需要对其进行调试。调试经验是框架不可或缺的一部分：使用Keras，调试工作流程的设计为用户做了考虑。

默认情况下，您的Keras模型被编译为高度优化的计算图，可缩短执行时间。这意味着您编写的Python代码（例如在自定义的`train_step`中）不是您实际执行的代码。这引入了一个间接层，可以使调试变得困难。

调试最好一步一步完成。您希望能够在代码中添加`print()`语句，以查看每次操作后数据的样子，并且希望能够使用`pdb`。您可以通过**积极运行模型**来实现。积极 运行模式下，您编写的Python代码就是要执行的代码。

只需将`run_eagerly = True`传递给`compile()`

```python
model.compile(optimizer='adam', loss='mse', run_eagerly=True)
```

当然，缺点是它会使您的模型速度大大降低。完成调试后，请确保将其关闭以获取编译后的计算图的好处！

通常，每次需要调试`fit()`调用中发生的情况时，都将使用`run_eagerly = True`。

## 加速训练

使用多个GPU加快训练速度

Keras通过`tf.distribute` API为多GPU培训和分布式多worker培训提供了内置的工业级别支持。

如果您的计算机上有多个GPU，则可以通过以下方法在所有GPU上训练模型：

- 创建一个`tf.distribute.MirroredStrategy`对象
- 在策略范围内构建和编译模型
- 照常在数据集上调用`fit()`和`valuate()`

```python
# 创建一个MirroredStrategy.
strategy = tf.distribute.MirroredStrategy()

# 打开策略范围
with strategy.scope():
  # 所有创建变量的内容都应在策略范围内。
  # 通常，这只是模型构造和compile()。
  model = Model(...)
  model.compile(...)

# 在所有可用设备上训练模型
train_dataset, val_dataset, test_dataset = get_dataset()
model.fit(train_dataset, epochs=2, validation_data=val_dataset)

# 在所有可用设备上测试模型
model.evaluate(test_dataset)
```

有关多GPU和分布式培训的详细介绍，请参阅[分布式训练](https://keras.io/guides/distributed_training/)。

## 同步异步预处理

在设备上进行同步与在主机CPU上进行异步预处理对比

您已经了解了预处理，并且已经看到了将图像预处理层（`CenterCrop`和`Rescaling`）直接放置在模型中的示例。

如果您想进行设备上的预处理，例如在GPU加速的功能归一化或图像增强方面进行训练，那么在训练过程中将预处理作为模型的一部分进行非常好。但是，有些预处理不适合此设置：特别是使用`TextVectorization`层进行文本预处理。由于其顺序性质以及只能在CPU上运行的事实，因此进行异步预处理通常是一个好主意。

使用异步预处理，您的预处理操作将在CPU上运行，并且在GPU忙于处理前一批数据时，预处理后的样本将被缓存到队列中。然后，将在GPU再次可用（预取）之前，将下一批预处理后的样本从队列中提取到GPU内存中。这确保了预处理不会被阻塞，并且您的GPU可以充分利用。

要进行异步预处理，只需使用`dataset.map`将预处理操作注入到数据管道中

```python
# string类型的示例训练数据
samples = np.array([["This is the 1st sample."], ["And here's the 2nd sample."]])
labels = [[0], [1]]

# 准备一个TextVectorization层
vectorizer = TextVectorization(output_mode="int")
vectorizer.adapt(samples)

# 异步预处理: 文本向量化是tf.data管道一部分
# 创建一个数据集
dataset = tf.data.Dataset.from_tensor_slices((samples, labels)).batch(2)
# 将文本向量化应用于样本
dataset = dataset.map(lambda x, y: (vectorizer(x), y))
# 预取缓冲区大小为2批
dataset = dataset.prefetch(2)

# 我们的模型应该期望整数序列作为输入
inputs = keras.Input(shape=(None,), dtype="int64")
x = layers.Embedding(input_dim=10, output_dim=32)(inputs)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

model.compile(optimizer="adam", loss="mse", run_eagerly=True)
model.fit(dataset)
```

将此与将文本向量化作为模型的一部分进行比较：

```python
# 我们的数据集将生成字符串样本
dataset = tf.data.Dataset.from_tensor_slices((samples, labels)).batch(2)

# 我们的模型应该期望字符串作为输入
inputs = keras.Input(shape=(1,), dtype="string")
x = vectorizer(inputs)
x = layers.Embedding(input_dim=10, output_dim=32)(x)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

model.compile(optimizer="adam", loss="mse", run_eagerly=True)
model.fit(dataset)
```

在CPU上训练文本模型时，通常不会在两个设置之间看到任何性能差异。但是，在GPU上进行训练时，在GPU运行模型本身的同时在主机CPU上进行异步缓冲预处理可能会大大提高速度。

训练后，如果要导出包括预处理层的端到端模型，这很容易做到，因为`TextVectorization`是一个层：

```python
inputs = keras.Input(shape=(1,), dtype='string')
x = vectorizer(inputs)
outputs = trained_model(x)
end_to_end_model = keras.Model(inputs, outputs)
```

## 超参数配置

通过超参数调整找到最佳的模型配置

建立工作模型后，您将需要优化其配置-体系结构选择，层大小等。人的直觉只能走得那么远，因此您将需要利用一种系统化的方法：超参数搜索。

您可以使用[Keras Tuner](https://keras-team.github.io/keras-tuner/documentation/tuners/)查找适合您的Keras模型的最佳超参数。就像调用`fit()`一样简单。

在这里展示它是如何工作的。

首先，将模型定义放在一个带有单个`hp`参数的函数中。在此函数内，请调用超参数采样方法，例如，将要调整的任何值替换为`hp.Int()`或`hp.Choice()`：

```python
def build_model(hp):
    inputs = keras.Input(shape=(784,))
    x = layers.Dense(
        units=hp.Int('units', min_value=32, max_value=512, step=32),
        activation='relu')(inputs)
    outputs = layers.Dense(10, activation='softmax')(x)
    model = keras.Model(inputs, outputs)
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate',
                      values=[1e-2, 1e-3, 1e-4])),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])
    return model
```

该函数应返回已编译的模型。

接下来，实例化一个指定优化目标和其他搜索参数的调谐器对象：

```python
import kerastuner

tuner = kerastuner.tuners.Hyperband(
  build_model,
  objective='val_loss',
  max_epochs=100,
  max_trials=200,
  executions_per_trial=2,
  directory='my_dir')
```

最后，使用`search()`方法开始搜索，该方法采用与`Model.fit()`相同的参数：

```python
tuner.search(dataset, validation_data=val_dataset)
```

搜索结束后，您可以检索最佳模型：

```python
models = tuner.get_best_models(num_models=2)
```

或打印结果摘要

```python
tuner.results_summary()
```

## 端对端示例

要熟悉本简介中的概念，请参见以下端到端示例：

- [Text classification](https://keras.io/examples/nlp/text_classification_from_scratch/)
- [Image classification](https://keras.io/examples/vision/image_classification_from_scratch/)
- [Credit card fraud detection](https://keras.io/examples/structured_data/imbalanced_classification/)

## 进一步学习

- Learn more about the [Functional API](https://keras.io/guides/functional_api/).
- Learn more about the [features of `fit()` and `evaluate()`](https://keras.io/guides/training_with_built_in_methods/).
- Learn more about [callbacks](https://keras.io/guides/writing_your_own_callbacks/).
- Learn more about [creating your own custom training steps](https://keras.io/guides/customizing_what_happens_in_fit/).
- Learn more about [multi-GPU and distributed training](https://keras.io/guides/distributed_training/).
- Learn how to do [transfer learning](https://keras.io/guides/transfer_learning/).