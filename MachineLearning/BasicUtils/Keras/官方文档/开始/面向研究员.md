# 面向研究员

```python
import tensorflow as tf
from tensorflow import keras
```

## 概述

您是机器学习研究员吗？您是否在NeurIPS上发布并推动CV和NLP的最新发展？本指南将作为您首次介绍Keras API核心概念。

在本指南中，您将了解：

- 通过子类化`Layer`类来创建层
- 使用`GradientTape`计算梯度并编写低级训练循环
- 通过`add_loss()`方法跟踪由图层创建的损耗
- 在低级训练循环中跟踪指标
- 使用编译的`tf.function`加快执行速度
- 以训练或推理模式执行图层
- Keras功能API

您还将在两个端到端研究示例中看到Keras API的运行情况：变体自动编码器和超网络。

## `Layer`类

层是Keras中的基本抽象。层封装状态（权重）和一些计算（在call方法中定义）。

一个简单的层如下所示：

```python
class Linear(keras.layers.Layer):
    """y = w.x + b"""

    def __init__(self, units=32, input_dim=32):
        super(Linear, self).__init__()
        w_init = tf.random_normal_initializer()
        self.w = tf.Variable(
            initial_value=w_init(shape=(input_dim, units), dtype="float32"),
            trainable=True,
        )
        b_init = tf.zeros_initializer()
        self.b = tf.Variable(
            initial_value=b_init(shape=(units,), dtype="float32"), trainable=True
        )

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b
```

您将使用类似于Python函数的Layer实例：

```python
# 实例化我们的图层
linear_layer = Linear(units=4, input_dim=2)

# 可以将图层视为函数
# 我们使用一些数据调用它
y = linear_layer(tf.ones((2, 2)))
assert y.shape == (2, 4)
```

权重变量（在`__init__`中创建）在`weights`属性下自动跟踪：

```python
assert linear_layer.weights == [linear_layer.w, linear_layer.b]
```

从Dense到Conv2D到LSTM，再到像Conv3DTranspose或ConvLSTM2D这样的更高级的层，您都有许多可用的内置层。明智地重用内置功能。

## 权重创建

`add_weight`方法为您提供了创建权重的快捷方式

```python
class Linear(keras.layers.Layer):
    """y = w.x + b"""

    def __init__(self, units=32):
        super(Linear, self).__init__()
        self.units = units

    def build(self, input_shape):
        self.w = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer="random_normal",
            trainable=True,
        )
        self.b = self.add_weight(
            shape=(self.units,), initializer="random_normal", trainable=True
        )

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b


# 实例化图层
linear_layer = Linear(4)

# 调用build(input_shape)创建权重
y = linear_layer(tf.ones((2, 2)))
```

## 梯度

您可以通过在`GradientTape`内部调用它来自动检索一个层的权重的梯度。使用这些梯度，您可以手动或使用优化器对象更新图层的权重。当然，如果需要，您可以在使用梯度之前先对其进行修改

```python
# 准备一个数据集
(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
dataset = tf.data.Dataset.from_tensor_slices(
    (x_train.reshape(60000, 784).astype("float32") / 255, y_train)
)
dataset = dataset.shuffle(buffer_size=1024).batch(64)

# 用10个单位实例化我们的线性层（上面定义）。
linear_layer = Linear(10)

# 实例化一个期望整数目标的逻辑损失函数。
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# 实例化优化器
optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)

# 遍历数据集的批次
for step, (x, y) in enumerate(dataset):

    # 打开一个GradientTape.
    with tf.GradientTape() as tape:

        # 前向传播
        logits = linear_layer(x)

        # 每批数据的损失值
        loss = loss_fn(y, logits)

    # 获得损失的权重梯度
    gradients = tape.gradient(loss, linear_layer.trainable_weights)

    # 更新线性层的权重
    optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))

    # 日志
    if step % 100 == 0:
        print("Step:", step, "Loss:", float(loss))
```

## 可训练和不可训练的权重

图层创建的权重可以是可训练的，也可以是不可训练的。它们分别以`trainable_weights`和`non_trainable_weights`公开。这是一个权重不可训练的图层:

```python
class ComputeSum(keras.layers.Layer):
    """Returns the sum of the inputs."""

    def __init__(self, input_dim):
        super(ComputeSum, self).__init__()
        # Create a non-trainable weight.
        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)

    def call(self, inputs):
        self.total.assign_add(tf.reduce_sum(inputs, axis=0))
        return self.total


my_sum = ComputeSum(2)
x = tf.ones((2, 2))

y = my_sum(x)
print(y.numpy())  # [2. 2.]

y = my_sum(x)
print(y.numpy())  # [4. 4.]

assert my_sum.weights == [my_sum.total]
assert my_sum.non_trainable_weights == [my_sum.total]
assert my_sum.trainable_weights == []
```

## 拥有图层的图层

可以递归嵌套图层以创建更大的计算块。每一层将跟踪其子层的权重（可训练和不可训练）。

```python
# 让我们通过上面定义的build方法重用Linear类。
class MLP(keras.layers.Layer):
    """Simple stack of Linear layers."""

    def __init__(self):
        super(MLP, self).__init__()
        self.linear_1 = Linear(32)
        self.linear_2 = Linear(32)
        self.linear_3 = Linear(10)

    def call(self, inputs):
        x = self.linear_1(inputs)
        x = tf.nn.relu(x)
        x = self.linear_2(x)
        x = tf.nn.relu(x)
        return self.linear_3(x)


mlp = MLP()

# 首次调用mlp对象将创建权重。
y = mlp(tf.ones(shape=(3, 64)))

# 权重被递归跟踪
assert len(mlp.weights) == 6
```

请注意，我们上面手动创建的MLP等效于以下内置选项

```python
mlp = keras.Sequential(
    [
        keras.layers.Dense(32, activation=tf.nn.relu),
        keras.layers.Dense(32, activation=tf.nn.relu),
        keras.layers.Dense(10),
    ]
)
```

## 追踪各层造成的损失

图层可能会通过`add_loss()`方法在前向传递过程中造成损失。这对于正则化损失特别有用。子层产生的损耗由父层递归跟踪。

这是一个创建活动正则化损失的层：

```python
class ActivityRegularization(keras.layers.Layer):
    """生成活动稀疏性规范化损失的层"""

    def __init__(self, rate=1e-2):
        super(ActivityRegularization, self).__init__()
        self.rate = rate

    def call(self, inputs):
        # 我们使用add_loss创建依赖于输入的正则化损失
        self.add_loss(self.rate * tf.reduce_sum(inputs))
        return inputs
```

任何包含此层的模型都将跟踪此正则化损失

```python
# 让我们在MLP块中使用损耗层
class SparseMLP(keras.layers.Layer):
    """具有稀疏正则化损失的线性层堆栈。"""

    def __init__(self):
        super(SparseMLP, self).__init__()
        self.linear_1 = Linear(32)
        self.regularization = ActivityRegularization(1e-2)
        self.linear_3 = Linear(10)

    def call(self, inputs):
        x = self.linear_1(inputs)
        x = tf.nn.relu(x)
        x = self.regularization(x)
        return self.linear_3(x)


mlp = SparseMLP()
y = mlp(tf.ones((10, 10)))

print(mlp.losses)  # 包含一个float32标量的列表
```

这些损失会在每次正向传递开始时由顶层清除-它们不会累积。`layer.losses`始终仅包含在上一个正向传递过程中创建的损耗。在编写训练循环时，通常会在计算梯度之前将它们相加来使用这些损耗。

```python
# 损失对应于最后前向通行证。
mlp = SparseMLP()
mlp(tf.ones((10, 10)))
assert len(mlp.losses) == 1
mlp(tf.ones((10, 10)))
assert len(mlp.losses) == 1  # No accumulation.

# 让我们演示如何在训练循环中使用这些损失。

# 准备数据集
(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
dataset = tf.data.Dataset.from_tensor_slices(
    (x_train.reshape(60000, 784).astype("float32") / 255, y_train)
)
dataset = dataset.shuffle(buffer_size=1024).batch(64)

# A new MLP.
mlp = SparseMLP()

# Loss and optimizer.
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)

for step, (x, y) in enumerate(dataset):
    with tf.GradientTape() as tape:

        # Forward pass.
        logits = mlp(x)

        # 此批次的外部损失值
        loss = loss_fn(y, logits)

        # 加上前向传递过程中产生的损失
        loss += sum(mlp.losses)

        # 获得损失的权重梯度
        gradients = tape.gradient(loss, mlp.trainable_weights)

    # 更新线性层的权重
    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))

    # 日志
    if step % 100 == 0:
        print("Step:", step, "Loss:", float(loss))
```

## 跟踪训练指标

Keras提供了广泛的内置指标，例如`tf.keras.metrics.AUC`或`tf.keras.metrics.PrecisionAtRecall`。用几行代码创建自己的指标也很容易。

要在自定义训练循环中使用指标，您需要：

- Instantiate the metric object, e.g. `metric = tf.keras.metrics.AUC()`
- Call its `metric.udpate_state(targets, predictions)` method for each batch of data
- Query its result via `metric.result()`
- Reset the metric's state at the end of an epoch or at the start of an evaluation via `metric.reset_states()`

这是一个简单的例子：

```python
# Instantiate a metric object
accuracy = tf.keras.metrics.SparseCategoricalAccuracy()

# Prepare our layer, loss, and optimizer.
model = keras.Sequential(
    [
        keras.layers.Dense(32, activation="relu"),
        keras.layers.Dense(32, activation="relu"),
        keras.layers.Dense(10),
    ]
)
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

for epoch in range(2):
    # Iterate over the batches of a dataset.
    for step, (x, y) in enumerate(dataset):
        with tf.GradientTape() as tape:
            logits = model(x)
            # Compute the loss value for this batch.
            loss_value = loss_fn(y, logits)

        # Update the state of the `accuracy` metric.
        accuracy.update_state(y, logits)

        # Update the weights of the model to minimize the loss value.
        gradients = tape.gradient(loss_value, model.trainable_weights)
        optimizer.apply_gradients(zip(gradients, model.trainable_weights))

        # Logging the current accuracy value so far.
        if step % 200 == 0:
            print("Epoch:", epoch, "Step:", step)
            print("Total running accuracy so far: %.3f" % accuracy.result())

    # Reset the metric's state at the end of an epoch
    accuracy.reset_states()
```

除此之外，类似于`self.add_loss()`方法，您还可以访问图层上的`self.add_metric()`方法。它跟踪您传递给它的任何数量的平均值。您可以通过在任何图层或模型上调用`layer.reset_metrics()`来重置这些指标的值。

## 编译函数

积极地运行对于调试非常有用，但是通过将计算编译为静态图形，可以提高性能。静态图是研究人员最好的朋友。您可以通过将任何函数包装在`tf.function`装饰器中来对其进行编译。

```python
# Prepare our layer, loss, and optimizer.
model = keras.Sequential(
    [
        keras.layers.Dense(32, activation="relu"),
        keras.layers.Dense(32, activation="relu"),
        keras.layers.Dense(10),
    ]
)
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

# Create a training step function.


@tf.function  # Make it fast.
def train_on_batch(x, y):
    with tf.GradientTape() as tape:
        logits = model(x)
        loss = loss_fn(y, logits)
        gradients = tape.gradient(loss, model.trainable_weights)
    optimizer.apply_gradients(zip(gradients, model.trainable_weights))
    return loss


# Prepare a dataset.
(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
dataset = tf.data.Dataset.from_tensor_slices(
    (x_train.reshape(60000, 784).astype("float32") / 255, y_train)
)
dataset = dataset.shuffle(buffer_size=1024).batch(64)

for step, (x, y) in enumerate(dataset):
    loss = train_on_batch(x, y)
    if step % 100 == 0:
        print("Step:", step, "Loss:", float(loss))
```

## 训练和推理模式

某些层，尤其是BatchNormalization层和Dropout层，在训练和推理期间具有不同的行为。对于此类层，标准做法是在`call`方法中公开训练（布尔）参数。

通过在调用中公开此参数，您可以启用内置的训练和评估循环（例如，拟合）以在训练和推理模式下正确使用图层

```python
class Dropout(keras.layers.Layer):
    def __init__(self, rate):
        super(Dropout, self).__init__()
        self.rate = rate

    def call(self, inputs, training=None):
        if training:
            return tf.nn.dropout(inputs, rate=self.rate)
        return inputs


class MLPWithDropout(keras.layers.Layer):
    def __init__(self):
        super(MLPWithDropout, self).__init__()
        self.linear_1 = Linear(32)
        self.dropout = Dropout(0.5)
        self.linear_3 = Linear(10)

    def call(self, inputs, training=None):
        x = self.linear_1(inputs)
        x = tf.nn.relu(x)
        x = self.dropout(x, training=training)
        return self.linear_3(x)


mlp = MLPWithDropout()
y_train = mlp(tf.ones((2, 2)), training=True)
y_test = mlp(tf.ones((2, 2)), training=False)
```

## 用于模型构建的功能性API

要构建深度学习模型，您不必一直使用面向对象的编程。到目前为止，我们已经看到的所有层也可以按功能进行组合，如下所示（我们将其称为“功能API”）：

```python
# We use an `Input` object to describe the shape and dtype of the inputs.
# This is the deep learning equivalent of *declaring a type*.
# The shape argument is per-sample; it does not include the batch size.
# The functional API focused on defining per-sample transformations.
# The model we create will automatically batch the per-sample transformations,
# so that it can be called on batches of data.
inputs = tf.keras.Input(shape=(16,), dtype="float32")

# We call layers on these "type" objects
# and they return updated types (new shapes/dtypes).
x = Linear(32)(inputs)  # We are reusing the Linear layer we defined earlier.
x = Dropout(0.5)(x)  # We are reusing the Dropout layer we defined earlier.
outputs = Linear(10)(x)

# A functional `Model` can be defined by specifying inputs and outputs.
# A model is itself a layer like any other.
model = tf.keras.Model(inputs, outputs)

# A functional model already has weights, before being called on any data.
# That's because we defined its input shape in advance (in `Input`).
assert len(model.weights) == 4

# Let's call our model on some data, for fun.
y = model(tf.ones((2, 16)))
assert y.shape == (2, 10)

# You can pass a `training` argument in `__call__`
# (it will get passed down to the Dropout layer).
y = model(tf.ones((2, 16)), training=True)
```

Functional API倾向于比子类更为简洁，并提供其他一些优点（与无类型OO开发相比，功能性，类型化语言通常具有的相同优势）。但是，它只能用于定义层的DAG-应当将递归网络定义为Layer子类。

在此处了解有关Functional API的[更多信息](https://keras.io/guides/functional_api/)。

在研究工作流程中，您可能经常会发现自己混合搭配了OO模型和功能模型。

请注意，Model类还具有内置的训练和评估循环（`fit()`和`valuate()`）。如果要为OO模型利用这些循环，则可以始终对Model类进行子类化（它的工作原理与对Layer进行子类化一样）。

## 端对端的示例

### 变体自动编码器

到目前为止，您已经学到了一些东西：

- 一层封装了状态（在`__init__`或`build`中创建）和一些计算（在`call`中定义）。
- 可以递归嵌套图层以创建更大的新计算块。
- 您可以通过打开`GradientTape`，在磁带范围内调用模型，然后检索渐变并通过优化程序应用它们来轻松编写易于破解的训练循环。
- 您可以使用`@ tf.function`装饰器加快训练周期。
- 层可以通过`self.add_loss()`创建和跟踪损失（通常是正则化损失）。

让我们将所有这些内容放到一个端到端的示例中：我们将实现一个变体自动编码器（VAE）。我们将用MNIST数字对其进行训练。

我们的VAE将是Layer的子类，构建为子类Layer的嵌套嵌套层。它将具有正则化损失（KL散度）。

以下是我们的模型定义。

首先，我们有一个`Encoder`类，它使用一个`Sampling`层将MNIST数字映射到一个潜在空间三元组`(z_mean,z_log_var,z)`。

```python
from tensorflow.keras import layers


class Sampling(layers.Layer):
    """Uses (z_mean, z_log_var) to sample z, the vector encoding a digit."""

    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon


class Encoder(layers.Layer):
    """Maps MNIST digits to a triplet (z_mean, z_log_var, z)."""

    def __init__(self, latent_dim=32, intermediate_dim=64, **kwargs):
        super(Encoder, self).__init__(**kwargs)
        self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)
        self.dense_mean = layers.Dense(latent_dim)
        self.dense_log_var = layers.Dense(latent_dim)
        self.sampling = Sampling()

    def call(self, inputs):
        x = self.dense_proj(inputs)
        z_mean = self.dense_mean(x)
        z_log_var = self.dense_log_var(x)
        z = self.sampling((z_mean, z_log_var))
        return z_mean, z_log_var, z
```

接下来，我们有一个Decoder类，该类将概率潜在空间坐标映射回MNIST数字。

```python
class Decoder(layers.Layer):
    """Converts z, the encoded digit vector, back into a readable digit."""

    def __init__(self, original_dim, intermediate_dim=64, **kwargs):
        super(Decoder, self).__init__(**kwargs)
        self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)
        self.dense_output = layers.Dense(original_dim, activation=tf.nn.sigmoid)

    def call(self, inputs):
        x = self.dense_proj(inputs)
        return self.dense_output(x)
```

最后，我们的`VariationalAutoEncoder`将编码器和解码器组合在一起，并通过`add_loss()`创建KL发散正则化损失。

```python
class VariationalAutoEncoder(layers.Layer):
    """Combines the encoder and decoder into an end-to-end model for training."""

    def __init__(self, original_dim, intermediate_dim=64, latent_dim=32, **kwargs):
        super(VariationalAutoEncoder, self).__init__(**kwargs)
        self.original_dim = original_dim
        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)
        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)

    def call(self, inputs):
        z_mean, z_log_var, z = self.encoder(inputs)
        reconstructed = self.decoder(z)
        # Add KL divergence regularization loss.
        kl_loss = -0.5 * tf.reduce_mean(
            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
        )
        self.add_loss(kl_loss)
        return reconstructed
```

现在，让我们编写一个训练循环。我们的训练步骤以`@ tf.function`装饰，以编译为超快速图形函数。

```python
# Our model.
vae = VariationalAutoEncoder(original_dim=784, intermediate_dim=64, latent_dim=32)

# Loss and optimizer.
loss_fn = tf.keras.losses.MeanSquaredError()
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

# Prepare a dataset.
(x_train, _), _ = tf.keras.datasets.mnist.load_data()
dataset = tf.data.Dataset.from_tensor_slices(
    x_train.reshape(60000, 784).astype("float32") / 255
)
dataset = dataset.shuffle(buffer_size=1024).batch(32)


@tf.function
def training_step(x):
    with tf.GradientTape() as tape:
        reconstructed = vae(x)  # Compute input reconstruction.
        # Compute loss.
        loss = loss_fn(x, reconstructed)
        loss += sum(vae.losses)  # Add KLD term.
    # Update the weights of the VAE.
    grads = tape.gradient(loss, vae.trainable_weights)
    optimizer.apply_gradients(zip(grads, vae.trainable_weights))
    return loss


losses = []  # Keep track of the losses over time.
for step, x in enumerate(dataset):
    loss = training_step(x)
    # Logging.
    losses.append(float(loss))
    if step % 100 == 0:
        print("Step:", step, "Loss:", sum(losses) / len(losses))

    # Stop after 1000 steps.
    # Training the model to convergence is left
    # as an exercise to the reader.
    if step >= 1000:
        break
```

如您所见，在Keras中建立和训练这种类型的模型既快速又轻松。

现在，您可能会发现上面的代码有些冗长：我们手动处理每个小细节。这样可以提供最大的灵活性，但同时也需要一些工作。

让我们看一下VAE的Functional API版本的外观：

```python
original_dim = 784
intermediate_dim = 64
latent_dim = 32

# Define encoder model.
original_inputs = tf.keras.Input(shape=(original_dim,), name="encoder_input")
x = layers.Dense(intermediate_dim, activation="relu")(original_inputs)
z_mean = layers.Dense(latent_dim, name="z_mean")(x)
z_log_var = layers.Dense(latent_dim, name="z_log_var")(x)
z = Sampling()((z_mean, z_log_var))
encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name="encoder")

# Define decoder model.
latent_inputs = tf.keras.Input(shape=(latent_dim,), name="z_sampling")
x = layers.Dense(intermediate_dim, activation="relu")(latent_inputs)
outputs = layers.Dense(original_dim, activation="sigmoid")(x)
decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name="decoder")

# Define VAE model.
outputs = decoder(z)
vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name="vae")

# Add KL divergence regularization loss.
kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)
vae.add_loss(kl_loss)
```

更简洁吧？

顺便说一句，Keras在其Model类上还具有内置的训练和评估循环（`fit()`和`valuate()`）。看看这个：

```python
# Loss and optimizer.
loss_fn = tf.keras.losses.MeanSquaredError()
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

# Prepare a dataset.
(x_train, _), _ = tf.keras.datasets.mnist.load_data()
dataset = tf.data.Dataset.from_tensor_slices(
    x_train.reshape(60000, 784).astype("float32") / 255
)
dataset = dataset.map(lambda x: (x, x))  # Use x_train as both inputs & targets
dataset = dataset.shuffle(buffer_size=1024).batch(32)

# Configure the model for training.
vae.compile(optimizer, loss=loss_fn)

# Actually training the model.
vae.fit(dataset, epochs=1)
```

使用Functional API和拟合将我们的示例从65行减少到25行（包括模型定义和培训）。Keras的理念是为您提供诸如此类的提高生产力的功能，同时使您能够自己编写所有内容以获得对每个小细节的绝对控制。就像我们在低级训练循环中所做的一样，前面两段。

### 超网络

让我们看一下另一种研究实验：超网络。

超网络是一种深度神经网络，其权重是由另一个网络（通常较小）生成的。

让我们实现一个非常琐碎的超网络：我们将使用一个小的2层网络来生成一个较大的3层网络的权重。

```python
import numpy as np

input_dim = 784
classes = 10

# This is the model we'll actually use to predict labels (the hypernetwork).
outer_model = keras.Sequential(
    [keras.layers.Dense(64, activation=tf.nn.relu), keras.layers.Dense(classes),]
)

# It doesn't need to create its own weights, so let's mark its layers
# as already built. That way, calling `outer_model` won't create new variables.
for layer in outer_model.layers:
    layer.built = True

# This is the number of weight coefficients to generate. Each layer in the
# hypernetwork requires output_dim * input_dim + output_dim coefficients.
num_weights_to_generate = (classes * 64 + classes) + (64 * input_dim + 64)

# This is the model that generates the weights of the `outer_model` above.
inner_model = keras.Sequential(
    [
        keras.layers.Dense(16, activation=tf.nn.relu),
        keras.layers.Dense(num_weights_to_generate, activation=tf.nn.sigmoid),
    ]
)
```

这是我们的训练循环。对于每批数据：

- 我们使用`inner_model`生成权重系数数组`weights_pred`
- 我们将这些系数重塑为`outer_model`的核和偏差张量
- 我们运行`outer_model`的正向传递以计算实际的MNIST预测
- 我们对`inner_model`的权重进行反向传播，以最大程度地减少最终分类损失

```python
# Loss and optimizer.
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)

# Prepare a dataset.
(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
dataset = tf.data.Dataset.from_tensor_slices(
    (x_train.reshape(60000, 784).astype("float32") / 255, y_train)
)

# We'll use a batch size of 1 for this experiment.
dataset = dataset.shuffle(buffer_size=1024).batch(1)


@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        # Predict weights for the outer model.
        weights_pred = inner_model(x)

        # Reshape them to the expected shapes for w and b for the outer model.
        # Layer 0 kernel.
        start_index = 0
        w0_shape = (input_dim, 64)
        w0_coeffs = weights_pred[:, start_index : start_index + np.prod(w0_shape)]
        w0 = tf.reshape(w0_coeffs, w0_shape)
        start_index += np.prod(w0_shape)
        # Layer 0 bias.
        b0_shape = (64,)
        b0_coeffs = weights_pred[:, start_index : start_index + np.prod(b0_shape)]
        b0 = tf.reshape(b0_coeffs, b0_shape)
        start_index += np.prod(b0_shape)
        # Layer 1 kernel.
        w1_shape = (64, classes)
        w1_coeffs = weights_pred[:, start_index : start_index + np.prod(w1_shape)]
        w1 = tf.reshape(w1_coeffs, w1_shape)
        start_index += np.prod(w1_shape)
        # Layer 1 bias.
        b1_shape = (classes,)
        b1_coeffs = weights_pred[:, start_index : start_index + np.prod(b1_shape)]
        b1 = tf.reshape(b1_coeffs, b1_shape)
        start_index += np.prod(b1_shape)

        # Set the weight predictions as the weight variables on the outer model.
        outer_model.layers[0].kernel = w0
        outer_model.layers[0].bias = b0
        outer_model.layers[1].kernel = w1
        outer_model.layers[1].bias = b1

        # Inference on the outer model.
        preds = outer_model(x)
        loss = loss_fn(y, preds)

    # Train only inner model.
    grads = tape.gradient(loss, inner_model.trainable_weights)
    optimizer.apply_gradients(zip(grads, inner_model.trainable_weights))
    return loss


losses = []  # Keep track of the losses over time.
for step, (x, y) in enumerate(dataset):
    loss = train_step(x, y)

    # Logging.
    losses.append(float(loss))
    if step % 100 == 0:
        print("Step:", step, "Loss:", sum(losses) / len(losses))

    # Stop after 1000 steps.
    # Training the model to convergence is left
    # as an exercise to the reader.
    if step >= 1000:
        break
```

用Keras实施任意的研究思想是简单而高效的。想象一下每天尝试25个想法（每个实验平均20分钟）！

Keras的设计宗旨是尽可能快地从想法到结果，因为我们相信这是进行出色研究的关键。

我们希望您喜欢此快速入门。让我们知道您使用Keras构建的内容！