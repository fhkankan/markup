# keras入门

## 数据处理

keras正对几种常见的输入深度学习模型和输入数据形态，提供了几个易于使用的工具来处理数据，包括针对序列模型的数据预处理、针对文字输入的数据处理、针对图片输入的数据处理。

### 文字预处理

在文字的建模实践中，一般都需要把原始文字拆解成单字、单词或者词组，然后将这些拆分后的要素进行索引、标记化供机器学习算法使用。这种预处理叫做标注。虽然可以使用python实现，但是keras提供了现成的高效方法。一般来说，对于已经读入的文字的预处理包含以下几个步骤

```
1. 文字拆分
2. 建立索引
3. 序列补齐
4. 转换为矩阵
5. 使用标注类批量处理文本文件
```

所有跟文字相关的预处理函数都在`Keras.preprocessing.text`库中，但是这个是为英文设计，若是处理中文，可以使用结巴分词里提供的切分函数`cut`进行文字拆分

- 文字拆分

```python
from keras_preprocessing.text import text_to_word_sequence
from jieba import lcut

txt = "Those that come through the Ivory Gate cheat us with empty promises that never see fulfillment. " \
      "Those that come through the Gate of Horn inform the dreamer  of the truth"

out1 = text_to_word_sequence(txt)  # 对句子进行分词，默认字母小写化
out2 = text_to_word_sequence(txt, lower=False)  # 将字母非小写化
out3 = text_to_word_sequence(txt, lower=False, filters="Tha")  # 非小写化，过滤掉Tha字符
print(out1[:6], out2[:6], out3[:6], sep="\n")

# 中文分词，采用结巴分词
chn = '此开卷第一回也。作者自云：曾历过一番梦幻之后，故将真事隐去，而借通灵说此《石头记》一书也，故曰“甄士隐”云云。但书中所记何事何人?' \
      '自己又云：“今风尘碌碌，一事无成，忽念及当日所有之女子：一一细考较去，觉其行止见识皆出我之上。' \
      '我堂堂须眉诚不若彼裙钗，我实愧则有馀，悔又无益，大无可如何之日也。当此日，欲将已往所赖天恩祖德，锦衣纨之时，饫甘餍肥之日，' \
      '背父兄教育之恩，负师友规训之德，以致今日一技无成、半生潦倒之罪，编述一集，以告天下；知我之负罪固多，然闺阁中历历有人，' \
      '万不可因我之不肖，自护己短，一并使其泯灭也。所以蓬牖茅椽，绳床瓦灶，并不足妨我襟怀；况那晨风夕月，阶柳庭花，更觉得润人笔墨。' \
      '我虽不学无文，又何妨用假语村言敷演出来?亦可使闺阁昭传。复可破一时之闷，醒同人之目，不亦宜乎？' \
      '”故曰“贾雨村”云云。更于篇中间用“梦”“幻”等字，却是此书本旨，兼寓提醒阅者之意。'

chnout1 = lcut(chn, cut_all=False)  # 不采用细颗粒度模式
chnout2 = lcut(chn, cut_all=True)  # 采用细颗粒度模式
chnout3 = lcut(chn, cut_all=True, HMM=True)  # 采用细颗粒度模式，使用HMM模型
print([len(chnout1), len(chnout2), len(chnout3)],  chnout1[:8], chnout2[:15], chnout3[:15], sep="\n")
```

- 建立索引

完成分词之后，得到的单字或者单词不能直接用于模型，还需要将它们转换成数字序号，才能进行后续处理。这就是建立索引。

对拆分出来的每一个单字或单词，排序之后编号即可；也可使用One Hot编码法，对于K个不同的单字或词，依次设定一个1到K的数值来索引这个K个字或词构成的词汇表。

```python
# a.排序编号
out1.sort(reverse=True)  # 对原有字符串反向排序
res = dict(list(zip(out1, np.arange(len(out1)))))  # 排序后增加对应索引
print(res)

# one hot
xin = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
tout = (text_to_word_sequence(str(xin)))
xout = one_hot(str(xin), 5)
for s in range(len(xin)):
    print(s, hash(tout[s]) % (5 - 1), xout[s])
```

一般要将大量不同的数据映射到一个有限空间中，通常采用的方法都是哈希表。这个方法的问题在于如果最大索引值不小心设置成小于不同单字或单词 的数量，就有哈希碰撞的问题，那么取值后同样取值的字符实质上并不具备任何关系。而如果最大索引值跟不同字符串的数量一样的话，就会产生极度稀疏的输入矩阵。无论哪种情况，建立的索引在以后建模时效果 都不好。

- 序列补齐

最终索引之后的文字信息会被按照索引编号放入多维矩阵中用来建模。这个多维矩阵的行宽对应于所有拆分后的单字或单词，但是在将索引引入矩阵中之前，需要先进行序列补齐的工作。这是因为将一段话拆分成单一的词以后，丢失了重要的上下文信息，因此将上下文的一组词放在一起建模能保持原来的上下文信息，从而提高建模的质量。序列补齐分两种情况。

第一种情况是自然文本序列，每句话的长度不一，需要进行补齐为统一长度。

第二种情况是将一个由K个(较大)具备一定顺序的单词拆分成小块的连续子串，每个子串只有M个（M<K）单词。这种情况一般是一大段文字按照固定长度移动一个窗口，将窗口内的单词索引载入多维矩阵的每一行，因此一句话可能会对应于矩阵的多行数据，形成时间步（timestep）

对于序列补齐，可以使用`pad_sequences`函数，其输入的要素是列表串。假设有一个列表串，包含了单词的索引号，如下展示了不同设置选项下使用这个函数来补齐序列

```python
from keras_preprocessing.sequence import pad_sequences

x = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]
y0 = pad_sequences(x)  # 默认以最长的从前面补齐
y1 = pad_sequences(x, maxlen=5, padding='post')  # 后面补齐
y2 = pad_sequences(x, maxlen=3, padding='post')  # 截断、后面补齐
y3 = pad_sequences(x, maxlen=3, padding='pre')  # 截断、前面补齐
print(y0, y1, y2, y3, sep="\n========\n")
"""
[[0 1 2 3]
 [0 0 4 5]
 [6 7 8 9]]
========
[[1 2 3 0 0]
 [4 5 0 0 0]
 [6 7 8 9 0]]
========
[[1 2 3]
 [4 5 0]
 [7 8 9]]
========
[[1 2 3]
 [0 4 5]
 [7 8 9]]
"""
```

- 转为矩阵

所有的建模都只能使用多维矩阵，因此需要将索引过的文字元素转换成可以用于建模的矩阵。

方法一：使用`pad_sequences`函数。

```python
max_sentence_len = 50
X = []
for sentences in text:
      x = [word_idx[w] for w in sentences]
      X.append(x)
pad_sequences(X, maxlen=max_sentence_len)
```

方法二：使用标注类

- 使用标注类批量处理

当批量处理文本文件时，需要一种更高效的方法。Keras提供了一个标注类来进行文本处理。

当批量处理文本文件时，一般所有文本会被读入一个大的列表中，每个元素是单个文件的文本或者一段文本。这是针对单一字符串设计的。

标注类中的方法是针对一个文本列表的，标注类中包含了几个使用方法，也能返回所生成的数据的重要统计量，对应的方法包含`texts`或者`sequences`字样，对应于文本列表的方法都将文本拆分成单词串以后执行相应的操作。

```python
# 假设通过open(file).read()将一系列文本文件读入alltext这个列表变量中，每一个元素是一个文本文件中的文本。
# 在进行所有预处理前，先初始化标注对象
from Keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(nb_words=1000)
tokenizer.fit_on_text(alltext)   # 对于输入的文本计算一些关键统计量，并对里面的元素进行索引

# 对整个列表中的元素进行拆分
word_sequence = tokenizer.text_to_sequences(alltext)
# 将文本字符串拆分后，需对每个字符串进行序列补齐，才能转换为可用矩阵
padded_word_sequences = pad_sequences(word_sequence, maxlen=Max_sequence_length)
# 转换为矩阵的方法
tokenizer.text_to_matrix(alltext)
tokenizer.sequence_to_matrix(word_sequence)
```

### 序列数据预处理

处理方法一：不论是补齐还是截断，都是将相邻的连续N个元素连在一起。

处理方法二：把每个单词映射到一个M维的空间（Word2Vec）

```python
from keras_preprocessing.sequence import skipgrams

# 序列数据预处理
z0 = skipgrams([1, 2, 3], 3)  
# 讲一个词向量索引标号按照两种可选方式转化为一系列两两元素的组合(w1,w2)和标注z
# 如果w2跟w1是紧挨着的，标注z为1，是正样本；若w2是从不相邻的其他元素中抽取的，则标注为0，是负样本
res = list(zip(z0[0], z0[1]))
for s in res: print(s)
```

### 图片数据预处理

```python
from keras_preprocessing.image import ImageDataGenerator
```

这个类生成一个数据生成器（Generator）对象，依照循环批量产生对应于图像信息的多维矩阵。多维矩阵的不同维度对应的信息分别是图像二维的像素点，第三维对应于色彩通道，因此，如果是灰度图像，那么色彩通道只有一个维度，如果是RGB色彩，那么色彩通道有三个维度。

## 模型

设定了两类深度学习模型：序列模型（Sequential类）和通用模型（Model类）。其差异在于不同的拓扑结构，实现中体现在定义输入层到输出层的各层结构：

1. 在序列模型中，先定义序列模型对象；而在通用模型中，是先定义从输入层到输出层各层要素，包过尺寸结构
2. 在序列模型中，有了一个模型对象后，可以通过add方法对其依次添加各层信息，包括激活函数和网络尺寸来定义整个神经网络；而在通用模型中，是通过不停地封装各层网络结构的函数作为参数来定义网络结构的。
3. 在序列模型中，各层只能依次线性添加；在通用模型中，因为采用了封装的概念，可以在原有的网络结构上应用新的结构来快速生成新的模型，因此灵活度要高很多，尤其是在具有多种类型输入数据的情况。

- 序列模型

序列模型属于通用模型的一个子类，因为常见，故单独列出来。这种模型各层之间是依次顺序的线性关系，在第k层和第k+1层之间可以加上各种元素来构造神经网络。这些元素可以通过一个列表来制定，然后作为参数传递给序列模型来生成相应的模型。

```python
from keras.models import Sequential
from keras.layers import Dense, Activation

# 方法一：集中添加
layers = [Dense(32, input_shape=(784,)), Activation('relu'), Dense(10), Activation('softmax')]
model = Sequential(layers)
# 方法二：分层添加
model = Sequential()
model.add(Dense(32, input_shape=(784,)))
model.add(Activation('relu'))
model.add(Dense(10))
model.add(Activation('softmax'))

```

- 通用模型

通用模型可以用来设计非常复杂、任意拓扑结构 的神经网络，如有向无环网络、共享网络等。

类似于序列模型，通用模型通过函数化的应用接口来定义模型。

```python
from keras.layers import Input, Dense
from keras.models import Model
from keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# 定义输入层Input，主要是为了定义输入的多维矩阵的尺寸。
input = Input(shape=(784,))
# 定义各个连接层
x = Dense(64, activation='relu')(input)  # 隐含层，使用输入层作为参数
x = Dense(64, activation='relu')(x)  # 隐含层，使用上一层作为参数
# 定义输出层
y = Dense(10, activation='softmax')(x)
# 定义模型对象
model = Model(inputs=input, outputs=y)
# 编译，对数据进行拟合
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train)

```

## 重要对象

### 激活对象

在keras中使用激活对象的方法：一种是单独定义一个激活层，一种是在前置层里面通过激活选项来定义所需的激活函数

```python
# 方法一
model.add(Dense(32, input_shape=(784,)))
model.add(Activation('tanh'))
# 方法二
model.add(Dense(32, input_shape=(784,), activation='tanh'))
```

预定义的常用激活函数

| 函数名       | 描述                                                         |
| ------------ | ------------------------------------------------------------ |
| softmax      | 归一化的指数函数，将K维的实数域压缩到K维的值域(0,1)，数值总和为1 |
| softplus     | 将原始值从任意实数区间投影到正实数区间(0,inf)                |
| softsign     | 作用类似三角函数，将实数域上的数值投影到区间(-1,1)           |
| elu          | 参数小于0时，使用$\alpha(\exp(x)-1)$，大于等于0时，输出参数值，值域$(-\alpha, \inf)$ |
| relu         | 参数小于0时，取值为0，大于等于0时，输出参数值，值域$[0,\inf)$ |
| tanh         | 运用三角函数中的双曲正切函数将实数域压缩到区间(-1, 1)        |
| sigmoid      | 将实数域上的取值压缩到(0,1)区间的函数                        |
| hard_sigmoid | 标准激活函数的多段线性逼近形式，避免 $\exp()$ 函数计算，加快速度 |
| linear       | 线性激活函数不对参数做任何变换。当激活函数选项设置为None时，即选择此 |

### 初始化对象

初始化对象用于随机设定网络层激活函数中权重值或者偏置项的初始值，包括`kernel_initializer,bias_initializer`。好的权重初始值能帮助加快模型收敛速度。keras预先定义了很多不同的初始化对象，包括：

- Zeros

将所有参数值都初始化为0

- Ones

将所有参数值都初始化为1

- Constant(value=1)

将所有参数值都初始化为某一个常量，比如为1

- RandomNormal

将所有参数值都按照一个正态分布所生成的随机数来初始化，正态分布的均值默认为0，而标准差默认为0.05，可以通过mean和s tddev选项来修改。

- TruncatedNormal

使用一个截断正态分布生成的随机数来初始化参数向量，默认参数均值为0，标准差为0.05。对于均值的两个标准差之外的随机数会被遗弃并重新取样。这种初始化方法既有一定的多样性，又不会产生特别偏的值。因此是比较推荐的方法。

针对不同的常用分布选项，Keras还提供了两个基于这种方法的特例：`glorot_normal,he_normal`。前者的标准差不再是0.05，而是输入向量和输出向量的维度的函数$stddev=\sqrt{2/(n_1+n_2)}$，其中$n_1$是输入向量的维度，而$n_2$ 是输出向量的维度；后者的标准差只是输入向量的维度的函数 $stddev=\sqrt{2/n_1}$

- RandomUniform

按照均匀分布所生成的随机数来初始化参数值，默认的分布参数最小值为-0.05，最大值为0.05，可以通过minval和maxval选项分别修改。

针对常用的分布选项，keras还提供了两个基于这个分布的特例：`glorot_uniform,he_uniform`。前者均匀分布的上下限是输入向量和输出向量的维度的函数$minval/maxval=-/+\sqrt{6/(n_1+n_2)}$；而在后者上下限只是输入向量的维度的函数$minval/maxval=-/+\sqrt{6/n_1}$

- 自定义

用户可以自定义一个与参数维度相符合的初始化函数。

```python
from keras.layers import Dense
from keras import backend as K

def my_init(shape, dtype=None):
    return K.random_normal(shape, dtype=dtype)  # 使用后台的正态分布函数生成一组初始值

model.add(Dense(64, kernel_initializer=my_init))
```

### 正则化对象

在建模的时候，正则化湿防止过度拟合的一个很常用的手段。在神经网络中也提供了正则化的手段，分别应用于权重参数、偏置项以及激活函数，对应的选项分别是`kernel_regularizer,bias_regularizer,activity_regularizer`。它们都可以应用`keras.regularizers.Regularizer`对象，这个对象提供了定义好的一阶、二阶和混合的正则方法，分别将前面的`Regularizer`替换为`l1(x),l2(x),l1_l2(x1,x2)`，其中`x,x1,x2`为非负实数，表明正则化的权重。

也可以自定义实现针对权重矩阵的正则项，只要接受权重矩阵为参数，并且输出单个数值即可。

```python
def l1_reg(weight_matrix):
    return 0.01*K.sum(K.abs(weight_matrix))

model.add(Dense(64, input_dim=64, kernel_regularizer=l1_reg))
```

## 网络层构造

在keras中，定义神经网络的具体结构是通过组织不同的网络层（Layer）来实现的。

### 核心层

核心层（Core Layer）是构成神经网络最常用的网络层的集合，包括：全连接层、激活层、放弃层、扁平化层、重构层、排列层、向量反复层、Lambda层、激活值正则化层、掩盖层。所有的层都包含一个输入端和输出端，中间包含激活函数以及其他相关参数等。

- 全连接层

在这个层中实现对神经网络里面的神经元的激活。

```python
model.add(Dense(32, activation="relu", user_bias=True, kernel_initializer='uniform', activity_regularizer=regularizers.l1_l2(0.2, 0.5)))

# 32表示向下一层输出向量的维度
# activation表示使用relu函数作为对应神经元的激活函数
# kernel_initializer表示使用均匀分布来初始化权重向量
# activity_regularizer表示使用弹性网络作为正则项，其中一阶的正则化参数为0.2，二阶的正则化参数为0.5
```

- 激活层

激活层是对上一层的输出应用激活函数的网络层。使用方法有两种，若是整个网络的第一层，则需要用`input_shape`制定输入向量的维度。

```python
# 方法一
model.add(Dense(32, input_shape=(784,)))
model.add(Activation('tanh'))
# 方法二
model.add(Dense(32, input_shape=(784,), activation='tanh'))
```

- 放弃层

放弃层（Dropout）对该层的输入向量应用放弃策略。在模型训练更新参数的步骤中，网络的某些隐含层节点按照一定比例随机设置为不更新状态，但是权重仍然保留，从而防止过拟合。这个比例通过参数rate设定为0～1之间的实数。在模型训练时不更新这些节点的参数，因此这些节点并不属于当时的网络，但是保留其权重，因此在以后的迭代次序中可能会影响网络，在打分的过程中也会产生影响，所以这个放弃策略通过不同的参数估计值已经相对固化在模型中了。

- 扁平化层

扁平化层（Flatten）是将一个维度大于或等于3的高纬矩阵按照设定压扁为一个二维的低纬矩阵，其压缩方法是保留第一个维度的大小，然后将所有剩下的数据压缩到第二个维度中，因此第二个维度的大小是原矩阵第二个维度之后所有维度大小的乘积。这里第一个维度通常为每次迭代所需的小批量样本数量，而压缩后的第二个维度就是表达原图像所需的向量长度。

- 重构层

重构层（Reshape）功能和Numpy的Reshape方法一样，将一定维度的多维矩阵重排列构造为一个新的保持同样元素数量但是不同维度尺寸的矩阵。其参数为一个元组，制定输出向量的维度尺寸，最终的向量输出维度的第一个维度的尺寸是数据批量的大小，从第二个维度开始制定输出向量的维度大小。

```python
model = Sequential()
model.add(Reshape((4, 4), input_shape=(16,)))
# 将有16个元素的输入向量重构为一个(None, 4, 4)的新二维矩阵
```

- 排列层

排列层（Permute）按照给定的模式来排列输入向量的维度。这个方法在连接卷积网络和时间递归网络的时候非常有用。其参数是输入矩阵的维度编号在输出矩阵中的位置。

```python
model.add(Permute((1, 3, 2), input_shape=(10, 16, 8)))
# 将输入向量的第二维和第三维的数据进行交换后输出，但是第一维的数据还是待在第一维
```

- 向量反复层

向量反复层（RepeatVector）就是将输入矩阵重复多次。

```python
model.add(Dense(64, input_dim=(784,)))  # 输入是有784个元素的向量，输出是一个维度为(one, 64)的矩阵
model.add(RepeatVector(3))  # 将该矩阵反复3次，从而变成维度为(None, 3, 64)的多维矩阵，反复的次数构成第二个维度，第一个维度永远是数据批量的大小
```

- Lambda层

可以将任意表达式包装成一个网络层对象。参数就是表达式，一般是一个函数，可以是一个自定义函数，也可以是任意已有的函数。如果用Theano和自定义函数，可能还需要定义输出矩阵的维度，如果后台使用CNTK和Tensorflow，可以自动探测输出矩阵的维度。

```python
# 简单案例
model.add(Lambda x: numpy.sin(x))

# 复杂案例
def antirectifier(x):
    x = K.mean(x, axis=1, keepdims=True)
    x = K.l2_normalize(x, axis=1)
    pos = K.relu(x)
    neg = K.relu(-x)
    return K.concatenate([pos, neg], axis=1)


def antirectifier_output_shape(input_shape):
    shape = list(input_shape)
    assert len(shape) == 2
    shape[-1] *= 2
    return tuple(shape)


model.add(Lambda(antirectifier, output_shape=antirectifier_output_shape))
```



- 激活值正则化层

对输入的损失函数更新正则化

- 掩盖层

该网络层主要使用在跟时间相关的模型中，比如LSTM。其作用是输入张量的时间步，在给定位置使用指定的数值进行屏蔽，用以定位需要跳过的时间步。

输入张量的时间步一般是输入张量的第1维度（维度从0开始算），如果输入张量在该时间步上等于指定数值，则该时间步对应的数据将在模型下来的所有支持屏蔽的网络层被跳过，即被屏蔽。如果模型接下来的一些层不支持屏蔽，却接收到屏蔽过的数据，则抛出异常。

```python
model = Sequential()
model.add(Masking(mask_value=0, input_shape=(timesteps, features)))
model.add(LSTM(32))
# 如果输入张量X[batch, timestep, data]对应于timestep=5，7的数值是0，即X[:,[5,7],:]=0，那么上面的代码指定需要屏蔽的对象是所有数据为0的时间步，然后接下来的长短记忆网络在遇到时间步为5和7的0值数据时都会将其忽略掉
```

### 卷积层

针对常见的卷积操作，Keras提供了相应的卷积层API，包括一维、二维和三维的卷积操作、切割操作、补零操作等。

卷积操作分为一维、二维和三维，对应的方法分别是`Conv1D,Conv2D,Conv3D`，这些方法有相同的选项，只是作用于不同维度的数据上，因此适用于不同的业务情景。当作为首层使用时，需要提供输入数据维度的选项`input_shape`。

一维卷积通常被称为时域卷积，因为其主要应用在以时间排列的序列数据上，其使用卷积核对一维数据的临近信号及性能卷积操作来生成一个张量。二维卷积通常称为空域卷积，一般应用在域图像相关的输入数据上，也是使用卷积核对输入数据进行卷积操作的。三维卷积也执行同样的操作。

选项
```python
filters
# 卷积滤子输出的维度，要求整数
kernel_size
# 卷积核的空域或时域窗长度，要求是整数或整数的列表，或者元组，如果是单一整数，则应用于所有适用的维度。
strides
# 卷积在宽或着高维度的步长。要求是整数或整数的列表，或者元组。如果是单一整数，则应用于所有适用的维度。如果设定步长不为1，则dilation_rate选项的取值必须为1
padding
# 补齐策略，取值为`valid,same,causal`。`causal`将产生因果（膨胀的）卷积，即`output[t]`不依赖于`input[t+1:]`，在不能违反时间顺序的时序信号建模时有用。`valid`代表只进行有效的卷积，即对边界数据不处理。`same`代表保留边界处的卷积结果，通常会导致输出`shape`与输入`shape`相同。
data_format
# 数据格式，取值为`channels_last,channels_first`。这个选项决定了数据维度次序，其中`channels_last`对应的数据维度次序是（批量数，高，宽，频道数），而`channels_first` 对应的数据维度次序为（批量数，频道数，高，宽）
activation
# 激活函数，为预定义或者自定义的激活函数名。若不指定，则不会使用任何激活函数（即使用线性激活函数$g(x)=x$）
dilation_rate
# 指定扩展卷积中的扩张比例。要求为整数或由单个整数构成的列表/元组，若不为1，则步长一项必须设为1。
user_bias
# 指定是痘使用偏置项，取值为True或False。
kernel_initializer
# 权重初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的函数。
bias_initializer
# 偏置初始化方法，为预定义初始化放大的字符串，或用于初始化偏置的函数。
kernel_regularizer
# 施加在权重上的正则项
bias_regularizer
# 施加在偏置项上的正则项
activity_regularizer
# 施加在输出上的正则项
kernel_constraints
# 施加在权重上的约束项
bias_constraints
# 施加在偏置项上的约束项
```
### 池化层

池化是在卷积神经网络中对图像特征的一种处理，通常在卷积操作之后进行。池化的目的是为了计算特征在局部的充分统计量，从而降低总体的特征数量，防止过度拟合和减少计算量。

Keras的池化层按照计算的统计量分为最大统计量池化和平均统计量池化；按照维度分为一维、二维和三维池化层；按照计量计算区域分为局部池化和全局池化。

- 最大统计量池化

```python
MaxPooling1D
# 对一维的时域数据计算最大统计量的池化函数，输入数据的格式要求为(批量数，时间步，各个维度的特征值),输出数据为三维张量(批量数，下采样后的时间步数，各个维度的特征值)
MaxPooling2D
# 对二维的图像数据计算最大统计量的池化函数，输入输出数据均为四维张量，具体格式根据data_format选项要求分别为：data_format="channels_first"时，输入数据(样本数、频道数、行、列),输出数据(样本数、频道数、池化后行数、池化后列数)；data_format="channels_last"时，输入数据(样本数、行、列、频道数),输出数据(样本数、池化后行数、池化后列数、频道数)
MaxPooling3D
# 对三维的时空数据计算最大统计量的池化函数，输入输出数据均为五维张量，具体格式根据data_format选项要求分别为：data_format="channels_first"时，输入数据(样本数、频道数、一维长度、二维长度、三维长度),输出数据(样本数、频道数、池化后一维长度、池化后二维长度、池化后三维长度)；data_format="channels_last"时，输入数据(样本数、一维长度、二维长度、三维长度、频道数),输出数据(样本数、池化后一维长度、池化后二维长度、池化后三维长度、频道数)
```

- 平均统计量池化

数据格式与上面相同，池化放大使用局部平均值而不是局部最大值作为填充统计量。方法名分别为`AveragePooling1D,AveragePooling2D,AveragePooling3D`。

- 全局池化方法

该方法应用全部特征维度的统计量来代表特征，因此会压缩数据维度。在局部池化方法中，输出维度和输入维度是一样的，只是特征的维度尺寸因为池化变小，但是在全局池化方法中，输出维度小于输入维度。全局池化也分为最大统计量池化和平均统计量池化以及一维和二维池化方法。

```python
GlobalMaxPooling1D/GlobalAveragePooling1D
# 一维池化，输入数据格式(批量数、步进数、特征值)，输出数据格式(批量数、频道数)
GlobalMaxPooling2D/GlobalAveragePooling2D
# 二维池化，当data_format="channels_first"时输入数据(批量数、行、列、频道数)，data_format="channels_last"时，输入数据(批量数、频道数、行、列),输出数据(批量数、频道数)
```

### 循环层

循环层（Recurrent Layer）用来构造跟序列有关的神经网络。但是其本身是一个抽象类，无法实例化对象，在使用时应该使用LSTM/GRU/SimpleRNN三个子类来构造网络层。

选项

```python
units
# 输出向量的大小，为整数
activation
# 激活函数，为预定义或者自定义的激活函数名。若不指定，则不会使用任何激活函数（即使用线性激活函数$g(x)=x$）
user_bias
# 指定是痘使用偏置项，取值为True或False。
kernel_initializer
# 权重初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的函数。
recurrent_initializer
# 循环层状态节点权重初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的函数。
bias_initializer
# 偏置初始化方法，为预定义初始化放大的字符串，或用于初始化偏置的函数。
kernel_regularizer
# 施加在权重上的正则项
recurrent_regularizer
# 施加在循环层状态节点权重上的正则项
bias_regularizer
# 施加在偏置项上的正则项
activity_regularizer
# 施加在输出上的正则项
kernel_constraints
# 施加在权重上的约束项
recurrent_constraints
# 是假在循环状态节点权重上的约束项
bias_constraints
# 施加在偏置项上的约束项
dropout
# 指定输入节点的放弃率，为0～1之间的实数
recurrent_dropout
# 指定循环层状态节点的放弃率，为0～1之间的实数
recurrent_activation
# LSTM/GRU中包含的控制循环步所使用的激活函数
```

### 嵌入式层

嵌入层（Embedding Layer）是使用在模型第一层的一个网络层，其目的是将所有索引标号映射到致密的低纬向量中，通常用在对文本数据进行建模的时候。输入数据要求是一个二维张量(批量数，序列长度)，输出数据为一个三维张量(批量数，序列长度，致密向量的维度)

选项

```python
output_dim
# 输出维度，这是需要映射到致密的低纬向量中的维度，为大于或等于0的的整数
embedding_initializer
# 嵌入矩阵的初始化方法
embedding_regularizer
# 嵌入矩阵的正则化方法
embedding_constraint
# 嵌入层的约束方法
mask_zero
# 是否屏蔽0值。通常输入值里的0是通过补齐策略对不同长度输入补齐的结构，如果为0，则需要将其屏蔽。如果输入张量在该时间步上都等于0，则该时间步对应的数据将在模型接下来的所有支持屏蔽的网络层被跳过，即被屏蔽，如果模型接下来的一些层不支持屏蔽，却接收到屏蔽过的水，则抛出异常，如果设定了屏蔽0值，则词典不能从0开始做索引标号，因为这时0值已经具有特殊含义了
input_length
# 输入徐磊长度。当需要连接扁平化和全连接层时，需要指定该选项，否则无法计算全连接层输出的维度
```

### 合并层

合并层是指将多个网络产生的张量通过一定方法合并在一起。合并层支持不同的合并方法，包括

```python
merge.Add
# 元素相加，要求合并的张量的维度大小完全一致。
merge.Multiply
# 元素相乘，要求合并的张量的维度大小完全一致。
merge.Average
# 元素取平均，要求合并的张量的维度大小完全一致。
merge.Maximum
# 元素取最大，要求合并的张量的维度大小完全一致。
merge.Concatenate
# 叠加，要求指定按照哪个维度(axis)进行叠加，除了叠加的维度，其他维度的大小必须一致
merge.Dot
# 矩阵相乘，需要指定沿着哪个维度(axis)进行乘法操作。同时可以指定是否标准化，若是是的话，则先将两个张量归一化以后再相乘，这时得到的是余弦的相似度
```



## 奇异值矩阵分解

奇异值分解是一种基本的数学工具，被应用于大量的数据挖掘算法中，比较有名的有协同过滤、PCA回归等算法。矩阵分解的目的是解析矩阵的结构，提取重要信息，去除噪声，实现数据压缩等。比如奇异值矩阵分解中，信息都集中在头几个特征向量中，使用这几个向量有可能较好地（均方差尽可能小地）复原原来的矩阵，同时只需要保留较少的数据。
$$
X_{m\times n} = U_{m\times r}S_{r \times r}V_{r \times n}^T
$$
在keras来操作矩阵分解时使用嵌入工具和合并工具。嵌入工具能够将一组正整数（比如序列的索引）转换为固定维度的致密实数，而合并工具能够按照不同的方法，比如求和、叠加或者乘积方式将两个网络合并在一起。

```python
user_in = Input(shape=(1,), dtype='int64', name='user_in')
u = Embedding(n_users, n_factors, input_length=1)(user_in)
movie_in = Input(shape=(1,), dtype='int64', name='movie_in')
v = Embedding(n_movies, n_factors, input_length=1)(movie_in)

x = merge([u, v], mode="dot")
x = Flatten()(x)
model = Model([user_in, movie_in], x)
model.compile(Adam(0.001), loss="mse")
model.fit([trn.userId, trn,rating, batch_size=64, epochs=1])
```



