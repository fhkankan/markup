# 情感分析

全连接网络

```python
import keras
import numpy as np
from keras.datasets import imdb
import matplotlib.pyplot as plt

# 探索数据
(X_train, y_train), (X_test, y_test) = imdb.load_data()
np.reshape(X_train[0], (1, -1))
print(X_train[0], X_train.shape, y_train.shape, sep="\n")  # (25000,)
# 查看平均每个评论的字数
avg_len = list(map(len, X_train))
print(np.mean(avg_len))  # 239
# 图示词频分布的直方图
plt.hist(avg_len, bins=range(min(avg_len), max(avg_len) + 50, 50))
plt.show()
"""
多层全连接神经网络训练情感分析
不同于已经训练好的词向量，keras提供了设计嵌入层的模板。只要在建模的时候加一行EmbeddingLayer函数的代码即可
注意：嵌入层一般是需要通过数据学习的，也可以借助已经训练好的Word2Vec中预训练好的词向量直接放入模型，
或者把预训练好的词向量作为嵌入层初始值，进行再训练
"""
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence

(X_train, y_train), (X_test, y_test) = imdb.load_data()
# 计算最长的文本长度
m = max(list(map(len, X_train)) + list(map(len, X_test)))
print(m)
# 最长文本为2494个字符,而平均长度为239，属于异常值。此处设定为最长400个字符，不足的使用空格填充，超过的截取400个字符，默认截取后400个
maxword = 400
X_train = sequence.pad_sequences(X_train, maxlen=maxword)
X_test = sequence.pad_sequences(X_test, maxlen=maxword)
vocab_size = np.max([np.max(X_train[i]) for i in range(X_train.shape[0])]) + 1  # 1代表空格，其索引被认为是0

# 建立序列模型
model = Sequential()
# 嵌入层，矩阵为vocab_size*64。每个训练段落为其中的maxword*64矩阵，作为数据的输入，填入输入层
model.add(Embedding(vocab_size, 64, input_length=maxword))
# 把输入层压平，矩阵维度(maxword,64)->(1,maxword*64)
model.add(Flatten())
# 搭建全连接网络
model.add(Dense(500, activation='relu'))
model.add(Dense(500, activation='relu'))
model.add(Dense(200, activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(1, activation='sigmoid'))  # 计算输出的是0或1的概率

# 损失函数为交叉熵，优化方法采用adam，评估指标为准确度
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=100*4, verbose=1)
# 计算准确率
score = model.evaluate(X_test, y_test)
print(score[1])  # 85.2%
```

卷积神经网络

```python
"""
卷积神经网络训练情感分析
全连接神经网络几乎对网络模型没有任何限制，但缺点是过度拟合。特点是灵活、参数多。在实际使用中会对模型增加一些限制，使其适合数据的特点，
相应地减少了参数，降低了模型的复杂度，使其普适性增高。
卷积在自然语言中的作用在于利用文字的局部特征，一个词的前后几个词必然和这个词本身相关，这组成该词所代表的词群。
词群进而会对段落文字的意思进行影响，决定这个段落到底是正向还是负向。对比传统方法，利用Bag of Words和TF-IDF等，其思想有相通之处。
但最大的区别在于，传统方法是人为构造用于分类的特征，而深度学习中的卷积让神经网络去构造特征
"""
from keras.models import Sequential
from keras.layers.embeddings import Embedding
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv1D, MaxPooling1D

model = Sequential()
model.add(Embedding(vocab_size, 64, input_length=maxword))
model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
"""
Dropout技巧：在每个批量训练过程中，对每个节点，不论是在输入层还是在隐藏层，都有独立的概率让节点变成0。这样的好处在于，每次批量训练都相当于
在不同的小神经网络中进行计算，当训练数据大的时候，每个节点的权重都会被调整过多次。另外，在每次训练的时候，系统会努力在有限的节点和小神经网络
中找到最佳的权重，这样可以最大化地找到重要特征，避免过拟合。
"""
model.add(Dropout(0.25))
model.add(Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
print(model.summary())
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=100*4)
scores = model.evaluate(X_test, y_test, verbose=1)
print(scores[1])  # 87.2%
```

循环神经网络

```python
"""
循环神经网络训练情感分析
LSTM
"""
from keras.layers import LSTM

model = Sequential()
model.add(Embedding(vocab_size, 64, input_length=maxword))
model.add(LSTM(128, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(64, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(32))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
print(model.summary())
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=100*4)
scores = model.evaluate(X_test, y_test)
print(scores[1])  # 86.8%

```

