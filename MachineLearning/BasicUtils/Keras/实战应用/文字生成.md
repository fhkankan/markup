# 文字生成

基于深度学习的检索式对话系统

```python
import pandas as pd
import numpy as np
import pickle

import keras
from keras.models import Sequential, Model
from keras.layers import Input, Dense, Activation, Dropout, Embedding, Reshape, Dot, Concatenate, Multiply
from keras.layers import LSTM
from keras.optimizers import RMSprop
from keras.utils.data_utils import get_file
from keras.preprocessing.sequence import pad_sequences
from keras.models import model_from_json

import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import jieba

"""
使用双编码长短记忆(Dual Encoder LSTM)模型
"""

plt.rcParams['figure.figsize'] = (20, 10)

# 读入数据
with open("./data/dataset_1MM/dataset.pkl", "rb") as f:
    data = pickle.load(f)
print(len(data))

# 探索数据
# for j in range(len(data)):
#     print("======= %s" % j)
#     for i, k in enumerate(data[j]):
#         print(k)

# 这里分析最长的句子的长度
context_list = list(map(len, data[0]['c']))
response_list = list(map(len, data[0]['r']))
context_length = np.max(context_list)
response_length = np.max(response_list)
mean_length = np.mean(context_list)
median_length = np.median(context_list)
print('句子的长度：问的长度{},答的长度{}，问的平均值为{}, 问的中位数为{}'.
      format(context_length, response_length, mean_length, median_length))
Y = data[0]['r']

# 这里分析整个词典的大小
context_size = np.max(list(map(lambda x: max(x) if len(x) > 0 else 0, data[0]['c'])))
response_size = max(list(map(lambda x: max(x) if len(x) > 0 else 0, data[0]['r'])))
volcabulary_size = max(context_size, response_size)
print('字典的大小：问的最大值为{},答的最大值{},最大为{}'.format(context_size, response_size, volcabulary_size))

print('Begin Modeling...')

context_length = 120  # 选择中位数作为补齐长度
response_length = 120
embedding_dim = 64
lstm_dim = 64

# 对上下文部分进行嵌入和建模
context = Input(shape=((context_length,)), dtype='int32', name='context_input')
# 嵌入操作，将高维的索引标号数据投影到一个固定低纬度的实数向量中。由于长短不一，设置长度标准化补齐操作
context_embedded = Embedding(input_length=context_length, output_dim=embedding_dim, input_dim=volcabulary_size)(context)
context_lstm = LSTM(lstm_dim)(context_embedded)

# 对回应部分进行嵌入和建模
response = Input(shape=((response_length,)), dtype='int32', name='response_input')
response_embedded = Embedding(input_length=response_length, output_dim=embedding_dim,
                              input_dim=volcabulary_size)(response)
response_lstm = LSTM(lstm_dim)(response_embedded)

# 将两个不同的LSTM网络通过运用张量点乘的方法合并，在骑上训练一个全连接网络
x = Dot([1, 1])([context_lstm, response_lstm])
# x = Multiply()([context_lstm, response_lstm])
yhat = Dense(2, activation='softmax')(x)
model = Model(inputs=[context, response], outputs=yhat)
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
print('Finish compiling...')
model.summary()

"""
针对一组批量的输入索引下标向量列表进行补齐，否则一次性对所有样本数据进行操作内存消耗太大。在这个data_gen中，先从原始数据里抽取一组指定批量数大小的样本，这组样本是下标识向量列表，而这些下标识对应的就是不定长的句子里面每个字符的索引号。
对于这组列表，按照指定长度进行向量嵌入有两种：一种是指定长度大于等于最长句子的长度，这样每段对话都可以用一个向量表达，能够对所有列表中的向量一次进行补齐；第二种是指定长度短于最长句子的长度，比如用中位值作为指定长度，这时对于不够指定长度的较短向量可以直接补齐，而对于超过指定长度的向量，则按照指定的长度建立移动窗口，建立不同时间步对应的对话上下文向量列表，最后对补齐的输出进行映射。不论哪种方法，都需要生成三维张量，中间维度是时间步，对于第1种情况，时间步长度永远为1，对于第2种情况，时间步长度则是可变的
"""
# 针对该数据定制的generator。一般应该将三个部分分离以后再编制generator
def data_gen(data, batch_size=100):
    contextRaw = data['c']
    responseRaw = data['r']
    yRaw = data['y']

    number_of_batches = len(contextRaw) // batch_size
    counter = 0

    context_length = np.max(list(map(len, contextRaw))) // 3
    response_length = np.max(list(map(len, responseRaw))) // 3

    context_length = 120
    response_length = 120

    while 1:
        lowerBound = batch_size * counter
        upperBound = batch_size * (counter + 1)
        Ctemp = contextRaw[lowerBound: upperBound]
        C_batch = pad_sequences(Ctemp, maxlen=context_length, padding='post')
        C_res = np.zeros((batch_size, context_length), dtype=np.int)

        Rtemp = responseRaw[lowerBound: upperBound]
        R_batch = pad_sequences(Rtemp, maxlen=response_length, padding='post')
        R_res = np.zeros((batch_size, response_length), dtype=np.int)
        for k in np.arange(batch_size):
            C_res[k, :] = C_batch[k, :]
            R_res[k, :] = R_batch[k, :]
        y_res = keras.utils.to_categorical(yRaw[lowerBound: upperBound])
        counter += 1
        yield ([C_res.astype('float32'), R_res.astype('float32')], y_res.astype('float32'))
        if (counter < number_of_batches):
            counter = 0


# 下面训练这个模型。在6GB显存的GTX 1060上，小批量的大小不能超过200。读者有时间可以试试多次迭代，看看效果。
# Y = keras.utils.to_categorical(data[0]['y'], num_classes=2)
batch_size = 168
model.fit_generator(data_gen(data[0], batch_size=batch_size),
                    steps_per_epoch=len(data[0]['c']) // batch_size,
                    validation_data=data_gen(data[1]),
                    validation_steps=len(data[1]['c']) // batch_size,
                    epochs=1)
# 下面我们将模型存入磁盘。我们也可以在拟合过程中使用checkponit选项将每一步的结果都分别存入一个磁盘文件中。
# 将模型结构存为JSON格式
model_json = model.to_json()
with open("./data/dual_lstm_model.json", "w") as json_file:
    json_file.write(model_json)
# 将模型拟合得到的权重存入HDF5文件中
model.save_weights("./data/dual_lstm_model.h5")
print("模型已经写入磁盘")

# 如果要调用已有模型，可以通过如下方法

# 从磁盘载入模型结构
json_file = open('./data/dual_lstm_model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
model = model_from_json(loaded_model_json)
# 从磁盘读入模型权重
model.load_weights("./data/dual_lstm_model.h5")
print("载入模型完毕")
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
print('模型编译完毕...')

# 下面进行预测。输入数据的组织形式应该遵循data_generator里面的数据处理和输出组织形式，但是我们可以通过predict_generator方法
# 直接引用现有的data generator，只是用在测试集，而不是训练集上。
batch_size = 256
ypred = model.predict_generator(data_gen(data[2], batch_size=batch_size), steps=(len(data[2]['c']) // batch_size),
                                verbose=1)
yTest = data[1]['y']

ypred2 = (2 - (ypred[:, 0] > ypred[:, 1])) - 1
z = [str(ypred2[i]) == yTest[i] for i in range(len(ypred2))]
print(np.mean(z))

```

基于文字生成的对话系统

```python
import pandas as pd
import numpy as np
import gc

import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.layers import LSTM
from keras.optimizers import RMSprop
from keras.utils.data_utils import get_file

import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import jieba

"""
我们使用《四世同堂》这部小说作为训练集。读者也可以选用其他长篇小说，或者爬取网上新闻作为训练集。通常句式和语言比较有自己风格的
长篇小说训练起来相对容易产出好的结果，就像我们读了武侠小说就比较容易学那种写法一个道理。因此读者也不妨选用名家的武侠小说，比如
金庸全集等来训练自己的模型。网上爬取的新闻则具有数据量大，风格一致的特点，也适合用来训练模型。

对于中文环境下，中文的粉刺有时候是一个问题，很多具体的业务需要立自己的分词效果，对于一些新出现饿词组不能有效进行分割，但是这些新的词组
往往是业务发展的体现，因此，即使是较少的错误分词仍然可能造成大的问题。因此为了简化建模流程，在没有专门分词库的情况下，选择对单个中文字符
及相关符号进行建模，这样就跳过了建立自己的分词库或者使用通用分词库但是分词效果不一定好的情况。

对文字序列建模的顺序：
1.首先对所有待建模的单字和字符进行索引
2.其次构造句子序列
3.然后建立神经网络模型，对索引标号序列进行向量嵌入后的向量构造长短记忆神经网络
4.最后检验建模效果
"""

plt.rcParams['figure.figsize'] = (20, 10)
np.random.seed(82832)

# 不符合下面固定句长设定的程序要求
# 但是可用于计算平均句长
fileopen = open("./data/四世同堂.txt", encoding='utf-8')
with fileopen as fo:
    alltext0 = fo.readlines()

# 一次性读入，便于以每个单字作为建模对象
alltext = open("./data/四世同堂.txt", encoding='utf-8').read()
print(len(set(alltext)))

# 我们先按照单个字来建模。首先把所有的字符抽取出来。
'''
较naive的做法
charset = {}
id = 0
for line in alltext:
    length = len(line)
    for k in range(length):
        w = line[k]
        if not w in charset:            
            charset[w]=id
            id+=1

print(len(charset))
'''
sortedcharset = sorted(set(alltext))  # 对用set抽取的每个单字的集合按照编码从小到大排序
char_indices = dict((c, i) for i, c in enumerate(sortedcharset))  # 对每一个单字进行编号索引
indices_char = dict((i, c) for i, c in enumerate(sortedcharset))  # 反向操作，对每个索引建立单字的词典，主要是为了方便预测出来的索引标号向量转换为人能够阅读的文字

# 现在把原文按照指定长度划分为虚拟的句子。这个指定虚拟句子的长度一般使用平均句子的字数。
sentencelength = 0
k = 0
for line in alltext0:
    k = k + 1
    linelength = len(line)
    sentencelength = (k - 1) / k * sentencelength + linelength / k
print(sentencelength)
print(k)

# 构造句子序列
"""
构造句子序列的原因是原始数据是单字列表，因此需要人为构造句子的序列来模仿句子序列
注意：因为句子是人工构造的，都有固定长度，因此这里不需要进行句子补齐操作，同时，这些句子的向量其实都是一个稀疏矩阵，
因为它们只将包含数据的索引编号计入
"""
maxlen = 40  # 标识人工构造的句子长度
# 跳字的目的是为了增加句子与句子之间的变化，否则每两个相邻句子之间只有一个单字的差异，但是这两个相邻句子是 用来构造前后对话序列的，
# 缺乏变化使得建模效果不好。当然，跳字太多，那么会大大降低数据量。跳字的个数是在建模的时候要根据情况调整的一个参数
step = 3  # 表示在构造句子时每次跳过3个单字，
sentences = []
next_chars = []
for i in range(0, len(alltext) - maxlen, step):
    sentences.append(alltext[i: i + maxlen])
    next_chars.append(alltext[i + maxlen])
print('nb sequences:', len(sentences))

"""
人工构造句子完毕后就可以对其矩阵化，即对每一句话，将其中的索引标号映射到所有出现的单字和符号，每一句话所对应的40个字符的向量被投影到一个
3545个元素的向量中，在这个向量中，如果某个元素出现在这句话中，则其值为1，否则为0
"""
# 下面对虚拟句子进行矩阵化
print('Vectorization...')
X = np.zeros((len(sentences), maxlen, len(sortedcharset)), dtype=np.bool)
y = np.zeros((len(sentences), len(sortedcharset)), dtype=np.bool)
print('Finished initialization...')
for i, sentence in enumerate(sentences):
    if (i % 30000 == 0):
        print(i)
    for t in range(maxlen):
        char = sentence[t]
        X[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1

print(X.shape)

"""
但是这么直接构造得是非常浪费空间的密集矩阵，这个矩阵占据大约30GB的内存，如果把句长再增加一些，那么在很多机器上无法运行。
同时这么大的数据无法送给显卡进行计算，需要每次取一小块批量供GPU计算所需。
这时候需要使用fit_generator方法，对一个具有较小批量数的样本进行投影操作，而不是原来的fit方法。
fit_generator将每个batch的数据读入，从原始数据的稀疏矩阵变为当前批量的密集矩阵，然后计算。这样对内存的压力大大降低。
"""


# data generator for fit_generator method
def data_generator(X, y, batch_size):
    """
    同时处理X和y矩阵的小批量生成，要求输入和和输出数据都是Numpy多维矩阵而不是列表的列表
    :param X: x矩阵
    :param y: y矩阵
    :param batch_size: 批量数
    :return:
    """
    if batch_size < 1:
        batch_size = 256
    number_of_batches = X.shape[0] // batch_size
    counter = 0
    shuffle_index = np.arange(np.shape(y)[0])
    np.random.shuffle(shuffle_index)
    # reset generator
    while 1:
        index_batch = shuffle_index[batch_size * counter:batch_size * (counter + 1)]
        X_batch = (X[index_batch, :, :]).astype('float32')
        y_batch = (y[index_batch, :]).astype('float32')
        counter += 1
        yield (np.array(X_batch), y_batch)
        if (counter < number_of_batches):
            np.random.shuffle(shuffle_index)
            counter = 0


# char=subsentences[j][1]
char = sentences[1][1]
print(char_indices[char])
print(next_chars[1])

# 建立模型: a single LSTM
batch_size = 300
print('Build model...')
model = Sequential()
# 长短记忆网络，输入数据的维度为(时间步数，所有出现的不重复字符的个数)，即输入的数据是对应每一句话处理以后的形式，并且对输入神经元权重
# 和隐藏状态权重分别设定了10%的放弃率。
model.add(LSTM(256, batch_size=batch_size, input_shape=(maxlen, len(sortedcharset)),
               recurrent_dropout=0.1, dropout=0.1))
# model.add(Dense(1024, activation='relu'))
# model.add(Dropout(0.25))
# 全连接层的输出维度为所有自渎的个数，放便最后的激活函数计算
model.add(Dense(len(sortedcharset)))
model.add(Activation('softmax'))

# 指定网络优化算法的参数
# optimizer = RMSprop(lr=0.01)
adamoptimizer = keras.optimizers.Adam(lr=1e-4)
model.compile(loss='categorical_crossentropy', optimizer=adamoptimizer)
print('Finished compiling')
model.summary()
# 训练模型，这里使用fit_generator,将每个批量的数据读入，从稀疏矩阵变为密集矩阵，然后计算，降低了内存的压力
model.fit_generator(data_generator(X, y, batch_size=batch_size), steps_per_epoch=X.shape[0] // batch_size, epochs=25)


def sample(preds, temperature=1.0):
    """
    用于从预测结果的到新生成的文字。这是因为这个模型返回的是对于每个字符在下一句里的出现频率，而这个函数就是负责根据这个得到的概率对所有
    索引标号进行依概率的随机取样。
    :param preds: 预测结果
    :param temperature: 温度，控制概率差异的扩大或者缩小，当=1时，对预测概率没有影响，当<1时，预测概率的差异被扩大，
                        有利于增加生成语句的多样性；当>1时，预测概率的差异被缩小，会缩小生成语句的多样性，即在很多时候生成的语句都非常
                        类似，会有很多单字不断地重复出现
    :return: 
    """
    # helper function to sample an index from a probability array
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)

# 随机抽取一组40个连续的字符，然后生成对应的投影到所有字符空间的自变量x
start_index = 2799
sentence = alltext[start_index: start_index + maxlen]
sentence0 = sentence
x = np.zeros((1, maxlen, len(sortedcharset)))

generated = ''
x = np.zeros((1, maxlen, len(sortedcharset))).astype('float32')
for t, char in enumerate(sentence):
    x[0, t, char_indices[char]] = 1.
# 依次对每一句的下20个字符进行预测，并根据预测得到的索引标号找出对应的文字供人阅读
for i in range(20):
    preds = model.predict(x, verbose=0)[0]
    next_index = sample(preds, 0.9)
    next_char = indices_char[next_index]
    generated += next_char
    sentence = sentence[1:] + next_char

print(sentence0)
print("=================")
print(' '.join(generated))

# del(X, y)
# del(model)

for i in range(19):
    gc.collect()

batch_size = 10240
number_of_batches = len(sentences) // batch_size
counter = 0
shuffle_index = np.arange(len(sentences))
np.random.shuffle(shuffle_index)

# reset generator

for i in range(number_of_batches):
    index_batch = shuffle_index[batch_size * counter:batch_size * (counter + 1)]
    subsentences = [sentences[s] for s in index_batch]
    X = np.zeros((batch_size, maxlen, len(sortedcharset)), dtype=np.bool)
    y = np.zeros((batch_size, len(sortedcharset)), dtype=np.bool)
    for j in range(len(subsentences)):
        for t in range(maxlen):
            char = subsentences[j][t]
            X[j, t, char_indices[char]] = 1
        y[j, char_indices[next_chars[j]]] = 1
    X = X.astype('float32')
    y = y.astype('float32')
    counter += 1
    print((X.shape, y.shape))

# 但是这种方法仍然需要一开始生成巨大的特征矩阵和因变量矩阵。我们可以将生成这两个矩阵的操作移入数据生成器中，
# 这样无需产生大量数据等待输入GPU，而是每次只取所需并生成相应的矩阵并即刻输入GPU运算即可。
# build the model: a single LSTM
batch_size = 300
print('Build model...')
model = Sequential()
model.add(
    LSTM(256, batch_size=batch_size, input_shape=(maxlen, len(sortedcharset)), recurrent_dropout=0.1, dropout=0.1))
# model.add(Dense(1024, activation='relu'))
# model.add(Dropout(0.25))
model.add(Dense(len(sortedcharset)))
model.add(Activation('softmax'))

# optimizer = RMSprop(lr=0.01)
adamoptimizer = keras.optimizers.Adam(lr=1e-4)
model.compile(loss='categorical_crossentropy', optimizer=adamoptimizer)
print('Finished compiling')
model.summary()


def data_generator2(sentences, sortedcharset, char_indices, maxlen=40, batch_size=256):
    if batch_size < 1:
        batch_size = 256
    number_of_batches = len(sentences) // batch_size
    counter = 0
    shuffle_index = np.arange(len(sentences))
    np.random.shuffle(shuffle_index)
    # reset generator
    while 1:
        index_batch = shuffle_index[batch_size * counter:batch_size * (counter + 1)]
        subsentences = [sentences[s] for s in index_batch]
        X = np.zeros((batch_size, maxlen, len(sortedcharset)), dtype=np.bool)
        y = np.zeros((batch_size, len(sortedcharset)), dtype=np.bool)
        for j, sentence in enumerate(subsentences):
            for t in range(maxlen):
                char = sentence[t]
                X[j, t, char_indices[char]] = 1
            y[j, char_indices[next_chars[j]]] = 1
        X = X.astype('float32')
        y = y.astype('float32')
        counter += 1
        yield ((np.array(X), np.array(y)))
        if (counter < number_of_batches):
            np.random.shuffle(shuffle_index)
            counter = 0


model.fit_generator(data_generator2(sentences, sortedcharset, char_indices, maxlen=maxlen, batch_size=batch_size),
                    steps_per_epoch=len(sentences) // batch_size,
                    epochs=25)


def sample(preds, temperature=1.0):
    # helper function to sample an index from a probability array
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)


start_index = 2799
sentence = alltext[start_index: start_index + maxlen]
sentence0 = sentence
x = np.zeros((1, maxlen, len(sortedcharset)))

generated = ''
x = np.zeros((1, maxlen, len(sortedcharset))).astype('float32')
for t, char in enumerate(sentence):
    x[0, t, char_indices[char]] = 1.
for i in range(20):
    preds = model.predict(x, verbose=0)[0]
    next_index = sample(preds, 1.1)
    next_char = indices_char[next_index]
    generated += next_char
    sentence = sentence[1:] + next_char

print(sentence0)
print("=================")
print(' '.join(generated))

start_index = 2799
sentence = alltext[start_index: start_index + maxlen]
sentence0 = sentence
x = np.zeros((1, maxlen, len(sortedcharset)))


def GenSentence(original):
    sentence = original
    generated = ''
    for i in range(20):
        x = np.zeros((1, maxlen, len(sortedcharset))).astype('float32')
        for t, char in enumerate(sentence):
            x[0, t, char_indices[char]] = 1.
        preds = model.predict(x, verbose=0)[0]
        next_index = sample(preds, 1.20)
        next_char = indices_char[next_index]
        generated += next_char
        sentence = sentence[1:] + next_char
    return (generated)


start_index = 3041
sentence0 = alltext[start_index: start_index + maxlen]
generated0 = GenSentence(sentence0)
print(sentence0 + "----->" + generated0)
print("==========")
generated1 = GenSentence(generated0)
print(generated0 + "------>" + generated1)

try:
    del (X, y, model)
except:
    print('Objects not found...')

for i in range(10):
    gc.collect()

```

