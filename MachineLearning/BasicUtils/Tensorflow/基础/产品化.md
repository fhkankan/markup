# 产品化

## 单元测试

在MNIST数据集上创建一个简单的CNN网络，并实现三种不同的单元测试

```python
# -*- coding: utf-8 -*-
# Implementing Unit Tests
#----------------------------------
#
# Here, we will show how to implement different unit tests
#  on the MNIST example

import sys
import numpy as np
import tensorflow as tf
from tensorflow.python.framework import ops
ops.reset_default_graph()

# Start a graph session
sess = tf.Session()

# Load data
data_dir = 'temp'
mnist = tf.keras.datasets.mnist
(train_xdata, train_labels), (test_xdata, test_labels) = mnist.load_data()
train_xdata = train_xdata / 255.0
test_xdata = test_xdata / 255.0

# Set model parameters
batch_size = 100
learning_rate = 0.005
evaluation_size = 100
image_width = train_xdata[0].shape[0]
image_height = train_xdata[0].shape[1]
target_size = max(train_labels) + 1
num_channels = 1 # greyscale = 1 channel
generations = 100
eval_every = 5
conv1_features = 25
conv2_features = 50
max_pool_size1 = 2 # NxN window for 1st max pool layer
max_pool_size2 = 2 # NxN window for 2nd max pool layer
fully_connected_size1 = 100
dropout_prob = 0.75

# Declare model placeholders
x_input_shape = (batch_size, image_width, image_height, num_channels)
x_input = tf.placeholder(tf.float32, shape=x_input_shape)
y_target = tf.placeholder(tf.int32, shape=(batch_size))
eval_input_shape = (evaluation_size, image_width, image_height, num_channels)
eval_input = tf.placeholder(tf.float32, shape=eval_input_shape)
eval_target = tf.placeholder(tf.int32, shape=(evaluation_size))

# Dropout placeholder
dropout = tf.placeholder(tf.float32, shape=())

# Declare model parameters
conv1_weight = tf.Variable(tf.truncated_normal([4, 4, num_channels, conv1_features],
                                               stddev=0.1, dtype=tf.float32))
conv1_bias = tf.Variable(tf.zeros([conv1_features], dtype=tf.float32))

conv2_weight = tf.Variable(tf.truncated_normal([4, 4, conv1_features, conv2_features],
                                               stddev=0.1, dtype=tf.float32))
conv2_bias = tf.Variable(tf.zeros([conv2_features], dtype=tf.float32))

# fully connected variables
resulting_width = image_width // (max_pool_size1 * max_pool_size2)
resulting_height = image_height // (max_pool_size1 * max_pool_size2)
full1_input_size = resulting_width * resulting_height * conv2_features
full1_weight = tf.Variable(tf.truncated_normal([full1_input_size, fully_connected_size1], stddev=0.1, dtype=tf.float32))
full1_bias = tf.Variable(tf.truncated_normal([fully_connected_size1], stddev=0.1, dtype=tf.float32))
full2_weight = tf.Variable(tf.truncated_normal([fully_connected_size1, target_size],
                                               stddev=0.1, dtype=tf.float32))
full2_bias = tf.Variable(tf.truncated_normal([target_size], stddev=0.1, dtype=tf.float32))


# Initialize Model Operations
def my_conv_net(input_data):
    # First Conv-ReLU-MaxPool Layer
    conv1 = tf.nn.conv2d(input_data, conv1_weight, strides=[1, 1, 1, 1], padding='SAME')
    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))
    max_pool1 = tf.nn.max_pool(relu1, ksize=[1, max_pool_size1, max_pool_size1, 1],
                               strides=[1, max_pool_size1, max_pool_size1, 1], padding='SAME')

    # Second Conv-ReLU-MaxPool Layer
    conv2 = tf.nn.conv2d(max_pool1, conv2_weight, strides=[1, 1, 1, 1], padding='SAME')
    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_bias))
    max_pool2 = tf.nn.max_pool(relu2, ksize=[1, max_pool_size2, max_pool_size2, 1],
                               strides=[1, max_pool_size2, max_pool_size2, 1], padding='SAME')

    # Transform Output into a 1xN layer for next fully connected layer
    final_conv_shape = max_pool2.get_shape().as_list()
    final_shape = final_conv_shape[1] * final_conv_shape[2] * final_conv_shape[3]
    flat_output = tf.reshape(max_pool2, [final_conv_shape[0], final_shape])

    # First Fully Connected Layer
    fully_connected1 = tf.nn.relu(tf.add(tf.matmul(flat_output, full1_weight), full1_bias))

    # Second Fully Connected Layer
    final_model_output = tf.add(tf.matmul(fully_connected1, full2_weight), full2_bias)
    
    # Add dropout
    final_model_output = tf.nn.dropout(final_model_output, dropout)
    
    return final_model_output


model_output = my_conv_net(x_input)
test_model_output = my_conv_net(eval_input)

# Declare Loss Function (softmax cross entropy)
loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=y_target))

# Create a prediction function
prediction = tf.nn.softmax(model_output)
test_prediction = tf.nn.softmax(test_model_output)


# Create accuracy function
def get_accuracy(logits, targets):
    batch_predictions = np.argmax(logits, axis=1)
    num_correct = np.sum(np.equal(batch_predictions, targets))
    return 100. * num_correct/batch_predictions.shape[0]


# Create an optimizer
my_optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)
train_step = my_optimizer.minimize(loss)

# Initialize Variables
init = tf.global_variables_initializer()
sess.run(init)


# Check values of tensors!
# 使用tf.test.TestCase()类来测试占位符或者变量的值
class drop_out_test(tf.test.TestCase):
    # Make sure that we don't drop too much
    def dropout_greaterthan(self):
        with self.test_session():
            self.assertGreater(dropout.eval(), 0.25)  # 确保droupout概率大于0.25


# Test accuracy function
# 测试准确度
class accuracy_test(tf.test.TestCase):
    # Make sure accuracy function behaves correctly
    def accuracy_exact_test(self):
        with self.test_session():
            test_preds = [[0.9, 0.1], [0.01, 0.99]]
            test_targets = [0, 1]
            test_acc = get_accuracy(test_preds, test_targets)
            self.assertEqual(test_acc.eval(), 100.)  # 确保准确度100%


# Test tensorshape
# 测试张量的形状
class shape_test(tf.test.TestCase):
    # Make sure our model output is size [batch_size, num_classes]
    def output_shape_test(self):
        with self.test_session():
            numpy_array = np.ones([batch_size, target_size])
            self.assertShapeEqual(numpy_array, model_output)


def main(argv):
    # Start training loop
    train_loss = []
    train_acc = []
    test_acc = []
    for i in range(generations):
        rand_index = np.random.choice(len(train_xdata), size=batch_size)
        rand_x = train_xdata[rand_index]
        rand_x = np.expand_dims(rand_x, 3)
        rand_y = train_labels[rand_index]
        train_dict = {x_input: rand_x, y_target: rand_y, dropout: dropout_prob}

        sess.run(train_step, feed_dict=train_dict)
        temp_train_loss, temp_train_preds = sess.run([loss, prediction], feed_dict=train_dict)
        temp_train_acc = get_accuracy(temp_train_preds, rand_y)

        if (i + 1) % eval_every == 0:
            eval_index = np.random.choice(len(test_xdata), size=evaluation_size)
            eval_x = test_xdata[eval_index]
            eval_x = np.expand_dims(eval_x, 3)
            eval_y = test_labels[eval_index]
            test_dict = {eval_input: eval_x, eval_target: eval_y, dropout: 1.0}
            test_preds = sess.run(test_prediction, feed_dict=test_dict)
            temp_test_acc = get_accuracy(test_preds, eval_y)

            # Record and print results
            train_loss.append(temp_train_loss)
            train_acc.append(temp_train_acc)
            test_acc.append(temp_test_acc)
            acc_and_loss = [(i + 1), temp_train_loss, temp_train_acc, temp_test_acc]
            acc_and_loss = [np.round(x, 2) for x in acc_and_loss]
            print('Generation # {}. Train Loss: {:.2f}. Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))


if __name__ == '__main__':
    cmd_args = sys.argv
    if len(cmd_args) > 1 and cmd_args[1] == 'test':
        # Perform unit tests
        tf.test.main(argv=cmd_args[1:])
    else:
        # Run TF App
        tf.app.run(main=None, argv=cmd_args)

```

## 多设备使用

计算图适合并行计算，可以由不同的处理器来处理。例中介绍在同一台机器上访问不同的处理器

```python
# -*- coding: utf-8 -*-
# Using Multiple Devices
#----------------------------------
#
# This function gives us the ways to use
#  multiple devices (executors) in TensorFlow.

import tensorflow as tf
from tensorflow.python.framework import ops
ops.reset_default_graph()

# To find out where placement occurs, set 'log_device_placement'
# 可以查看正在使用什么设备
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))

a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)

# Runs the op.
print(sess.run(c))


# If we load a graph and want device placement to be forgotten, we set a parameter in our session:
# tensorflow会自动在计算设备(CPU/GPU)上分配计算，但是有时希望搞清楚在使用的设备，当加载先前保存过的模型，并且该模型在计算图中已分配固定设备时，本地机器可以提供不同的设备给计算图用
config = tf.ConfigProto()
config.allow_soft_placement = True
sess_soft = tf.Session(config=config)

# GPUs
#---------------------------------
# Note that the GPU must have a compute capability > 3.5 for TF to use.
# http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability


# Careful with GPU memory allocation, TF never releases it.  TF starts with almost all of the GPU memory allocated.  We can slowly grow to that limit with an option setting:
# 当使用GPU时，tensorflow默认占据大部分GPU内存。虽然这是时常期望的，但是也要谨慎分配GPU内存。当Tensorflow一直不释放GPU内存时，如有必要，可以设置GPU内存增长选项让GPU内存分配缓慢增大到最大限制
config.gpu_options.allow_growth = True
sess_grow = tf.Session(config=config)

# Also, we can limit the size of GPU memory used, with the following option
# 可以限制死TF使用GPU内存百分比
config.gpu_options.per_process_gpu_memory_fraction = 0.4
sess_limited = tf.Session(config=config)


# How to set placements on multiple devices.
# Here, assume we have three devies CPU:0, GPU:0, and GPU:1
# 代码在GPU内存合适时利用GPU计算能力，并分配指定操作给GPU
if tf.test.is_built_with_cuda():
    with tf.device('/cpu:0'):
        a = tf.constant([1.0, 3.0, 5.0], shape=[1, 3])
        b = tf.constant([2.0, 4.0, 6.0], shape=[3, 1])
        
        with tf.device('/gpu:1'):
            c = tf.matmul(a,b)
            c = tf.reshape(c, [-1])
        
        with tf.device('/gpu:2'):
            d = tf.matmul(b,a)
            flat_d = tf.reshape(d, [-1])
        
        combined = tf.multiply(c, flat_d)
    print(sess.run(combined))

```

## 分布式实践

分布式tf允许构建一个tf集群（分布于不同的worker机器），共享相同的训练和评估模型的计算任务。使用分布式tensorflow只需简单设置worker节点参数，然后为不同的worker节点分配不同的作业。

```python
# -*- coding: utf-8 -*-
# Parallelizing TensorFlow
#----------------------------------
#
# We will show how to use TensorFlow distributed

import tensorflow as tf

# We will setup a local cluster (on localhost)

# Cluster for 2 local workers (tasks 0 and 1):
# 定义两个本地worker(端口分别为2222,2223)
cluster = tf.train.ClusterSpec({'local': ['localhost:2222', 'localhost:2223']})
# Server definition:
# 将两个worker加入到集群中，并标记task数字
server = tf.train.Server(cluster, job_name="local", task_index=0)
server = tf.train.Server(cluster, job_name="local", task_index=1)
# Finish and add
# server.join()

# Have each worker do a task
# Worker 0 : create matrices
# Worker 1 : calculate sum of all elements
# 为每个worker分配一个task，worker 0初始化两个矩阵，worker1计算每个矩阵所有元素的和。然后自动分配将两个和求和的任务并打印出结果。
mat_dim = 25
matrix_list = {}

with tf.device('/job:local/task:0'):
    for i in range(0, 2):
        m_label = 'm_{}'.format(i)
        matrix_list[m_label] = tf.random_normal([mat_dim, mat_dim])

# Have each worker calculate the Cholesky Decomposition
sum_outs = {}
with tf.device('/job:local/task:1'):
    for i in range(0, 2):
        A = matrix_list['m_{}'.format(i)]
        sum_outs['m_{}'.format(i)] = tf.reduce_sum(A)

    # Sum all the cholesky decompositions
    summed_out = tf.add_n(list(sum_outs.values()))

with tf.Session(server.target) as sess:
    result = sess.run(summed_out)
    print('Summed Values:{}'.format(result))

```

## 产品化提示

```python
# -*- coding: utf-8 -*-
# Tips for TensorFlow to Production
#----------------------------------
#
# Various Tips for Taking TensorFlow to Production

############################################
#
# THIS SCRIPT IS NOT RUNNABLE.
#  -it only contains tips for production code
#
############################################

# Also you can clear the default graph from memory
# 当运行tf程序时，希望确保内存中没有其他计算图绘画，或者每次调试程序时重置计算图会话
import tensorflow as tf
from tensorflow.python.framework import ops
ops.reset_default_graph()

# Saving Models
# File types created from saving:    
# checkpoint file:  Holds info on where the most recent models are
# events file:      Strictly for viewing graph in Tensorboard
# pbtxt file:       Textual protobufs file (uncompressed), used for debugging
# chkp file:        Holds data and model weights (large file)
# meta chkp files:  Model Graph and Meta-data (learning rate and operations)
# 当处理文本或任意数据管道，我们需要确保保存处理过的数据，以便随后用相同的方式处理评估数据。
# 如果处理文本数据，需要确定能够保存和加载词汇字典
# 存储格式可以时json,txt,csv或二进制等文件。

# Saving data pipeline structures (vocabulary, )
word_list = ['to', 'be', 'or', 'not', 'to', 'be']
vocab_list = list(set(word_list))
vocab2ix_dict = dict(zip(vocab_list, range(len(vocab_list))))
ix2vocab_dict = {val:key for key,val in vocab2ix_dict.items()}

# Save vocabulary
import json
with open('vocab2ix_dict.json', 'w') as file_conn:
    json.dump(vocab2ix_dict, file_conn)

# Load vocabulary
with open('vocab2ix_dict.json', 'r') as file_conn:
    vocab2ix_dict = json.load(file_conn)

# After model declaration, add a saving operations
# 为了保存算法模型的计算图和变量，在计算图中创建saver()操作。建议在模型训练过程中按照一定规则保存模型。
# saver()操作也可以传参。它能接收变量和张量的字典来保存指定元素，也可以接收checkpoint_every_n_hours参数来按时间规则保存操作。默认保存操作只保存最近的五个模型，但是也可以通过max_to_keep选项改变
saver = tf.train.Saver()
# Then during training, save every so often, referencing the training generation
for i in range(generations):
    ...
    if i%save_every == 0:
        saver.save(sess, 'my_model', global_step=step)

# Can also save only specific variables:
saver = tf.train.Saver({"my_var": my_variable})


# other options for saver are 'keep checkpoint_every_n_hours'
#      also 'max_to_keep'= default 5.
        
# Be sure to name operations, and variables for easy loading for referencing later
# 在保存算法模型之前，确保为模型重要的操作命名。若不提前命名，很难加载指定占位符、操作或变量
conv_weights = tf.Variable(tf.random_normal(), name='conv_weights')
loss = tf.reduce_mean(... , name='loss')

# Instead of tyring argparse and main(), TensorFlow provides an 'app' function
#  to handle running and loading of arguments

# At the beginning of the file, define the flags.
# 使用tf.apps.flags库使得执行命令行参数解析方便，可以定义string,float，integer,float等命令行参数
tf.flags.DEFINE_string("worker_locations", "", "List of worker addresses.")
tf.flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')
tf.flags.DEFINE_integer('generations', 1000, 'Number of training generations.')
tf.flags.DEFINE_boolean('run_unit_tests', False, 'If true, run tests.')
FLAGS = tf.flags.FLAGS

# Need to define a 'main' function for the app to run
def main(_):
    worker_ips = FLAGS.worker_locations.split(",")
    learning_rate = FLAGS.learning_rate
    generations = FLAGS.generations
    run_unit_tests = FLAGS.run_unit_tests

# Run the TensorFlow app
if __name__ == "__main__":
    # The following is looking for a "main()" function to run and will pass.
    tf.app.run()
    # Can modify this to be more custom:
    tf.app.run(main=my_main_function(), argv=my_arguments)


# Use of TensorFlow's built in logging:
# 使用内建的日志功能，可设置日志级别
# Five levels: DEBUG, INFO, WARN, ERROR, and FATAL
tf.logging.set_verbosity(tf.logging.WARN)
# WARN is the default value, but to see more information, you can set it to
#    INFO or DEBUG
tf.logging.set_verbosity(tf.logging.DEBUG)
# Note: 'DEBUG' is quite verbose.
```

## 产品化实例

产品级机器学习模型的最佳实践是分开训练和评估代码。

训练模型

```python
# -*- coding: utf-8 -*-
# TensorFlow Production Example (Training)
#----------------------------------
#
# We pull together everything and create an example
#    of best tensorflow production tips
#
# The example we will productionalize is the spam/ham RNN
#    from Chapter 9 (RNNs), section 2

import os
import re
import io
import sys
import requests
import numpy as np
import tensorflow as tf
from zipfile import ZipFile
from tensorflow.python.framework import ops
ops.reset_default_graph()

# Define App Flags
tf.flags.DEFINE_string("storage_folder", "temp", "Where to store model and data.")
tf.flags.DEFINE_float('learning_rate', 0.0005, 'Initial learning rate.')
tf.flags.DEFINE_float('dropout_prob', 0.5, 'Per to keep probability for dropout.')
tf.flags.DEFINE_integer('epochs', 20, 'Number of epochs for training.')
tf.flags.DEFINE_integer('batch_size', 250, 'Batch Size for training.')
tf.flags.DEFINE_integer('rnn_size', 15, 'RNN feature size.')
tf.flags.DEFINE_integer('embedding_size', 25, 'Word embedding size.')
tf.flags.DEFINE_integer('min_word_frequency', 20, 'Word frequency cutoff.')
tf.flags.DEFINE_boolean('run_unit_tests', False, 'If true, run tests.')

FLAGS = tf.flags.FLAGS


# Define how to get data
def get_data(storage_folder=FLAGS.storage_folder, data_file="text_data.txt"):
    """
    This function gets the spam/ham data.  It will download it if it doesn't
    already exist on disk (at specified folder/file location).
    """
    # Make a storage folder for models and data
    if not os.path.exists(storage_folder):
        os.makedirs(storage_folder)
    
    if not os.path.isfile(os.path.join(storage_folder, data_file)):
        zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'
        r = requests.get(zip_url)
        z = ZipFile(io.BytesIO(r.content))
        file = z.read('SMSSpamCollection')
        # Format Data
        text_data = file.decode()
        text_data = text_data.encode('ascii', errors='ignore')
        text_data = text_data.decode().split('\n')

        # Save data to text file
        with open(os.path.join(storage_folder, data_file), 'w') as file_conn:
            for text in text_data:
                file_conn.write("{}\n".format(text))
    else:
        # Open data from text file
        text_data = []
        with open(os.path.join(storage_folder, data_file), 'r') as file_conn:
            for row in file_conn:
                text_data.append(row)
        text_data = text_data[:-1]
    text_data = [x.split('\t') for x in text_data if len(x) >= 1]
    [y_data, x_data] = [list(x) for x in zip(*text_data)]
    
    return x_data, y_data


# Create a text cleaning function
def clean_text(text_string):
    text_string = re.sub(r'([^\s\w]|_|[0-9])+', '', text_string)
    text_string = " ".join(text_string.split())
    text_string = text_string.lower()
    return text_string


# Test clean_text function
class clean_test(tf.test.TestCase):
    # Make sure cleaning function behaves correctly
    def clean_string_test(self):
        with self.test_session():
            test_input = '--TensorFlow\'s so Great! Don\t you think so?   '
            test_expected = 'tensorflows so great don you think so'
            test_out = clean_text(test_input)
            self.assertEqual(test_expected, test_out)


# Define RNN Model
def rnn_model(x_data_ph, vocab_size, embedding_size, rnn_size, dropout_keep_prob):
    # Create embedding
    embedding_mat = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))
    embedding_output = tf.nn.embedding_lookup(embedding_mat, x_data_ph)

    # Define the RNN cell
    cell = tf.contrib.rnn.BasicRNNCell(num_units=rnn_size)
    output, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)
    output = tf.nn.dropout(output, dropout_keep_prob)

    # Get output of RNN sequence
    output = tf.transpose(output, [1, 0, 2])
    last = tf.gather(output, int(output.get_shape()[0]) - 1)

    weight = tf.Variable(tf.truncated_normal([rnn_size, 2], stddev=0.1))
    bias = tf.Variable(tf.constant(0.1, shape=[2]))
    logits_out = tf.matmul(last, weight) + bias
    
    return logits_out


# Define accuracy function
def get_accuracy(logits, actuals):
    # Calulate if each output is correct
    batch_acc = tf.equal(tf.argmax(logits, 1), tf.cast(actuals, tf.int64))
    # Convert logical to float
    batch_acc = tf.cast(batch_acc, tf.float32)
    return batch_acc


# Define main program
def main(args):
    # Set verbosity to get more information from TensorFlow
    tf.logging.set_verbosity(tf.logging.INFO)
    
    # Create a visualizer object for Tensorboard viewing
    summary_writer = tf.summary.FileWriter('tensorboard', tf.get_default_graph())
    # Create tensorboard folder if not exists
    if not os.path.exists('tensorboard'):
        os.makedirs('tensorboard')
    
    # Set model parameters
    storage_folder = FLAGS.storage_folder
    learning_rate = FLAGS.learning_rate
    run_unit_tests = FLAGS.run_unit_tests
    epochs = FLAGS.epochs
    batch_size = FLAGS.batch_size
    rnn_size = FLAGS.rnn_size
    embedding_size = FLAGS.embedding_size
    min_word_frequency = FLAGS.min_word_frequency
    
    # Get text->spam/ham data
    x_data, y_data = get_data()
    
    # Clean texts
    x_data = [clean_text(x) for x in x_data]

    # Change texts into numeric vectors
    # Set a max sequence length for speeding up the computations.
    # But we can easily set "max_sequence_length = max([len(x) for x in x_data])" as well.
    max_sequence_length = 20
    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sequence_length,
                                                                         min_frequency=min_word_frequency)
    text_processed = np.array(list(vocab_processor.fit_transform(x_data)))
    
    # Save vocab processor (for loading and future evaluation)
    vocab_processor.save(os.path.join(storage_folder, "vocab"))
    
    # Shuffle and split data
    text_processed = np.array(text_processed)
    y_data = np.array([1 if x == 'ham' else 0 for x in y_data])
    shuffled_ix = np.random.permutation(np.arange(len(y_data)))
    x_shuffled = text_processed[shuffled_ix]
    y_shuffled = y_data[shuffled_ix]

    # Split train/test set
    ix_cutoff = int(len(y_shuffled)*0.80)
    x_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]
    y_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]
    vocab_size = len(vocab_processor.vocabulary_)

    with tf.Graph().as_default():
        sess = tf.Session()
        # Define placeholders
        x_data_ph = tf.placeholder(tf.int32, [None, max_sequence_length], name='x_data_ph')
        y_output_ph = tf.placeholder(tf.int32, [None], name='y_output_ph')
        dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')

        # Define Model
        rnn_model_outputs = rnn_model(x_data_ph, vocab_size, embedding_size, rnn_size, dropout_keep_prob)

        # Prediction
        # Although we won't use the following operation, we declare and name
        #   the probability outputs so that we can recall them later for evaluation
        rnn_prediction = tf.nn.softmax(rnn_model_outputs, name="probability_outputs")
        
        # Loss function
        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=rnn_model_outputs, labels=y_output_ph)
        # Remember that for this loss function, logits=float32, labels=int32
        loss = tf.reduce_mean(losses, name="loss")

        # Model Accuracy Operation
        accuracy = tf.reduce_mean(get_accuracy(rnn_model_outputs, y_output_ph), name="accuracy")
    
        # Add scalar summaries for Tensorboard
        with tf.name_scope('Scalar_Summaries'):
                tf.summary.scalar('Loss', loss)
                tf.summary.scalar('Accuracy', accuracy)
    
        # Declare Optimizer/train step
        optimizer = tf.train.GradientDescentOptimizer(learning_rate)
        train_step = optimizer.minimize(loss)
        
        # Declare summary merging operation
        summary_op = tf.summary.merge_all()
    
        # Create a graph/Variable saving/loading operations
        saver = tf.train.Saver()    
    
        init = tf.global_variables_initializer()
        sess.run(init)
    
        # Start training
        for epoch in range(epochs):

            # Shuffle training data
            shuffled_ix = np.random.permutation(np.arange(len(x_train)))
            x_train = x_train[shuffled_ix]
            y_train = y_train[shuffled_ix]
            num_batches = int(len(x_train)/batch_size) + 1
            #
            for i in range(num_batches):
                # Select train data
                min_ix = i * batch_size
                max_ix = np.min([len(x_train), ((i+1) * batch_size)])
                x_train_batch = x_train[min_ix:max_ix]
                y_train_batch = y_train[min_ix:max_ix]
        
                # Run train step
                train_dict = {x_data_ph: x_train_batch,
                              y_output_ph: y_train_batch,
                              dropout_keep_prob: 0.5}
                _, summary = sess.run([train_step, summary_op], feed_dict=train_dict)
                
                summary_writer = tf.summary.FileWriter('tensorboard')
                summary_writer.add_summary(summary, i)
        
            # Run loss and accuracy for training
            temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict=train_dict)

            test_dict = {x_data_ph: x_test, y_output_ph: y_test, dropout_keep_prob: 1.0}
            temp_test_loss, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)
            
            # Print Epoch Summary
            print('Epoch: {}, Train Loss:{:.2}, Train Acc: {:.2}'.format(epoch+1, temp_train_loss, temp_train_acc))
            print('Epoch: {}, Test Loss: {:.2}, Test Acc: {:.2}'.format(epoch+1, temp_test_loss, temp_test_acc))
            
            # Save model every epoch
            saver.save(sess, os.path.join(storage_folder, "model.ckpt"))


# Run main module/tf App
if __name__ == "__main__":
    cmd_args = sys.argv
    if len(cmd_args) > 1 and cmd_args[1] == 'test':
        # Perform unit tests
        tf.test.main(argv=cmd_args[1:])
    else:
        # Run TF App
        tf.app.run(main=None, argv=cmd_args)

```

评估模型

```python
# -*- coding: utf-8 -*-
# TensorFlow Production Example (Evaluating)
#----------------------------------
#
# We pull together everything and create an example
#    of best tensorflow production tips
#
# The example we will productionalize is the spam/ham RNN
#    from the RNN Chapter.

import os
import re
import numpy as np
import tensorflow as tf
from tensorflow.python.framework import ops
ops.reset_default_graph()

# 声明应用flag
tf.flags.DEFINE_string("storage_folder", "temp", "Where to store model and data.")
tf.flags.DEFINE_bool('model_file', False, 'Model file location.')
tf.flags.DEFINE_bool('run_unit_tests', False, 'If true, run tests.')
FLAGS = tf.flags.FLAGS


# Create a text cleaning function
# 声明清洗函数
def clean_text(text_string):
    text_string = re.sub(r'([^\s\w]|_|[0-9])+', '', text_string)
    text_string = " ".join(text_string.split())
    text_string = text_string.lower()
    return text_string


# Load vocab processor
def load_vocab():
    vocab_path = os.path.join(FLAGS.storage_folder, "vocab")
    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(vocab_path)
    return vocab_processor


# Process input data:
# 加载词汇处理函数
def process_data(input_data, vocab_processor):
    input_data = clean_text(input_data)
    input_data = input_data.split()
    processed_input = np.array(list(vocab_processor.transform(input_data)))
    return processed_input


# Get input function
# 获取数据来评估模型
def get_input_data():
    """
    For this function, we just prompt the user for a text message to evaluate
        But this function could also potentially read a file in as well.
    """
    input_text = input("Please enter a text message to evaluate: ")
    vocab_processor = load_vocab()
    return process_data(input_text, vocab_processor)


# Test clean_text function
# 对于单元测试，应确保文本处理函数的表现符合预期
class clean_test(tf.test.TestCase):
    # Make sure cleaning function behaves correctly
    def clean_string_test(self):
        with self.test_session():
            test_input = '--TensorFlow\'s so Great! Don\t you think so?   '
            test_expected = 'tensorflows so great don you think so'
            test_out = clean_text(test_input)
            self.assertEqual(test_expected, test_out)


# Main function
# 主函数获取数据集、建立计算图、加载模型变量，传入处理过的数据、打印输出结果
def main(args):
    # Get flags
    storage_folder = FLAGS.storage_folder
    
    # Get user input text
    x_data = get_input_data()
    
    # Load model
    graph = tf.Graph()
    with graph.as_default():
        sess = tf.Session()
        with sess.as_default():
            # Load the saved meta graph and restore variables
            saver = tf.train.import_meta_graph("{}.meta".format(os.path.join(storage_folder, "model.ckpt")))
            saver.restore(sess, os.path.join(storage_folder, "model.ckpt"))

            # Get the placeholders from the graph by name
            x_data_ph = graph.get_operation_by_name("x_data_ph").outputs[0]
            dropout_keep_prob = graph.get_operation_by_name("dropout_keep_prob").outputs[0]
            probability_outputs = graph.get_operation_by_name("probability_outputs").outputs[0]

            # Make the prediction
            eval_feed_dict = {x_data_ph: x_data, dropout_keep_prob: 1.0}
            probability_prediction = sess.run(tf.reduce_mean(probability_outputs, 0), eval_feed_dict)
            
            # Print output (Or save to file or DB connection?)
            print('Probability of Spam: {:.4}'.format(probability_prediction[1]))


# Run main module/tf App
if __name__ == "__main__":
    if FLAGS.run_unit_tests:
        # Perform unit tests
        tf.test.main()
    else:
        # Run evaluation
        tf.app.run()
```

## 服务部署

安装

```
sudo apt install teensorflow-model-server
```

更改模型代码，以protobuf格式将模型保存在使用tensorflow服务所需的合适文件夹结构中

训练代码

```python
# -*- coding: utf-8 -*-
# Using TensorFlow Serving
#----------------------------------
#
# We show how to use "TensorFlow Serving", a model serving api from TensorFlow to serve a model.
#
# Pre-requisites:
#  - Visit https://www.tensorflow.org/serving/setup
#    and follow all the instructions on setting up TensorFlow Serving (including installing Bazel).
#
# The example we will productionalize is the spam/ham RNN
#    from Chapter 9 (RNNs), section 2

import os
import re
import io
import sys
import requests
import numpy as np
import tensorflow as tf
from zipfile import ZipFile
from tensorflow.python.framework import ops

ops.reset_default_graph()

# Define App Flags
tf.flags.DEFINE_string("storage_folder", "temp", "Where to store model and data.")
tf.flags.DEFINE_float('learning_rate', 0.0005, 'Initial learning rate.')
tf.flags.DEFINE_float('dropout_prob', 0.5, 'Per to keep probability for dropout.')
tf.flags.DEFINE_integer('epochs', 20, 'Number of epochs for training.')
tf.flags.DEFINE_integer('batch_size', 250, 'Batch Size for training.')
tf.flags.DEFINE_integer('rnn_size', 15, 'RNN feature size.')
tf.flags.DEFINE_integer('embedding_size', 25, 'Word embedding size.')
tf.flags.DEFINE_integer('min_word_frequency', 20, 'Word frequency cutoff.')
tf.flags.DEFINE_boolean('run_unit_tests', False, 'If true, run tests.')

FLAGS = tf.flags.FLAGS


# Define how to get data
def get_data(storage_folder=FLAGS.storage_folder, data_file="text_data.txt"):
    """
    This function gets the spam/ham data.  It will download it if it doesn't
    already exist on disk (at specified folder/file location).
    """
    # Make a storage folder for models and data
    if not os.path.exists(storage_folder):
        os.makedirs(storage_folder)

    if not os.path.isfile(os.path.join(storage_folder, data_file)):
        zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'
        r = requests.get(zip_url)
        z = ZipFile(io.BytesIO(r.content))
        file = z.read('SMSSpamCollection')
        # Format Data
        text_data = file.decode()
        text_data = text_data.encode('ascii', errors='ignore')
        text_data = text_data.decode().split('\n')

        # Save data to text file
        with open(os.path.join(storage_folder, data_file), 'w') as file_conn:
            for text in text_data:
                file_conn.write("{}\n".format(text))
    else:
        # Open data from text file
        text_data = []
        with open(os.path.join(storage_folder, data_file), 'r') as file_conn:
            for row in file_conn:
                text_data.append(row)
        text_data = text_data[:-1]
    text_data = [x.split('\t') for x in text_data if len(x) >= 1]
    [y_data, x_data] = [list(x) for x in zip(*text_data)]

    return x_data, y_data


# Create a text cleaning function
def clean_text(text_string):
    text_string = re.sub(r'([^\s\w]|_|[0-9])+', '', text_string)
    text_string = " ".join(text_string.split())
    text_string = text_string.lower()
    return text_string


# Test clean_text function
class clean_test(tf.test.TestCase):
    # Make sure cleaning function behaves correctly
    def clean_string_test(self):
        with self.test_session():
            test_input = '--TensorFlow\'s so Great! Don\t you think so?   '
            test_expected = 'tensorflows so great don you think so'
            test_out = clean_text(test_input)
            self.assertEqual(test_expected, test_out)


# Define RNN Model
def rnn_model(x_data_ph, vocab_size, embedding_size, rnn_size, dropout_keep_prob):
    # Create embedding
    embedding_mat = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))
    embedding_output = tf.nn.embedding_lookup(embedding_mat, x_data_ph)

    # Define the RNN cell
    cell = tf.contrib.rnn.BasicRNNCell(num_units=rnn_size)
    output, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)
    output = tf.nn.dropout(output, dropout_keep_prob)

    # Get output of RNN sequence
    output = tf.transpose(output, [1, 0, 2])
    last = tf.gather(output, int(output.get_shape()[0]) - 1)

    weight = tf.Variable(tf.truncated_normal([rnn_size, 2], stddev=0.1))
    bias = tf.Variable(tf.constant(0.1, shape=[2]))
    logits_out = tf.matmul(last, weight) + bias

    return logits_out


# Define accuracy function
def get_accuracy(logits, actuals):
    # Calulate if each output is correct
    batch_acc = tf.equal(tf.argmax(logits, 1), tf.cast(actuals, tf.int64))
    # Convert logical to float
    batch_acc = tf.cast(batch_acc, tf.float32)
    return batch_acc


# Define main program
def main(args):
    # Set verbosity to get more information from TensorFlow
    tf.logging.set_verbosity(tf.logging.INFO)

    # Create a visualizer object for Tensorboard viewing
    summary_writer = tf.summary.FileWriter('tensorboard', tf.get_default_graph())
    # Create tensorboard folder if not exists
    if not os.path.exists('tensorboard'):
        os.makedirs('tensorboard')

    # Set model parameters
    storage_folder = FLAGS.storage_folder
    learning_rate = FLAGS.learning_rate
    run_unit_tests = FLAGS.run_unit_tests
    epochs = FLAGS.epochs
    batch_size = FLAGS.batch_size
    rnn_size = FLAGS.rnn_size
    embedding_size = FLAGS.embedding_size
    min_word_frequency = FLAGS.min_word_frequency

    # Get text->spam/ham data
    x_data, y_data = get_data()

    # Clean texts
    x_data = [clean_text(x) for x in x_data]

    # Change texts into numeric vectors
    # Set a max sequence length for speeding up the computations.
    # But we can easily set "max_sequence_length = max([len(x) for x in x_data])" as well.
    max_sequence_length = 20
    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sequence_length,
                                                                         min_frequency=min_word_frequency)
    text_processed = np.array(list(vocab_processor.fit_transform(x_data)))

    # Save vocab processor (for loading and future evaluation)
    vocab_processor.save(os.path.join(storage_folder, "vocab"))

    # Shuffle and split data
    text_processed = np.array(text_processed)
    y_data = np.array([1 if x == 'ham' else 0 for x in y_data])
    shuffled_ix = np.random.permutation(np.arange(len(y_data)))
    x_shuffled = text_processed[shuffled_ix]
    y_shuffled = y_data[shuffled_ix]

    # Split train/test set
    ix_cutoff = int(len(y_shuffled) * 0.80)
    x_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]
    y_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]
    vocab_size = len(vocab_processor.vocabulary_)

    with tf.Graph().as_default():
        sess = tf.Session()
        # Define placeholders
        x_data_ph = tf.placeholder(tf.int32, [None, max_sequence_length], name='x_data_ph')
        y_output_ph = tf.placeholder(tf.int32, [None], name='y_output_ph')
        dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')

        # Define Model
        rnn_model_outputs = rnn_model(x_data_ph, vocab_size, embedding_size, rnn_size, dropout_keep_prob)

        # Prediction
        # Although we won't use the following operation, we declare and name
        #   the probability outputs so that we can recall them later for evaluation
        rnn_prediction = tf.nn.softmax(rnn_model_outputs, name="probability_outputs")

        # Loss function
        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=rnn_model_outputs, labels=y_output_ph)
        # Remember that for this loss function, logits=float32, labels=int32
        loss = tf.reduce_mean(losses, name="loss")

        # Model Accuracy Operation
        accuracy = tf.reduce_mean(get_accuracy(rnn_model_outputs, y_output_ph), name="accuracy")

        # Add scalar summaries for Tensorboard
        with tf.name_scope('Scalar_Summaries'):
            tf.summary.scalar('Loss', loss)
            tf.summary.scalar('Accuracy', accuracy)

        # Declare Optimizer/train step
        optimizer = tf.train.GradientDescentOptimizer(learning_rate)
        train_step = optimizer.minimize(loss)

        # Declare summary merging operation
        summary_op = tf.summary.merge_all()

        # Create a graph/Variable saving/loading operations
        saver = tf.train.Saver()

        init = tf.global_variables_initializer()
        sess.run(init)

        # Start training
        for epoch in range(epochs):

            # Shuffle training data
            shuffled_ix = np.random.permutation(np.arange(len(x_train)))
            x_train = x_train[shuffled_ix]
            y_train = y_train[shuffled_ix]
            num_batches = int(len(x_train) / batch_size) + 1
            #
            for i in range(num_batches):
                # Select train data
                min_ix = i * batch_size
                max_ix = np.min([len(x_train), ((i + 1) * batch_size)])
                x_train_batch = x_train[min_ix:max_ix]
                y_train_batch = y_train[min_ix:max_ix]

                # Run train step
                train_dict = {x_data_ph: x_train_batch,
                              y_output_ph: y_train_batch,
                              dropout_keep_prob: 0.5}
                _, summary = sess.run([train_step, summary_op], feed_dict=train_dict)

                summary_writer = tf.summary.FileWriter('tensorboard')
                summary_writer.add_summary(summary, i)

            # Run loss and accuracy for training
            temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict=train_dict)

            test_dict = {x_data_ph: x_test, y_output_ph: y_test, dropout_keep_prob: 1.0}
            temp_test_loss, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)

            # Print Epoch Summary
            print('Epoch: {}, Train Loss:{:.2}, Train Acc: {:.2}'.format(epoch + 1, temp_train_loss, temp_train_acc))
            print('Epoch: {}, Test Loss: {:.2}, Test Acc: {:.2}'.format(epoch + 1, temp_test_loss, temp_test_acc))

            # Save model every epoch
            saver.save(sess, os.path.join(storage_folder, "model.ckpt"))

        # Save the finished model for TensorFlow Serving (pb file)
        # Here, it's our storage folder / version number
        out_path = os.path.join(tf.compat.as_bytes(os.path.join(storage_folder, '1')))
        print('Exporting finished model to : {}'.format(out_path))
        builder = tf.saved_model.builder.SavedModelBuilder(out_path)

        # Build the signature_def_map.
        classification_inputs = tf.saved_model.utils.build_tensor_info(x_data_ph)
        classification_outputs_classes = tf.saved_model.utils.build_tensor_info(rnn_model_outputs)

        classification_signature = (tf.saved_model.signature_def_utils.build_signature_def(
                inputs={tf.saved_model.signature_constants.CLASSIFY_INPUTS: classification_inputs},
                outputs={tf.saved_model.signature_constants.CLASSIFY_OUTPUT_CLASSES: classification_outputs_classes},
                method_name=tf.saved_model.signature_constants.CLASSIFY_METHOD_NAME)
        )

        tensor_info_x = tf.saved_model.utils.build_tensor_info(x_data_ph)
        tensor_info_y = tf.saved_model.utils.build_tensor_info(y_output_ph)

        prediction_signature = (
            tf.saved_model.signature_def_utils.build_signature_def(
                inputs={'texts': tensor_info_x},
                outputs={'scores': tensor_info_y},
                method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))

        legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')
        builder.add_meta_graph_and_variables(
            sess, [tf.saved_model.tag_constants.SERVING],
            signature_def_map={
                'predict_spam':
                    prediction_signature,
                tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:
                    classification_signature,
            },
            legacy_init_op=legacy_init_op)

        builder.save()

        print('Done exporting!')


# Run main module/tf App
if __name__ == "__main__":
    cmd_args = sys.argv
    if len(cmd_args) > 1 and cmd_args[1] == 'test':
        # Perform unit tests
        tf.test.main(argv=cmd_args[1:])
    else:
        # Run TF App
        tf.app.run(main=None, argv=cmd_args)

        
#######终端启动服务#######
tennsorflow_model_server --port=900 --model_name=spam_ham --model_base_path=<...>/temp/
```

客户端

```python
# -*- coding: utf-8 -*-
# Using TensorFlow Serving (CLIENT)
#----------------------------------
#
# We show how to use "TensorFlow Serving", a model serving api from TensorFlow to serve a model.
#
# Pre-requisites:
#  - Visit https://www.tensorflow.org/serving/setup
#    and follow all the instructions on setting up TensorFlow Serving (including installing Bazel).
#
# The example we will query the TensorFlow-Serving-API we have running on port 9000

import os
import re
import grpc
import numpy as np
import tensorflow as tf

from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2

tf.flags.DEFINE_string('server', '9000', 'PredictionService host')
tf.flags.DEFINE_string('port', '0.0.0.0', 'PredictionService port')
tf.flags.DEFINE_string('data_dir', 'temp', 'Folder where vocabulary is.')
FLAGS = tf.flags.FLAGS


# Def a functions to process texts into arrays of indices
# Create a text cleaning function
def clean_text(text_string):
    text_string = re.sub(r'([^\s\w]|_|[0-9])+', '', text_string)
    text_string = " ".join(text_string.split())
    text_string = text_string.lower()
    return text_string


# Load vocab processor
def load_vocab():
    vocab_path = os.path.join(FLAGS.data_dir, 'vocab')
    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(vocab_path)
    return vocab_processor


# Process input data:
def process_data(input_data):
    vocab_processor = load_vocab()
    input_data = [clean_text(x) for x in input_data]
    processed_input = np.array(list(vocab_processor.transform(input_data)))
    return processed_input


def get_results(data, server, port):
    channel = grpc.insecure_channel(':'.join([server, port]))
    stub = prediction_service_pb2.PredictionServiceStub(channel)
    processed_data = process_data(data)

    results = []
    for input_x in processed_data:
        request = predict_pb2.PredictRequest()
        request.model_spec.name = 'spam_ham'
        request.model_spec.signature_name = 'predict_spam'  # Change to predict spam
        request.inputs['texts'].CopyFrom(tf.contrib.util.make_tensor_proto(input_x, shape=[4, 20]))  # 'texts'
        prediction_future = stub.Predict(request)
        prediction = prediction_future.result().outputs['scores']
        # prediction = np.array(prediction_future.result().outputs['scores'].float_val)
        results.append(prediction)
    return results


def main(data):
    if not FLAGS.server:
        print('please specify server host:port')
        return
    results = get_results(data, FLAGS.server, FLAGS.port)

    for input_text, output_pred in zip(data, results):
        print('Input text: {}, Prediction: {}'.format(input_text, output_pred))


if __name__ == '__main__':
    # Get sample data, here you may feel free to change this to a file, cloud-address, user input, etc...
    test_data = ['Please respond ASAP to claim your prize !',
                 'Hey, are you coming over for dinner tonight?',
                 'Text 444 now to see the top users in your area',
                 'drive safe, and thanks for visiting again!']

    tf.app.run(argv=test_data)

```

