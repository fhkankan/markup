# 多项式回归

非线性回归

## 原理

对线性回归增加特征维度，形成非线性回归

## 实现

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x **2 + x + 2 + np.random.normal(0, 1, size=100)

plt.scatter(x, y)
plt.show()

# 直接线性回归，拟合效果较差
lin_reg = LinearRegression()
lin_reg.fit(X, y)
y_predict = lin_reg.predict(X)

plt.scatter(x, y)
plt.plot(x, y_predict, color='r')
plt.show()

# 解决方案，添加一个特征
X2= np.hstack([X, X**2])
print(X.shape)
lin_reg2 = LinearRegression()
lin_reg2.fit(X2, y)
y_predict2 = lin_reg.predict(X2)

plt.scatter(x, y)
plt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')
plt.show()

print(lin_reg2.coef_)
print(lin_reg2.intercept_)
```

## sklearn

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x **2 + x + 2 + np.random.normal(0, 1, size=100)

poly = PolynomialFeatures(degree=2)
poly.fit(X)
X2 = poly.transform(X)
print(X2.shape)

lin_reg2 = LinearRegression()
lin_reg2.fit(X2, y)
y_predict2 = lin_reg2.predict(X2)

plt.scatter(x, y)
plt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')
plt.show()

print(lin_reg2.coef_)
print(lin_reg2.intercept_)
```

pipeline

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x **2 + x + 2 + np.random.normal(0, 1, size=100)

poly_reg = Pipeline([
  ("poly", PolynomialFeatures(degree=2)),
  ("std_scaler", StandardScaler()),
  ("lin_reg", LinearRegression())
])

poly_reg.fit(X, y)
y_predict = poly_reg.predict(X)

plt.scatter(x, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()

print(lin_reg2.coef_)
print(lin_reg2.intercept_)
```

## 拟合

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.pipline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler


np.random.seed(666)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * X ** 2 + x * 2 + np.random.normal(0, 1, size = 100)

plt.scatter(x, y)
plt.show()

# 线性回归,欠拟合underfitting
lin_reg = LinearRegression()
lin_reg.fit(X, y)
lin_reg.score(X, y)  # R2值比较低
y_predict = lin_reg.predict(X)
mean_squared_error(y, y_predict) # 使用均方误差来测量不同模型下的准确度

plt.scatter(x, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()

# 多项式回归
def PolynomialRegression(degree):
  	return Pipeline([
      	("poly", PolynomialFeatures(degree=degree)),
      	("std_scaler", StandardScaller()),
      	("lin_reg", LinearRegression())
    ])
poly2_reg = PolynomialRegression(degree=2)
poly2_reg.fit(X, y)
y2_predict = poly2_reg.predict(X)
mean_squared_error(y, y2_predict)

plt.scatter(x, y)
plt.plot(np.sort(x), y2_predict[np.argsort(x)], color='r')
plt.show()

# 过拟合,overfitting
poly100_reg = PolynomialRegression(degree=100)
poly100_reg.fit(X, y)
y100_predict = poly100_reg.predict(X)
mean_squared_error(y, y100_predict)

plt.scatter(x, y)
plt.plot(np.sort(x), y100_predict[np.argsort(x)], color='r')
plt.show()
```

## 学习曲线

随着训练样本的逐渐增多，算法训练出的模型的表现能力

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.pipline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler


np.random.seed(666)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * X ** 2 + x * 2 + np.random.normal(0, 1, size = 100)

plt.scatter(x, y)
plt.show()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)
print(X_train.shape)

train_score = []
test_score = []
for in in range(1, 76):
  	lin_reg = LinearRegression()
    lin_reg.fit(X_train[:i], y_train[:i])
    y_train_predict = lin_reg.predict(X_tain[:i])
    train_score.append(mean_squared_error(y_train[:i], y_train_predict))
    y_test_predict = lin_reg.predict(X_test)
    test_score.append(mean_squared_error(y_test, y_test_predict))
    
plt.plot([i for i in range(1, 76)], np.sqrt(train_score), label="train")
plt.plot([i for i in range(1, 76)], np.sqrt(test_score), label="test")
plt.legend()
plt.show()

# 封装函数
def plot_learning_curve(algo, X_train, X_test, y_train, y_test):
		train_score = []
		test_score = []
		for in in range(1, 76):
    		algo.fit(X_train[:i], y_train[:i])
    		y_train_predict = algo.predict(X_tain[:i])
    		train_score.append(mean_squared_error(y_train[:i], y_train_predict))
    		y_test_predict = algo.predict(X_test)
    		test_score.append(mean_squared_error(y_test, y_test_predict))
    
		plt.plot([i for i in range(1, 76)], np.sqrt(train_score), label="train")
		plt.plot([i for i in range(1, 76)], np.sqrt(test_score), label="test")
		plt.legend()
    plt.axis([0, len(X_train)+1, 0, 4])
		plt.show()
# 线性回归调用
plot_learning_curve(LinearRegression(), X_train, X_test, y_train, y_test)
# 多项式回归调用
def PolynomialRegression(degree):
  	return Pipeline([
      	("poly", PolynomialFeatures(degree=degree)),
      	("std_scaler", StandardScaller()),
      	("lin_reg", LinearRegression())
    ])
poly2_reg = PolynomialRegression(degree=2)
plot_learning_curve(poly2_reg, X_train, X_test, y_train, y_test)
# 过拟合调用
poly20_reg = PolynomialRegression(degree=20)
plot_learning_curve(poly20_reg, X_train, X_test, y_train, y_test)
```

