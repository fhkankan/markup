# 最佳实践

## 对话驱动

对话驱动开发 (CDD) 是倾听用户意见并利用这些见解改进 AI 助手的过程。这是聊天机器人开发的总体最佳实践方法。开发出色的 AI 助手具有挑战性，因为用户总是会说出您没有预料到的内容。 CDD 背后的原则是，在每次对话中，用户都用他们自己的话告诉您他们想要什么。通过在机器人开发的每个阶段练习 CDD，您可以让您的助手面向真实的用户语言和行为。

CDD 包括以下操作：

```
尽快与用户共享您的助手
定期查看对话
注释消息并将其用作 NLU 训练数据
测试您的助手是否始终如您所愿
跟踪助手失败时的行为和随着时间的推移衡量其性能
修复您的助手如何处理不成功的对话
```

CDD 不是一个线性过程；随着您开发和改进您的机器人，您将一遍又一遍地重复相同的操作。

### 项目早期CDD

如果您处于机器人开发的最早阶段，那么 CDD 似乎没有任何作用 - 毕竟，您还没有对话！但是，您可以在机器人开发的一开始就采取 CDD 操作：

```
1.有关使用 CDD 创建训练数据的详细信息，请参阅 NLU 数据和故事的最佳实践。
2.让您的机器人尽早测试用户。
3.设置 CI/CD 管道
```

### 生产环境CDD

一旦你的机器人投入生产，你将有更多的对话来获得洞察力。然后您可以完全应用 CDD 操作。在这个阶段，您可以在服务器上安装 Rasa X 以部署您的机器人并在生产中使用机器人启用 CDD。

- 预览

在对话中寻找用户真正想要的东西。

你的测试用户至少有一些关于机器人打算做什么的说明；真正的用户通常要么不知道，要么忽略给他们的指令。你不能迎合每一个意想不到的用户行为，但你可以尝试解决你注意到的主要摩擦点。以下是您可以考虑寻找的一些内容：

```
1.查看发生“out_of_scope”意图或回退行为的对话。这些可能表明潜在的新技能，或者只是错误分类的用户话语。
2.寻找用户的挫败感，例如转移给人类的请求。
3.如果助手接受了管道中包含的 UnexpectTEDIntentPolicy 的培训，您可以寻找在任何谈话轮次中预测action_unlikely_intent的对话。当用户表达的最后一个意图在当前对话上下文中出乎意料时，就会预测到 action_unlikely_intent。您还可以通过运行独立的 执行以下操作的脚本：
	a.从跟踪器存储中获取真实对话。
	b.对获取的对话运行`rasa test`，并在单独的警告文件中过滤包含`action_unlikely_intent`的对话。
	
	查看此对话子集可以帮助您了解真实用户是否采用了训练数据中不存在的对话路径，因此对于 TEDPolicy 等机器学习策略来说“令人惊讶”。添加这些对话路径（如果 TEDPolicy 随后失败，可能会进行更正）作为训练故事将导致通过 TEDPolicy 等策略进行更稳健的动作预测。鼓励用户调整 UnexpecTEDIntentPolicy 的容差参数，以控制对话应包含在警告文件中的“令人惊讶”程度。 
```

- 注解

在将来自真实对话的新用户话语添加到训练数据中时，继续遵循 NLU 的最佳实践。注意不要将 NLU 模型过度拟合到训练数据中已经存在的话语中。当您不断地将已经正确预测且具有高置信度的用户话语添加到您的训练数据中时，就会发生这种情况。为避免过度拟合并帮助您的模型泛化到更多样化的用户话语，请仅添加模型先前预测不正确或置信度较低的用户话语。

- 测试

将成功的用户对话添加到您的测试对话中。始终如一地这样做将有助于确保您在对机器人进行其他修复时不会引入回归。

- 追踪

寻找成功和失败的线索，以帮助您跟踪机器人的性能。

一些指标是机器人外部的。例如，如果您正在构建一个机器人来缓解对客户服务呼叫中心的需求，那么衡量成功的一个指标可能是呼叫中心流量的减少。其他可以直接从对话中获得的信息，例如用户是否达到了代表实现用户目标的某个动作。

自动跟踪的指标本质上是代理指标；获得真正成功衡量标准的唯一方法是单独审查和评价与您的机器人的每一次对话。虽然这显然不现实，但请记住，没有任何指标可以完美地代表您的机器人的性能，因此不要仅依靠指标来了解您的机器人需要改进的地方。

- 修正

随着您扩展和提高机器人的技能，继续遵循故事的最佳实践。让用户需求指导您添加哪些技能以及进行哪些修复。经常进行较小的更改，而不是偶尔进行一次大更改。这将帮助您衡量所做更改的有效性，因为您将更频繁地获得用户反馈。您的 CI/CD 管道应该让您有信心这样做。

## 生成nlu数据

NLU（Natural Language Understanding）是 Rasa Open Source 的一部分，它执行意图分类、实体提取和响应检索。

NLU 将接受诸如“我在市中心寻找一家法国餐厅”之类的句子并返回结构化数据如：

```json
{
  "intent": "search_restaurant",
  "entities": {
    "cuisine": "French",
    "location": "center"
  }
}
```

构建 NLU 模型很难，构建可用于生产的模型更难。以下是设计 NLU 训练数据和管道以充分利用机器人的一些技巧。

### NLU 的对话驱动开发

对话驱动开发 (CDD) 意味着让真实的用户对话指导您的开发。对于构建一个出色的 NLU 模型，这意味着两个关键的事情：

- **收集真实数据**

在构建 NLU 训练数据时，开发人员有时会尝试使用文本生成工具或模板来快速增加训练示例的数量。这是一个坏主意，原因有两个

```
1.您的合成数据看起来不像用户实际发送给您的助手的消息，因此您的模型将表现不佳。
2.通过对合成数据进行训练和测试，您欺骗了自己，会认为您的模型实际上表现良好，并且您不会注意到重大问题
```

请记住，如果您使用脚本生成训练数据，您的模型唯一可以学习的是如何对脚本进行逆向工程。

避免这些问题，收集尽可能多的真实用户数据作为训练数据总是一个好主意。真实的用户消息可能很混乱，包含拼写错误，并且与您的意图的“理想”示例相去甚远。但请记住，这些是您要求模型进行预测的消息！你的助手最初总会犯错，但对用户数据进行训练和评估的过程将使您的模型在现实场景中更有效地泛化。 

- **尽快给测试人员分享**

为了收集真实数据，您将需要真实的用户消息。机器人开发人员只能提出有限范围的示例，用户所说的话总是使你感到惊讶。这意味着您应该尽早与开发团队之外的测试用户共享您的机器人。有关更多详细信息，请参阅完整的 CDD 指南。

### 避免意图混乱

使用从训练示例中提取的字符和单词级特征对意图进行分类，具体取决于您添加到 NLU 管道中的特征化器。当不同的意图包含以相似方式排序的相同单词时，这会给意图分类器造成混淆。

### 实体和意图分割

当您希望助手的响应以用户提供的信息为条件时，通常会发生意图混淆。例如，"How do I migrate to Rasa from IBM Watson?"和"I want to migrate from Dialogflow."。

由于这些消息中的每一个都会导致不同的响应，因此您最初的方法可能是为每种迁移类型创建单独的意图，例如watson_migration 和 dialogflow_migration。但是，这些意图试图实现相同的目标（迁移到 Rasa）并且可能会用类似的措辞，这可能会导致模型混淆这些意图。

为避免意图混淆，请将这些训练示例分组为单个`migration`意图并做出响应取决于来自实体的分类`product`槽的值。这也使得在没有提供实体时处理这种情况变得容易，例如“How do I migrate to Rasa?”例如：

```
stories:
- story: migrate from IBM Watson
  steps:
    - intent: migration
      entities:
      - product
    - slot_was_set:
      - product: Watson
    - action: utter_watson_migration

- story: migrate from Dialogflow
  steps:
    - intent: migration
      entities:
      - product
    - slot_was_set:
      - product: Dialogflow
    - action: utter_dialogflow_migration

- story: migrate from unspecified
  steps:
    - intent: migration
    - action: utter_ask_migration_product
```

### 提高实体识别

使用 Rasa Open Source，您可以定义自定义实体并在训练数据中对其进行注释，以教您的模型识别它们。 Rasa Open Source 还提供了提取预训练实体的组件，以及其他形式的训练数据，以帮助您的模型识别和处理实体。

- **预训练的实体提取器**

姓名、地址和城市等常见实体需要大量训练数据才能使 NLU 模型有效泛化。
Rasa open source提供了两个很好的预训练提取选项：SpacyEntityExtractor 和 DucklingEntityExtractor。因为这些提取器已经在大量数据上进行了预训练，所以您可以使用它们来提取它们支持的实体，而无需在训练数据中对其进行注释。

- **正则表达式**


正则表达式可用于对结构化模式（例如5位美国邮政编码）执行实体提取。正则表达式模式可用于生成 NLU 模型学习的特征，或作为直接实体匹配的方法。有关详细信息，请参阅正则表达式功能。

- **查找表**

查找表作为正则表达式模式处理，用于检查训练示例中是否存在任何查找表条目。与正则表达式类似，查找表可用于为模型提供特征以改进实体识别，或用于执行基于匹配的实体识别。查找表的有用应用示例包括冰淇淋口味、瓶装水品牌，甚至袜子长度样式

- **同义词**


将同义词添加到您的训练数据对于将某些实体值映射到单个规范化实体很有用。然而，同义词并不意味着提高模型的实体识别并且对 NLU 性能没有影响。

同义词的一个很好的用例是在对属于不同组的实体进行规范化时。例如，在询问用户他们感兴趣的保险政策时，他们可能会回答`my truck, a car, I drive a batmobile`。将`truck,car, batmobile`映射到标准化值`auto`是一个好主意，这样处理逻辑只需要考虑一组狭窄的可能性

### 处理边缘情况

- **拼写错误**

遇到拼写错误是不可避免的，因此您的机器人需要一种有效的方法来处理这个问题。请记住，目标不是纠正拼写错误，而是正确识别意图和实体。出于这个原因，虽然拼写检查器似乎是一个显而易见的解决方案，但调整您的特征化器和训练数据通常足以解决拼写错误。

添加字符级特征化器通过考虑部分单词而不是只有整个单词。您可以使用 CountVectorsFeaturizer 的 char_wb 分析器将字符级特征化添加到管道中，例如
```yml
pipeline:
# <other components>
- name: CountVectorsFeaturizer
  analyze: char_wb
  min_ngram: 1
  max_ngram: 4
# <other components>
```
除了字符级特征化之外，您还可以将常见的拼写错误添加到训练数据中。

- **定义范围外的意图**


在您的机器人中定义 out_of_scope 意图以捕获机器人域之外的任何用户消息始终是一个好主意。当识别出 out_of_scope 意图时，您可以回复诸如“我不确定如何处理，这里有一些您可以问我...”之类的消息，以优雅地引导用户获得支持的技能。

### 控制更新

像对待代码一样对待您的数据。就像你永远不会在没有审查的情况下发布代码更新一样，应该仔细审查你的训练数据的更新，因为它会对你的模型的性能产生重大影响。

使用版本控制系统，如 Github 或 Bitbucket 来跟踪更改您的数据并在必要时回滚更新。

确保为您的 NLU 模型构建测试，以评估训练数据和超参数变化时的性能。在 Jenkins 或 Git Workflow 等 CI 管道中自动执行这些测试，以简化您的开发流程并确保仅发布高质量的更新。

## 书写对话数据

对话数据包括构成 Rasa 助手对话管理模型的训练数据的故事和规则。精心编写的对话数据使您的助手能够可靠地遵循您布置的对话路径并推广到意想不到的路径。

### 设计stories

在设计故事时，需要考虑两组对话交互：快乐和不快乐的路径。愉快的路径描述了用户何时按照您的预期遵循对话流程，并在出现提示时始终提供必要的信息。但是，用户通常会因问题、闲聊或其他问题而偏离愉快的道路。我们称之为不愉快的路径。

对于你的机器人来说，优雅地处理不愉快的路径很重要，但也无法预测给定用户可能采取的路径。通常，开发人员在设计不愉快的路径时会尝试考虑每条可能的分歧路径。为状态机中的每个可能状态（其中许多永远不会达到）进行规划需要大量额外的工作并显着增加训练时间。

相反，我们建议在设计不愉快的路径时采用对话驱动的开发方法。对话驱动开发促进尽早与测试用户共享您的机器人并收集真实的对话数据，准确地告诉您用户如何偏离快乐的路径。从这些数据中，您可以创建故事来完成用户的请求，并开始思考如何引导他们回到幸福的道路上。 

### 何时写规则与stories

规则是对话管理器用于处理应始终遵循相同路径的对话片段的一种训练数据。

在实施时，规则可能很有用：

```
1.单轮交互：某些消息不需要任何上下文来回答它们。规则是将意图映射到响应的简单方法，指定这些消息的固定答案。
2.回退行为：结合 FallbackClassifier，您可以编写规则以响应具有特定回退行为的低置信度用户消息。
3.表单：两者激活和提交表单通常会遵循固定的路径。您还可以编写规则来处理表单期间的意外输入。
```

由于规则不会推广到看不见的对话，因此您应该将它们保留用于单轮对话片段，并使用故事来训练多轮对话。

机器人返回固定响应“utter_greet”到用户消息的规则示例意图“问候”将是：

```yml
rules:
- rule: Greeting Rule
  steps:
  - intent: greet
  - action: utter_greet
```

对于多轮交互，您应该定义一个故事，例如：

```yml
stories:
 - story: Greeting and ask user how they're doing
   steps:
   - intent: greet
   - action: utter_greet
   - action: utter_ask_how_doing
   - intent: doing_great
   - action: utter_happy
```

### 管理对话流程

以下是管理故事中对话流的一些提示

#### 平衡是否使用插槽来影响对话

插槽充当您的机器人的内存。当你定义一个槽时，你可以定义一个槽是否应该影响对话。属性`influence_conversation` 设置为 `false` 的槽只能存储信息。属性 `influence_conversation` 设置为 `true` 的插槽可以根据存储在其中的信息影响对话流。

需要将影响对话的插槽添加到您的故事或规则中。如果插槽是由自定义操作设置的，这也适用于这种情况。例如，您可以使用自定义操作设置的布尔槽来根据其值控制对话流，使用以下故事：

```yml
stories:
- story: Welcome message, premium user
  steps:
   - intent: greet
   - action: action_check_profile
   - slot_was_set:
     - premium_account: true
   - action: utter_welcome_premium

- story: Welcome message, basic user
  steps:
   - intent: greet
   - action: action_check_profile
   - slot_was_set:
     - premium_account: false
   - action: utter_welcome_basic
   - action: utter_ask_upgrade
```

如果您不希望槽影响对话流，则应将槽的属性 `influence_conversation` 设置为 `false`。您不需要在不影响对话的故事中包含 `slot_was_set` 事件。

#### 实现分支逻辑

在编写故事时，有时下一个操作将取决于您的一个自定义操作中返回的值。在这些情况下，重要的是要在返回槽和使用自定义操作代码直接影响您的机器人下一步做什么之间找到适当的平衡。

如果一个值仅用于确定机器人的响应，请考虑将决策逻辑嵌入到自定义行动，而不是在故事中使用特色槽。这有助于降低整体复杂性并使您的故事更易于管理。

例如，您可以转换这些故事：

```yml
stories:
- story: It's raining now
  steps:
  - intent: check_for_rain
  - action: action_check_for_rain
  - slot_was_set:
    - raining: true
  - action: utter_is_raining
  - action: utter_bring_umbrella

- story: It isn't raining now
  steps:
  - intent: check_for_rain
  - action: action_check_for_rain
  - slot_was_set:
    - raining: false
  - action: utter_not_raining
  - action: utter_no_umbrella_needed
```

将其变为单独的故事

```yml
stories:
- story: check for rain
  steps:
  - intent: check_for_rain
  - action: action_check_for_rain
```

自定义动作代码

```python
def run(self, dispatcher, tracker, domain):
    is_raining = check_rain()
    if is_raining:
        dispatcher.utter_message(template="utter_is_raining")
        dispatcher.utter_message(template="utter_bring_umbrella")
    else:
        dispatcher.utter_message(template="utter_not_raining")
        dispatcher.utter_message(template="utter_no_umbrella_needed")
    return []
```

如果该值用于影响未来的动作流，则返回一个特征化槽以确定故事。例如，如果您想收集有关新用户的信息，而不是返回用户的信息，您的故事可能如下所示：

```yml
stories:
- story: greet new user
  steps:
  - intent: greet
  - action: check_user_status
  - slot_was_set:
    - new_user: true
  - action: utter_greet
  - action: new_user_form
  - active_loop: new_user_form
  - active_loop: null

- story: greet returning user
  steps:
  - intent: greet
  - action: check_user_status
  - slot_was_set:
    - new_user: false
  - action: utter_greet
  - action: utter_how_can_help
```

#### 使用 OR 语句和检查点

OR 语句和检查点可用于减少您必须编写的故事的数量。但是，应谨慎使用它们。过度使用 OR 语句或检查点会减慢训练速度，创建过多的检查点会使您的故事难以理解。

- OR语句

在您的机器人以相同方式处理不同意图的故事中，您可以使用 OR 语句作为创建新故事的替代方法。

例如，您可以合并这两个故事：

```yml
stories:
- story: newsletter signup
  steps:
  - intent: signup_newsletter
  - action: utter_ask_confirm_signup
  - intent: affirm
  - action: action_signup_newsletter

- story: newsletter signup, confirm via thanks
  steps:
  - intent: signup_newsletter
  - action: utter_ask_confirm_signup
  - intent: thanks
  - action: action_signup_newsletter
```

变为一个带OR语句的故事

```yml
stories:
- story: newsletter signup with OR
  steps:
  - intent: signup_newsletter
  - action: utter_ask_confirm_signup
  - or:
    - intent: affirm
    - intent: thanks
  - action: action_signup_newsletter
```

在训练时，这个故事将被分成两个原始故事。

- 检查点

检查点对于将您的故事模块化成经常重复的单独块很有用。例如，如果您希望您的机器人在每个对话流结束时询问用户反馈，您可以使用检查点来避免在每个故事结束时包含反馈交互：

```yml
stories:
- story: beginning of conversation
  steps:
  - intent: greet
  - action: utter_greet
  - intent: goodbye
  - action: utter_goodbye
  - checkpoint: ask_feedback

- story: user provides feedback
  steps:
  - checkpoint: ask_feedback
  - action: utter_ask_feedback
  - intent: inform
  - action: utter_thank_you
  - action: utter_anything_else

- story: user doesn't have feedback
  steps:
  - checkpoint: ask_feedback
  - action: utter_ask_feedback
  - intent: deny
  - action: utter_no_problem
  - action: utter_anything_else
```

#### 在stories中创建逻辑中断

在设计对话流时，通常很容易创建长故事示例，从头到尾捕捉完整的对话交互。在许多情况下，这将增加考虑分支路径所需的训练故事的数量。相反，考虑将较长的故事分成处理子任务的较小对话块。

处理丢失信用卡的快乐路径故事可能如下所示：

```yml
stories:
- story: Customer loses a credit card, reviews transactions, and gets a new card
  steps:
  - intent: card_lost
  - action: check_transactions
  - slot_was_set:
    - reviewed_transactions: ["starbucks"]
  - action: utter_ask_fraudulent_transactions
  - intent: inform
  - action: action_update_transactions
  - intent: affirm
  - action: utter_confirm_transaction_dispute
  - action: utter_replace_card
  - action: mailing_address_form
  - active_loop: mailing_address
  - active_loop: null
  - action: utter_sent_replacement
  - action: utter_anything_else
  - intent: affirm
  - action: utter_help
```

处理丢失的信用卡涉及一系列子任务，即检查欺诈交易的消费历史，确认替换卡的邮寄地址，然后跟进用户的任何其他请求。在此对话弧中，机器人在多个地方提示用户输入，创建需要考虑的分支路径。

例如，当提示“utter_ask_fraudulent_transactions”时，如果没有，用户可能会以“拒绝”意图响应适用的。当被问及机器人是否可以帮助他们时，用户还可以选择以“拒绝”意图进行响应。

我们可以将这个长篇故事分成几个较小的故事：

```yml
stories:
- story: Customer loses a credit card
  steps:
  - intent: card_lost
  - action: utter_card_locked
  - action: spending_history_form
  - active_loop: spending_history_form
  - active_loop: null
  - slot_was_set:
    - reviewed_transactions: ["starbucks"]
  - action: utter_ask_fraudulent_transactions

- story: Customer reviews transactions and gets a new card
  steps:
  - action: utter_ask_fraudulent_transactions
  - intent: inform
  - action: action_update_transactions
  - slot_was_set:
    - reviewed_transactions: ["target", "starbucks"]
  - intent: affirm
  - action: utter_confirm_transaction_dispute
  - action: utter_replace_card
  - action: mailing_address_form
  - active_loop: mailing_address
  - active_loop: null
  - action: utter_sent_replacement
  - action: utter_anything_else

- story: Customer has something else they need help with
  steps:
  - action: utter_anything_else
  - intent: affirm
  - action: utter_help
```

### 处理上下文切换

通常，用户不会用你向他们询问的信息做出回应，而是会用不相关的问题偏离快乐的道路。使用 CDD 了解您的用户正在采取哪些不愉快的路径，您可以创建故事来处理上下文切换。

- 使用规则

在某些情况下，例如单轮对话打断，您可以使用规则而不是通过故事来处理上下文切换。

默认情况下，经过单轮对话打断的多轮对话表单将继续保持活动状态并重新提示必要的信息，而无需创建额外的培训故事。

```yml
rules:
- rule: Check my account balance
  steps:
  - intent: check_account_balance
  - action: action_get_account_balance
```

- 使用stories

当用户的打断需要多次对话时，您需要编写额外的故事来处理上下文切换。如果您有两个不同的会话流并希望用户能够在流之间切换，则需要创建故事来指定切换将如何发生以及如何维护上下文。

```yml
stories:
- story: Context switch from credit card payment to money transfer
  steps:
  - intent: pay_credit_card
  - action: credit_card_payment_form
  - active_loop: credit_card_payment_form
  - intent: transfer_money                         # - user requests a money transfer
  - active_loop: null                              # - deactivate the credit card form
  - action: transfer_money_form                    # - switch to the money transfer form
  - active_loop: transfer_money_form
  - active_loop: null
  - action: utter_continue_credit_card_payment     # - once the money transfer is completed,
                                                   #   ask the user to return to the
                                                   #   credit card payment form
```

### 管理对话数据文件

您可以将训练数据作为单个文件或作为包含多个文件的目录提供给 Rasa Open Source。在编写故事和规则时，通常最好根据所代表的对话类型创建单独的文件。

例如，您可以创建一个文件 chitchat.yml 用于处理 chitchat，并创建一个 faqs.yml 文件用于常见问题解答。有关复杂助手中故事文件管理的示例，请参阅我们的 rasa-demo 机器人。

### 使用互动学习

通过与您的机器人交谈并提供反馈，交互式学习可以轻松编写故事。这是探索您的机器人可以做什么的强大方法，也是修复它所犯任何错误的最简单方法。基于机器学习的对话的一个优点是，当您的机器人还不知道如何做某事时，您可以教它！

在 Rasa Open Source 中，您可以使用 rasa interactive 在命令行中运行交互式学习。 Rasa X 提供了交互式学习的 UI，您可以使用任何用户对话作为起点。请参阅 Rasa X 文档中的与您的机器人交谈。

- 命令行交互学习

CLI 命令 rasa interactive 将在命令行启动交互式学习。如果您的机器人具有自定义操作，请确保还在单独的终端窗口中运行您的操作服务器。

在交互模式下，您将被要求在机器人继续之前确认每个意图和动作预测。

```
? Next user input:  hello

? Is the NLU classification for 'hello' with intent 'hello' correct?  Yes

------
Chat History

 #    Bot                        You
────────────────────────────────────────────
 1    action_listen
────────────────────────────────────────────
 2                                    hello
                         intent: hello 1.00
------

? The bot wants to run 'utter_greet', correct?  (Y/n)
```

您将能够在对话的每个步骤中查看对话历史记录和槽值。

如果您键入 y 以批准预测，机器人将继续。如果您输入 n，您将有机会在继续之前更正预测

```
? What is the next action of the bot?  (Use arrow keys)
 » <create new action>
   1.00 utter_greet
   0.00 ...
   0.00 action_back
   0.00 action_deactivate_loop
   0.00 action_default_ask_affirmation
   0.00 action_default_ask_rephrase
   0.00 action_default_fallback
   0.00 action_listen
   0.00 action_restart
   0.00 action_revert_fallback_events
   0.00 action_session_start
   0.00 action_two_stage_fallback
   0.00 utter_cheer_up
   0.00 utter_did_that_help
   0.00 utter_goodbye
   0.00 utter_happy
   0.00 utter_iamabot
```

在任何时候，您都可以使用 Ctrl-C 访问菜单，从而允许您创建更多故事并从您迄今为止创建的故事中导出数据。

```
? Do you want to stop?  (Use arrow keys)
 » Continue
   Undo Last
   Fork
   Start Fresh
   Export & Quit
```

