# 文本分析

文本分析和NLP是现代人工智能系统不可分割的一部分。NLP最常用的领域包括：搜索引擎、情感分析、主题建模、词性标注、实体识别等。

## NLTK

[官网](http://www.nltk.org/index.html)

NTLK是著名的Python自然语言处理工具包，但是主要针对的是英文处理。

应用：

文本提取、词汇切、词频分析、词袋模型、情感分析

- 安装

[参考](http://nltk.org/install.html)

下载NLTK包 

```
pip install nltk
```

运行Python，并输入下面的指令

```python
 import nltk
 nltk.download()  # 下载语料库
```

在弹出的窗口，建议安装所有的包

- 语料库的使用

```python
import nltk
# 需要下载brown语料库
nltk.download('brown')

# nltk的都语料库包含在nltk.corpus中
from nltk.corpus import brown 
# 引用布朗大学的语料库

# 查看语料库包含的类别
print(brown.categories())

# 查看brown语料库
print('共有{}个句子'.format(len(brown.sents())))
print('共有{}个单词'.format(len(brown.words())))
```

- 语言检测

```python
from __future__ import print_function

from langdetect import detect, detect_langs

if __name__ == '__main__':
    # Simple language detection
    print(detect('This is English'))
    print(detect('Dies ist Deutsch'))

    # Probabilistic language detection
    print(detect_langs('I really love you mon doux amour!'))
```

## 分词

tokenize

```
- 将句子拆分成 具有语言语义学上有意义的词
- 中、英文分词区别：
  - 英文中，单词之间是以空格作为自然分界符的
  - 中文中没有一个形式上的分界符，分词比英文复杂的多
- 中文分词工具，如：结巴分词 pip install jieba
- 得到分词结果后，中英文的后续处理没有太大区别
```

- 英文分词

例1

```python
import nltk
# 需要事先安装 punkt 分词模型
nltk.download('punkt')

text = "Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action."

# 句子解析器
from nltk.tokenize import sent_tokenize

sent_tokenize_list = sent_tokenize(text)
print("Sentence tokenizer:", sent_tokenize_list)

# 单词解析器
# 最基本的单词解析器
from nltk.tokenize import word_tokenize

print("Word tokenizer:", word_tokenize(text))

# Punktword单词解析器，以标点符号分割文本，如果是单词中的标点符号，则保留不做分割
from nltk.tokenize import PunktWordTokenizer

punkt_word_tokenizer = PunktWordTokenizer()
print("Punkt word tokenizer:", punkt_word_tokenizer.tokenize(text))

# wordPunct单词解析器，将标点符号保留到不同的句子标记中
from nltk.tokenize import WordPunctTokenizer

word_punct_tokenizer = WordPunctTokenizer()
print("Word punct tokenizer:", word_punct_tokenizer.tokenize(text))

```

例2

```python
# coding=utf-8

from __future__ import print_function

from nltk.tokenize import sent_tokenize
from nltk.tokenize import TreebankWordTokenizer  
from nltk.tokenize import RegexpTokenizer

if __name__ == '__main__':
    # Sentence tokenizing
    print('Generic text:')
    generic_text = 'Lorem ipsum dolor sit amet, amet minim temporibus in sit. Vel ne impedit consequat intellegebat.'
    print(sent_tokenize(generic_text))

    print('English text:')
    english_text = 'Where is the closest train station? I need to reach London'
    print(sent_tokenize(english_text, language='english'))

    print('Spanish text:')
    spanish_text = u'¿Dónde está la estación más cercana? Inmediatamente me tengo que ir a Barcelona.'
    for sentence in sent_tokenize(spanish_text, language='spanish'):
        print(sentence)

    # Word tokenizing
    # Create a Treebank word tokenizer
    tbwt = TreebankWordTokenizer()

    print('Simple text:')
    simple_text = 'This is a simple text.'
    print(tbwt.tokenize(simple_text))

    print('Complex text:')
    complex_text = 'This isn\'t a simple text'
    print(tbwt.tokenize(complex_text))

    # Create a Regexp tokenizer
    ret = RegexpTokenizer('[a-zA-Z0-9\'\.]+')
    print(ret.tokenize(complex_text))

    # Create a more restrictive Regexp tokenizer
    ret = RegexpTokenizer('[a-zA-Z\']+')

    complex_text = 'This isn\'t a simple text. Count 1, 2, 3 and then go!'
    print(ret.tokenize(complex_text))
```

- 中文分词

```python
# 导入jieba分词
import jieba

# 全模式
seg_list = jieba.cut("我来到清华大学", cut_all=True)
print("全模式: " + "/ ".join(seg_list))  
# 精确模式
seg_list = jieba.cut("我来到清华大学", cut_all=False)
print("精确模式: " + "/ ".join(seg_list))  
# 搜索引擎模式
seg_list = jieba.cut_for_search("小明硕士毕业于中国科技大学，后在美国斯坦福大学深造")
print("搜索引擎模式: "+"/".join(seg_list))
```

## 去除停用词

- 为节省存储空间和提高搜索效率，NLP中会自动过滤掉某些字或词

- 停用词都是人工输入、非自动化生成的，形成停用词表

- 分类

  > 语言中的功能词，如the, is…
  >
  > 词汇词，通常是使用广泛的词，如want

- 中文停用词表

  > 中文停用词库
  >
  > 哈工大停用词表
  >
  > 四川大学机器智能实验室停用词库
  >
  > 百度停用词列表

- 其他语言停用词表

  > <http://www.ranks.nl/stopwords>

- 使用NLTK去除停用词

  > stopwords.words()

```python
import nltk
# 需要下载stopwords
nltk.download('stopwords')

from nltk.corpus import stopwords 

filtered_words = [word for word in words if word not in stopwords.words('english')]
print('原始词：', words)
print('去除停用词后：', filtered_words)

# 运行结果：
# 原始词： ['Python', 'is', 'a', 'widely', 'used', 'programming', 'language', '.']
# 去除停用词后： ['Python', 'widely', 'used', 'programming', 'language', '.']
```

## 词形问题

- 同词不同形：look, looked, looking
- 影响语料学习的准确度
- 词形归一化

### 词干提取

stemming

- 英文

处理文本文档时，可能会碰到单词的不同形式。在文本分析中，提取这些单词的原形非常有用，有助于提取一些统计信息来分析整个文本。词干提取的目的是将不同词形的单词都变为其原形。词干提取使用启发式处理方法截取单词的尾部，以提取单词的原形。

NLTK中常用的stemmer：`PorterStemmer, SnowballStemmer, LancasterStemmer`，其中`Porter`提取规则最宽松，`Lancaster`提取规则最严格，会造成单词模糊难以理解，故常用`Snowball`.

```python
# PorterStemmer
from nltk.stem.porter import PorterStemmer

porter_stemmer = PorterStemmer()
print(porter_stemmer.stem('looked'))
print(porter_stemmer.stem('looking'))


# SnowballStemmer
from nltk.stem import SnowballStemmer

snowball_stemmer = SnowballStemmer('english')
print(snowball_stemmer.stem('looked'))
print(snowball_stemmer.stem('looking'))


# LancasterStemmer
from nltk.stem.lancaster import LancasterStemmer

lancaster_stemmer = LancasterStemmer()
print(lancaster_stemmer.stem('looked'))
print(lancaster_stemmer.stem('looking'))

```

- 中文关键词提取

jeiba实现了TF_IDF和TextRank两种关键词提取算法，直接调用即可。这里的关键词前提是中文分词，会使用jieba自带的前缀词典和IDF权重字典

```python
import jieba.analyse

# 字符串前面加u表示使用unicode编码
content = u'十八大以来，国内外形势变化和我国各项事业发展都给我们提出了一个重大时代课题，这就是必须从理论和实践结合上系统回答新时代坚持和发展什么样的中国特色社会主义、怎样坚持和发展中国特色社会主义，包括新时代坚持和发展中国特色社会主义的总目标、总任务、总体布局、战略布局和发展方向、发展方式、发展动力、战略步骤、外部条件、政治保证等基本问题，并且要根据新的实践对经济、政治、法治、科技、文化、教育、民生、民族、宗教、社会、生态文明、国家安全、国防和军队、“一国两制”和祖国统一、统一战线、外交、党的建设等各方面作出理论分析和政策指导，以利于更好坚持和发展中国特色社会主义。'

# 参数1：待提取关键词的文本，参数2：返回关键词的数量，重要性从高到低排序
# 参数3：是否同时返回每个关键词的权重，参数4：词性过滤，为空表示不过滤，若提供则仅返回符合词性要求的关键词
keywords = jieba.analyse.extract_tags(content, topK=20, withWeight=True, allowPOS=())
# 访问提取结果
for item in keywords:
	# 分别为关键词和响应的权重
    print item[0], item[1]
    
# 同样式四个参数，但allowPOS默认为('ns','n','vn','v'),即仅提取地名、名词、动名词、动词
keywords = jieba.analyse.textrank(content, topK=20, withWeight=True, allowPOS=('ns','n','vn','v'))
# 访问提取结果
for item in keywords:
    # 分别为关键词和响应的权重
    print item[0], item[1]
```

### 词形归并

lemmatization

词形归并的目标也是将单词转换为其原形，但它是一个更结构化的方法。若用词干提取技术提取`wolves`，则结果`wolv`不是一个有意义的单词。词形归并通过对单词进行词汇和语法分析来实现，故可解决上述问题，得到结果`wolf`。

lemmatization，词形归并，将单词的各种词形归并成一种形式，如am, is, are -> be, went->go

NLTK中的lemma：`WordNetLemmatizer`，其中指明词性可以更准确地进行lemma

```python
import nltk
# 需要下载wordnet语料库
nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer 


wordnet_lematizer = WordNetLemmatizer()
# lemmatize 默认为名词n
print(wordnet_lematizer.lemmatize('cats'))
print(wordnet_lematizer.lemmatize('boxes'))
print(wordnet_lematizer.lemmatize('are'))
print(wordnet_lematizer.lemmatize('went'))

# 运行结果：
# cat
# box
# are
# went


# 指明词性可以更准确地进行lemma
print(wordnet_lematizer.lemmatize('are', pos='v'))
print(wordnet_lematizer.lemmatize('went', pos='v'))

# 运行结果：
# be
# go
```

## 向量化

```python
from __future__ import print_function

import numpy as np

from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# For reproducibility
np.random.seed(1000)

ret = RegexpTokenizer('[a-zA-Z0-9\']+')
sw = set(stopwords.words('english'))
ess = SnowballStemmer('english', ignore_stopwords=True)


def tokenizer(sentence):
    tokens = ret.tokenize(sentence)
    return [ess.stem(t) for t in tokens if t not in sw]


if __name__ == '__main__':
    # Create a corpus
    corpus = [
        'This is a simple test corpus',
        'A corpus is a set of text documents',
        'We want to analyze the corpus and the documents',
        'Documents can be automatically tokenized'
    ]

    # Create a count vectorizer
    # 计数向量化
    print('Count vectorizer:')
    cv = CountVectorizer()

    vectorized_corpus = cv.fit_transform(corpus)
    print(vectorized_corpus.todense())

    print('CV Vocabulary:')
    print(cv.vocabulary_)

    # Perform an inverse transformation
    vector = [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1]
    print(cv.inverse_transform(vector))

    # Use a complete external tokenizer
    print('CV with external tokenizer:')
    cv = CountVectorizer(tokenizer=tokenizer)
    vectorized_corpus = cv.fit_transform(corpus)
    print(vectorized_corpus.todense())

    # Use an n-gram range equal to (1, 2)
    # N元模型
    print('CV witn n-gram range (1, 2):')
    cv = CountVectorizer(tokenizer=tokenizer, ngram_range=(1, 2))
    vectorized_corpus = cv.fit_transform(corpus)
    print(vectorized_corpus.todense())

    print('N-gram range (1,2) vocabulary:')
    print(cv.vocabulary_)

    # Create a Tf-Idf vectorizer
    # TF-IDF向量化
    print('Tf-Idf vectorizer:')
    tfidfv = TfidfVectorizer()
    vectorized_corpus = tfidfv.fit_transform(corpus)
    print(vectorized_corpus.todense())

    print('Tf-Idf vocabulary:')
    print(tfidfv.vocabulary_)

    # Use n-gram range equal to (1, 2) and L2 normalization
    print('Tf-Idf witn n-gram range (1, 2) and L2 normalization:')
    tfidfv = TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1, 2), norm='l2')
    vectorized_corpus = tfidfv.fit_transform(corpus)
    print(vectorized_corpus.todense())


```

## 词性

- 英文

```python
from nltk import word_tokenize, pos_tag, ne_chunk, tree2conlltag

if __name__ == '__main__':
    sentence_1 = 'My friend John lives in Paris'

    # Perform a POS tagging
    # 词性标注
    tokens_1 = word_tokenize(sentence_1)
    tags_1 = pos_tag(tokens_1)

    print(sentence_1)
    print(tags_1)

    # Peform a POS and NER tagging
    # 命名实体识别(NER)
    sentence_2 = 'Search a hotel in Cambridge near the MIT'

    tokens_2 = word_tokenize(sentence_2)
    tags_2 = pos_tag(tokens_2)

    print('\n')
    print(sentence_2)
    print(tree2conlltags(ne_chunk(tags_2)))

```

- 中文

jie在进程中文分词的同时，可以完成词性标注任务。根据分词结果中每个词的词性，可以初步实现命名实体识别，即将标注为nr的词视为人名，将标注为ns的词视为地名等。所有标点符号都被标注为x，因此可以根据这个方法去除分词结果中的标点符号

```python
# 加载jie.posseg并取个别名，方便调用
import jieba.posseg as pseg
words = pseg.cut("我爱北京天安门")
for word, flag in words:
    # 格式化模板并传入参数
    print('%s, %s' % (word, flag)) 
```

## 分块划分文本

分块是指基于任意随机条件将输入文本分割成块。与标记解析不同的是，分块没有条件约束，分块的结果不需要有实际意义。当处理非常大的文本文档时，就需要将文本进行分块，以便进行下一步分析。

```python
import numpy as np
from nltk.corpus import brown

# 将文本分割成块
def splitter(data, num_words):
    words = data.split(' ')
    output = []

    # 初始化变量
    cur_count = 0
    cur_words = []
    # 对单词进行迭代
    for word in words:
        cur_words.append(word)
        cur_count += 1
        # 获得的单词数量与所需的单词数量相等时，重置相应变量
        if cur_count == num_words:
            output.append(' '.join(cur_words))
            cur_words = []
            cur_count = 0

    # 将块添加到输出变量列表的最后
    output.append(' '.join(cur_words) )

    return output 

if __name__=='__main__':
    # 从布朗语料库加载数据
    data = ' '.join(brown.words()[:10000])

    # 定义每块包含的单词数目 
    num_words = 1700

    chunks = []
    counter = 0
	# 调用分块逻辑
    text_chunks = splitter(data, num_words)
    print("Number of text chunks =", len(text_chunks))

```

## 词袋模型

如果要处理包含数百万单词的文本文档，需要将其转化成某种数值表示形式，以便让机器用这些数据来学习算法。这些算法需要数值数据，以便可以对这些数据进行分析，并输出有用的信息。这里需要用到词袋(bag-of-words)。词袋是从所有文档的所有单词中学习词汇的模型。学习之后，词袋通过构建文档中所有单词的直方图来对每篇文档进行建模

```python
import numpy as np
from nltk.corpus import brown
from chunking import splitter

if __name__ == '__main__':
    # 加载布朗语料库数据
    data = ' '.join(brown.words()[:10000])

    # 将文本按块划分
    num_words = 2000
    chunks = []
    counter = 0
    text_chunks = splitter(data, num_words)

    # 创建基于文本块的词典
    for text in text_chunks:
        chunk = {'index': counter, 'text': text}
        chunks.append(chunk)
        counter += 1

    # 提取一个文档-词矩阵：记录文档中每个单词出现的频次
    # 使用sklearn而不是nltk，由于sklearn更简洁
    from sklearn.feature_extraction.text import CountVectorizer

    vectorizer = CountVectorizer(min_df=5, max_df=.95)
    doc_term_matrix = vectorizer.fit_transform([chunk['text'] for chunk in chunks])

    vocab = np.array(vectorizer.get_feature_names())
    print("Vocabulary:", vocab)

    print("Document term matrix:")
    chunk_names = ['Chunk-0', 'Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4']
    formatted_row = '{:>12}' * (len(chunk_names) + 1)  # 表格样式
    print(formatted_row.format('Word', *chunk_names))
    for word, item in zip(vocab, doc_term_matrix.T):
        # 'item'是压缩的稀疏矩阵(csr_matrix)数据结构 
        output = [str(x) for x in item.data]
        print(formatted_row.format(word, *output))

```

## 文本分类器

文本分类的目的是将文本文档分为不同的类，这里使用一种`tf-idf`的统计方法，表示词频-逆文档频率(`term frequency-inverse document frequency`)。这个统计工具有助于理解一个单词在一组文档中对某一个文档的重要性。可以作为特征向量来做文档分类。

```python
from sklearn.datasets import fetch_20newsgroups

# 创建一个类型列表，用词典映射的方式定义
category_map = {'misc.forsale': 'Sales', 'rec.motorcycles': 'Motorcycles',
                'rec.sport.baseball': 'Baseball', 'sci.crypt': 'Cryptography',
                'sci.space': 'Space'}
# 基于定义的类型加载训练数据
training_data = fetch_20newsgroups(subset='train', categories=category_map.keys(), shuffle=True, random_state=7)

# 特征提取
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
X_train_termcounts = vectorizer.fit_transform(training_data.data)
print("Dimensions of training data:", X_train_termcounts.shape)

# 训练分类器
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfTransformer

input_data = [
    "The curveballs of right handed pitchers tend to curve to the left",
    "Caesar cipher is an ancient form of encryption",
    "This two-wheeler is really good on slippery roads"
]

# 定义tf-idf变换器对象并训练 
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_termcounts)

# 得到特征向量，使用该数据训练多项式朴素贝叶斯分类器 
classifier = MultinomialNB().fit(X_train_tfidf, training_data.target)
# 用词频统计转换输入数据
X_input_termcounts = vectorizer.transform(input_data)
# 用tf-idf变换器变换输入数据
X_input_tfidf = tfidf_transformer.transform(X_input_termcounts)

# 预测输入句子的输出类型 
predicted_categories = classifier.predict(X_input_tfidf)

for sentence, category in zip(input_data, predicted_categories):
    print('Input:', sentence, 'Predicted category:', category_map[training_data.target_names[category]])

```

## 性别识别

通过姓名识别性别，这里使用启发式方法，即姓名的最后几个字符可以界定性别特征。

```python
import random
from nltk.corpus import names
from nltk import NaiveBayesClassifier
from nltk.classify import accuracy as nltk_accuracy


# 提取输入单词的特性
def gender_features(word, num_letters=2):
    return {'feature': word[-num_letters:].lower()}


if __name__ == '__main__':
    # 提取标记名称
    labeled_names = ([(name, 'male') for name in names.words('male.txt')] +
                     [(name, 'female') for name in names.words('female.txt')])

    # 设置随机种子，并混合搅乱训练数据
    random.seed(7)
    random.shuffle(labeled_names)
    input_names = ['Leonardo', 'Amy', 'Sam']

    # 搜索参数空间：由于不知需要多少个末尾字符，初步设置1～5
    for i in range(1, 5):
        print('Number of letters:', i)
        featuresets = [(gender_features(n, i), gender) for (n, gender) in labeled_names]
        # 训练集、测试集
        train_set, test_set = featuresets[500:], featuresets[:500]
        # 朴素贝叶斯分类
        classifier = NaiveBayesClassifier.train(train_set)  

        # 使用参数空间的每一个值评估分类器的效果
        print('Accuracy ==>', str(100 * nltk_accuracy(classifier, test_set)) + str('%'))

        # 预测
        for name in input_names:
            print(name, '==>', classifier.classify(gender_features(name, i)))

```

## 情感分析

情感分析是指确定一段歌诶定的文本是积极还是消极的过程。有一些场景中，会将"中性"作为第三个选项。情感分析常用于发现人们对于一个特定主题的看法。情感分析用于分析很多场景中用户的情绪，如营销活动、社交媒体、电子商务客户等。

```python
import nltk.classify.util
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import movie_reviews


# 用于提取特征
def extract_features(word_list):
    return dict([(word, True) for word in word_list])


if __name__ == '__main__':
    # 加载积极与消极评论
    positive_fileids = movie_reviews.fileids('pos')
    negative_fileids = movie_reviews.fileids('neg')
    # 将评论分成积极和消极
    features_positive = [(extract_features(movie_reviews.words(fileids=[f])),
                          'Positive') for f in positive_fileids]
    features_negative = [(extract_features(movie_reviews.words(fileids=[f])),
                          'Negative') for f in negative_fileids]

    # 训练数据(80%)、测试数据
    threshold_factor = 0.8
    threshold_positive = int(threshold_factor * len(features_positive))
    threshold_negative = int(threshold_factor * len(features_negative))

    # 提取特征
    features_train = features_positive[:threshold_positive] + features_negative[:threshold_negative]
    features_test = features_positive[threshold_positive:] + features_negative[threshold_negative:]
    print("Number of training datapoints:", len(features_train))
    print("Number of test datapoints:", len(features_test))

    # 训练朴素贝叶斯分类器
    classifier = NaiveBayesClassifier.train(features_train)
    print("Accuracy of the classifier:", nltk.classify.util.accuracy(classifier, features_test))
    print("Top 10 most informative words:")
    for item in classifier.most_informative_features()[:10]:
        print(item[0])

    # 输入一些评论进行预测
    input_reviews = [
        "It is an amazing movie",
        "This is a dull movie. I would never recommend it to anyone.",
        "The cinematography is pretty great in this movie",
        "The direction was terrible and the story was all over the place"
    ]

    print("Predictions:")
    for review in input_reviews:
        print("Review:", review)
        probdist = classifier.prob_classify(extract_features(review.split()))
        pred_sentiment = probdist.max()
        print("Predicted sentiment:", pred_sentiment)
        print("Probability:", round(probdist.prob(pred_sentiment), 2))

```

例2

```python
from __future__ import print_function

import matplotlib.pyplot as plt
import multiprocessing
import numpy as np

from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem.lancaster import LancasterStemmer

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, roc_curve, auc

# For reproducibility
np.random.seed(1000)

# Path to the dataset (http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip)
dataset = 'dataset.csv'

rt = RegexpTokenizer('[a-zA-Z0-9\.]+')
sw = set(stopwords.words('english'))
ls = LancasterStemmer()


def tokenizer(sentence):
    tokens = rt.tokenize(sentence)
    return [ls.stem(t.lower()) for t in tokens if t not in sw]


if __name__ == '__main__':
    # Load corpus and labels
    corpus = []
    labels = []

    with open(dataset, 'r', encoding='utf-8') as df:
        for i, line in enumerate(df):
            if i == 0:
                continue

            parts = line.strip().split(',')
            labels.append(float(parts[1].strip()))
            corpus.append(parts[3].strip())

    # Vectorize the corpus (only 100000 records)
    tfv = TfidfVectorizer(tokenizer=tokenizer, sublinear_tf=True, ngram_range=(1, 2), norm='l2')
    X = tfv.fit_transform(corpus[0:100000])
    Y = np.array(labels[0:100000])

    # Prepare train and test sets
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)

    # Create and train a Random Forest
    rf = RandomForestClassifier(n_estimators=20, n_jobs=multiprocessing.cpu_count())
    rf.fit(X_train, Y_train)

    # Compute scores
    print('Precision: %.3f' % precision_score(Y_test, rf.predict(X_test)))
    print('Recall: %.3f' % recall_score(Y_test, rf.predict(X_test)))

    # Compute the ROC curve
    y_score = rf.predict_proba(X_test)
    fpr, tpr, thresholds = roc_curve(Y_test, y_score[:, 1])

    plt.figure(figsize=(8, 8))
    plt.plot(fpr, tpr, color='red', label='Random Forest (AUC: %.2f)' % auc(fpr, tpr))
    plt.plot([0, 1], [0, 1], color='blue', linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.01])
    plt.title('ROC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc="lower right")
    plt.show()

```

- 使用NLTK的VADER

对于英语，NLTK提供了一个已经训练过的模型VADER，采用规则引擎和字典来推断出一文字的情感强度。

```python
from nltk.sentiment.vader import SentimentIntensityAnalyzer

if __name__ == '__main__':
    text = 'This is a very interesting and quite powerful sentiment analyzer'

    vader = SentimentIntensityAnalyzer()
    print(vader.polarity_scores(text))
```



## 主题建模

主题建模指识别文本数据隐藏模式的过程，其目的是发现一组文档的隐藏主题结构。主题建模可以更好地组织文档，以便对这些文档进行分析。

主题建模通过识别文档中最有意义、最能表征主题的词来实现主题的分类。这些单词往往可以确定主题的内容。

```python
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer
from gensim import models, corpora
from nltk.corpus import stopwords


# 加载输入数据
def load_data(input_file):
    data = []
    with open(input_file, 'r') as f:
        for line in f.readlines():
            data.append(line[:-1])

    return data


# 类预处理文本
class Preprocessor(object):
    # 对各种操作进行初始化
    def __init__(self):
        # 创建正则表达式解析器，使用正则是因为只需要那些没有标点或其他标记的单词
        self.tokenizer = RegexpTokenizer(r'\w+')

        # 获取停用词列表，使用停用词可以减少干扰
        self.stop_words_english = stopwords.words('english')

        # 创建Snowball词干提取器
        self.stemmer = SnowballStemmer('english')

    # 标记解析、移除停用词、词干提取
    def process(self, input_text):
        # 标记解析(分词)
        tokens = self.tokenizer.tokenize(input_text.lower())

        # 移除停用词
        tokens_stopwords = [x for x in tokens if not x in self.stop_words_english]

        # 词干提取
        tokens_stemmed = [self.stemmer.stem(x) for x in tokens_stopwords]

        return tokens_stemmed


if __name__ == '__main__':
    input_file = 'data_topic_modeling.txt'

    data = load_data(input_file)

    # 创建预处理对象
    preprocessor = Preprocessor()

    # 创建一组经过预处理的文档
    processed_tokens = [preprocessor.process(x) for x in data]

    # 创建基于标记文档的词典
    dict_tokens = corpora.Dictionary(processed_tokens)

    # 创建文档-词矩阵
    corpus = [dict_tokens.doc2bow(text) for text in processed_tokens]

    # 假设文档可分为2个主题，使用隐含狄利克雷分布(LDA)做主题建模
    # 基于刚刚创建的语料库生成LDA模型
    num_topics = 2
    num_words = 4
    ldamodel = models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dict_tokens, passes=25)

    print("Most contributing words to the topics:")
    for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):
        print("Topic", item[0], "==>", item[1])

```

### 隐性语义分析

```python
from __future__ import print_function

import numpy as np
import matplotlib.pyplot as plt

from nltk.corpus import brown

from scipy.linalg import svd

from sklearn.feature_extraction.text import TfidfVectorizer


# For reproducibility
np.random.seed(1000)


def scatter_documents(X):
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))

    ax.scatter(X[:, 0], X[:, 1])
    ax.set_xlabel('t0')
    ax.set_ylabel('t1')
    ax.grid()
    plt.show()


if __name__ == '__main__':
    # Compose a corpus
    sentences = brown.sents(categories=['news'])[0:500]
    corpus = []

    for s in sentences:
        corpus.append(' '.join(s))

    # Vectorize the corpus
    vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english', sublinear_tf=True, use_idf=True)
    Xc = vectorizer.fit_transform(corpus).todense()

    # Perform SVD
    U, s, V = svd(Xc, full_matrices=False)

    # Extract a sub-space with rank=2
    rank = 2

    Uk = U[:, 0:rank]
    sk = np.diag(s)[0:rank, 0:rank]
    Vk = V[0:rank, :]

    # Check the top-10 word per topic
    Mwts = np.argsort(np.abs(Vk), axis=1)[::-1]

    for t in range(rank):
        print('\nTopic ' + str(t))
        for i in range(10):
            print(vectorizer.get_feature_names()[Mwts[t, i]])

    # Compute the structure of a document
    print('\nSample document:')
    print(corpus[0])

    Mdtk = Uk.dot(sk)
    print('\nSample document in the topic sub-space:')
    print('d0 = %.2f*t1 + %.2f*t2' % (Mdtk[0][0], Mdtk[0][1]))

    # Show a scatter plot of all documents
    scatter_documents(Mdtk)

```

### 概率隐形语义分析

```python
from __future__ import print_function

import numpy as np

from nltk.corpus import brown

from sklearn.feature_extraction.text import CountVectorizer


# For reproducibility
np.random.seed(1000)

rank = 2
alpha_1 = 1000.0
alpha_2 = 10.0

# Compose a corpus
sentences_1 = brown.sents(categories=['editorial'])[0:20]
sentences_2 = brown.sents(categories=['fiction'])[0:20]
corpus = []

for s in sentences_1 + sentences_2:
    corpus.append(' '.join(s))

# Vectorize the corpus
cv = CountVectorizer(strip_accents='unicode', stop_words='english')
Xc = np.array(cv.fit_transform(corpus).todense())

# Define the probability matrices
Ptd = np.random.uniform(0.0, 1.0, size=(len(corpus), rank))
Pwt = np.random.uniform(0.0, 1.0, size=(rank, len(cv.vocabulary_)))
Ptdw = np.zeros(shape=(len(cv.vocabulary_), len(corpus), rank))

# Normalize the probability matrices
for d in range(len(corpus)):
    nf = np.sum(Ptd[d, :])
    for t in range(rank):
        Ptd[d, t] /= nf

for t in range(rank):
    nf = np.sum(Pwt[t, :])
    for w in range(len(cv.vocabulary_)):
        Pwt[t, w] /= nf


def log_likelihood():
    value = 0.0

    for d in range(len(corpus)):
        for w in range(len(cv.vocabulary_)):
            real_topic_value = 0.0

            for t in range(rank):
                real_topic_value += Ptd[d, t] * Pwt[t, w]

            if real_topic_value > 0.0:
                value += Xc[d, w] * np.log(real_topic_value)

    return value


def expectation():
    global Ptd, Pwt, Ptdw

    for d in range(len(corpus)):
        for w in range(len(cv.vocabulary_)):
            nf = 0.0

            for t in range(rank):
                Ptdw[w, d, t] = Ptd[d, t] * Pwt[t, w]
                nf += Ptdw[w, d, t]

            Ptdw[w, d, :] = (Ptdw[w, d, :] / nf) if nf != 0.0 else 0.0


def maximization():
    global Ptd, Pwt, Ptdw

    for t in range(rank):
        nf = 0.0

        for d in range(len(corpus)):
            ps = 0.0

            for w in range(len(cv.vocabulary_)):
                ps += Xc[d, w] * Ptdw[w, d, t]

            Pwt[t, w] = ps
            nf += Pwt[t, w]

        Pwt[:, w] /= nf if nf != 0.0 else alpha_1

    for d in range(len(corpus)):
        for t in range(rank):
            ps = 0.0
            nf = 0.0

            for w in range(len(cv.vocabulary_)):
                ps += Xc[d, w] * Ptdw[w, d, t]
                nf += Xc[d, w]

            Ptd[d, t] = ps / (nf if nf != 0.0 else alpha_2)


if __name__ == '__main__':
    print('Initial Log-Likelihood: %f' % log_likelihood())

    for i in range(30):
        expectation()
        maximization()
        print('Step %d - Log-Likelihood: %f' % (i, log_likelihood()))

    # Show the top 5 words per topic
    Pwts = np.argsort(Pwt, axis=1)[::-1]

    for t in range(rank):
        print('\nTopic ' + str(t))
        for i in range(5):
            print(cv.get_feature_names()[Pwts[t, i]])
```

### 隐性狄利克雷分布

```python
from __future__ import print_function

import numpy as np

from nltk.corpus import brown

from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

# For reproducibility
np.random.seed(1000)

if __name__ == '__main__':
    # Compose a corpus
    sentences_1 = brown.sents(categories=['reviews'])[0:1000]
    sentences_2 = brown.sents(categories=['government'])[0:1000]
    sentences_3 = brown.sents(categories=['fiction'])[0:1000]
    sentences_4 = brown.sents(categories=['news'])[0:1000]
    corpus = []

    for s in sentences_1 + sentences_2 + sentences_3 + sentences_4:
        corpus.append(' '.join(s))

    # Vectorize the corpus
    cv = CountVectorizer(strip_accents='unicode', stop_words='english', analyzer='word')
    Xc = cv.fit_transform(corpus)

    # Perform LDA
    lda = LatentDirichletAllocation(n_topics=8, learning_method='online', max_iter=25)
    Xl = lda.fit_transform(Xc)

    # Show the top 5 words per topic
    Mwts_lda = np.argsort(lda.components_, axis=1)[::-1]

    for t in range(8):
        print('\nTopic ' + str(t))
        for i in range(5):
            print(cv.get_feature_names()[Mwts_lda[t, i]])

    # Test the model with new document
    print('Document 0:')
    print(corpus[0])
    print(Xl[0])

    print('Document 2500:')
    print(corpus[2500])
    print(Xl[2500])

    test_doc = corpus[0] + ' ' + corpus[2500]
    y_test = lda.transform(cv.transform([test_doc]))
    print(y_test)

```

## 词嵌入

词嵌入(Word Embedding)可以将文本和词语转换为机器能够接受的数值向量

### 原理

- 语言的表示

> 符号主义

```
符号主义中典型的代表是Bag of words，即词袋模型。基于词袋模型可以方便地用一个N维向量表示任何一句话，每个维度的值即对应的词出现的次数。

优点：简单
缺点：当词典中词的数量增大时，向量的维度将随之增大；无论是词还是句子的表示，向量过于稀疏，除了少数维度外其他维度均为0；每个词对应的向量在空间上都两两正交，任意一堆向量之间的内积等数值特征为0，无法表达词语之间的语义关联和差异；句子的向量表示丢失了词序特征，"我很不高兴"和“不我很高兴”对应的向量相同
```

> 分布式表示

```
分布式表示的典型代表是Word Embedding，即词嵌入。使用低维、稠密、实值得词向量来表示每一个词，从而赋予词语丰富的语义含义，并使得计算词语相关度成为可能。两个词具有语义相关或相似，则它们对应得词向量之间距离相近，度量向量之间的距离可以使用经典的欧拉距离和余弦相似度等。

词嵌入可以将词典中的每个词映射成对应的词向量，好的词嵌入模型具有：相关性好，类比关联
```

- 训练词向量

词向量的训练主要是基于无监督学习，从大量文本语料中学习出每个词的最佳词向量。训练的核心思想是，语义相关或相似的词语，大多具有相似的上下文，即它们经常在相似的语境中出现。

词嵌入模型中的典型代表是Word2Vec

### 实现

gensim是开源python工具包，用于从非结构化文本中无监督地学习文本隐层的主题向量表示，支持包括TF-IDF,LSA,LDA和Word2Vec在内的多种主题模型算法，并提供了诸如相似度计算、信息检索等常用任务的API接口。

语料：维基百科，[搜狗新闻](http://www.sogou.com/labs/resource/cs.php)

英文案例

```python
# 加载包
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence

# 训练模型
sentences = LineSentence('wiki.zh.word.text')
# size词向量的维度，window上下文环境的窗口大小，min_count忽略出现次数低于min_count的词
model = Word2Vec(sentences, size=128, window=5， min_count=5, workers=4)

# 保存模型
model.save('word_embedding_128')

# 若已经保存过模型，则直接加载即可
# 训练及保存代码可省略
# model = Word2Vec.load('word_embedding_128')

# 使用模型
# 返回一个词语最相关的多个词语及对应的相关度
items = model.most_similar(u'中国')
for item in items:
    # 词的内容，词的相关度
    print item[0], item[1]
# 返回连个词语之间的相关度
model.similarity(u'男人', u'女人')
```

中文案例

```python
import sys
import multiprocessing
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence


def save_model():
    """
    保存模型
    :return:
    """
    if len(sys.argv) < 3:
        sys.exit(1)

    # inp表示语料分词，outp表示模型
    inp, outp = sys.argv[1:3]
    """
    Word2Vec(LineSentence(inp), size=400, window=5, min_count=5)
    # 参数
    LineSentence(inp)：把word2vec训练模型的磁盘存储文件，转换成所需要的格式,如：[[“sentence1”],[”sentence1”]]
    size：是每个词的向量维度
    window：是词向量训练时的上下文扫描窗口大小，窗口为5就是考虑前5个词和后5个词
    min-count：设置最低频率，默认是5，如果一个词语在文档中出现的次数小于5，那么就会丢弃
    # 方法：
    inp:分词后的文本
    save(outp1):保存模型
    """
    model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5, workers=multiprocessing.cpu_count())
    model.save(outp)


def predict_model():
    """
    测试模型
    :return:
    """
    """
    model = gensim.models.Word2Vec.load("*.model")
    model.most_similar('警察')
    model.similarity('男人','女人')
    most_similar(positive=['女人', '丈夫'], negative=['男人'], topn=1)
    """
    model = Word2Vec.load("./model/corpus.model")
    res = model.most_similar("警察")
    print(res)


if __name__ == '__main__':
    # 保存模型
    save_model()
    """
    终端运行python trainword2vec.py ./corpus_seg.txt ./model/*
    """
    # 测试模型
    predict_model()
```

