# 特征工程

数据预处理涉及的策略和技术非常广泛，主要技术是属性选择技术和主成分分析技术。

属性选择

```
指从数据集中选择最具代表性的属性子集，删除冗余或不相关的属性，从而提高数据处理的效率，使模型更容易理解
```

主成分分析

```
利用降维的思想，把给定的一组相关属性通过线性变换转换成另一组不相关的属性，这些新属性按照方差依次递减的顺序进行排列
主成分分析将很多个复杂属性归结为少数几个主成分，将复杂问题简单化，便于分析和处理
```

离散化

```
将连续的数值型数据切分为若干个称为分箱(bin)的小段，是数据分析中常用的手段
离散化的实质是将无限空间中有限的个体映射到有限的空间中去，其作用是提高算法的时空效率，有时也为了能够使用特定的机器学习算法而将数据离散化
```

## 特征抽取

- 英文文本

```python
# 对文本数据进行特征值化
# 类
sklearn.feature_extraction.text.CountVectorizer

# 实例化
CountVectorizer()

CountVectorizer.fit_transform(X)       
X:文本或者包含文本字符串的可迭代对象
返回词频矩阵

CountVectorizer.get_feature_names()
返回值:单词列表
```

实现

```python
from sklearn.feature_extraction.text import CountVectorizer

data = ["life is short, i like python", "lisfe is too long, i disliake python"]

# 特征抽取，抽取词频矩阵
cv = CountVectorizer()
# fit提取特征名
name = cv.fit(data)
# transform根据提取出来的特征词，统计个数
result = cv.transform(data)
# fit_transform = fit + transform
# data是文本或包含文本字符串的可迭代对象，返回词频矩阵
# result = cv.fit_transform(data)
# 返回单词列表
print(cv.get_feature_names())
# 稀疏矩阵
print(result)
# sparse矩阵转换为array数组
print(result.toarray())
```

- 字典抽取

```python
# 对字典数据进行特征值化

# 类
sklearn.feature_extraction.DictVectorizer
# 实例化
DictVectorizer(sparse=True,…)

DictVectorizer.fit_transform(X)       
X:字典或者包含字典的迭代器
返回值：返回sparse矩阵

DictVectorizer.inverse_transform(X)
X:array数组或者sparse矩阵
返回值:转换之前数据格式

DictVectorizer.get_feature_names()
返回类别名称

DictVectorizer.transform(X)
按照原先的标准转换
```

实现

```python
from sklearn.feature_extraction import DictVectorizer

data = [{'city': '北京', 'temperature': 100},
        {'city': '上海', 'temperature': 60},
        {'city': '深圳', 'temperature': 30}]
# 字典数据特征抽取，默认稀疏矩阵
dv = DictVectorizer(sparse=False)
# 提取特征名及词频
# 输入是字典或者包含字典的迭代器，返回值是sparse矩阵
result = dv.fit_transform(data)
# dv.fit(data)
# result = dv.transform(data)
# 输入是array数组后者sparse矩阵，返回值是转换之前数据格式
origin = dv.inverse_transform(result)
# 返回了表的名称
print(dv.get_feature_names())
print(result)
print(origin)
```

- one-hot

```
3种方式：
1. DictVectorizer(注意，数字不会进行转换)
2. OneHotEncoder(Numpy)
3. pd.get_dummies(Pandas)
```

实现

```python
import pandas as pd
from sklearn.feature_extraction import DictVectorizer
from sklearn.preprocessing import OneHotEncoder


df = pd.read_csv("./one_hot_test.csv")
# print(df)

# 1.DictVecotrizer
# df.gender = df.gender.map({0: "female", 1: "male", })
# print(type(df))
# print(df["gender"])  # Series对象
# print(df[["gender"]]) # DataFrame对象
# print(list(df[["gender"]].T.to_dict().values()))
# data = list(df[["gender"]].T.to_dict().values())
# dv = DictVectorizer(sparse=False)
# result = dv.fit_transform(data)
# print(dv.get_feature_names())
# print(result)

# 2.OneHotEncoder
encoder = OneHotEncoder(sparse=False)
result = encoder.fit_transform(df[["gender"]])
print(result)

# 3.pd.get_dummies转换之后不需要在做合并
data = pd.get_dummies(data=df, columns=["gender"])
print(data)
```

- 中文文本

```python
import jieba
from sklearn.feature_extraction.text import CountVectorizer

data = "生活很短，我喜欢python, 生活太久了，我不喜欢python"

# 分词,返回值是generator
cut_ge = jieba.cut(data)
# 方法一：生成器转列表
# content = []
# for word in cut_ge:
#     content.append(word)
# data = [" ".join(content)]
# 方法二，join(可迭代)
data = " ".join(cut_ge)
cv = CountVectorizer()
result = cv.fit_transform(data)
print(cv.get_feature_names())
print(result)
print(result.toarray())
```

- TF-IDF

TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

```python
# 类
sklearn.feature_extraction.text.TfidfVectorizer
# 返回词的权重矩阵
TfidfVectorizer(stop_words=None,…)

TfidfVectorizer.fit_transform(X,y)       
X:文本或者包含文本字符串的可迭代对象
返回值：返回sparse矩阵

TfidfVectorizer.inverse_transform(X)
X:array数组或者sparse矩阵
返回值:转换之前数据格式

TfidfVectorizer.get_feature_names()
返回值:单词列表
```

实现

```python
import jieba 
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

def cut_words():
    s1 = "今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。"
    s2 = "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。"
    s3 = "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"   
    s1_ge = jieba.cut(s1)
    s2_ge = jieba.cut(s2)
    s3_ge = jieba.cut(s3)
    return " ".join(s1_ge), " ".join(s2_ge), " ".join(s3_ge)

words1, words2, words3 = cut_words()
# 使用TFIDF特征抽取
tfidf = TfidfVectorizer(stop_words=["一种", "每个"])
# 输入：文本或包含文本字符创的可迭代对象，返回值：saprse矩阵
result = tfidf.fit_transform([words1, words2, words3])
# 返回值：单词列表
print(tfidf.get_feature_names())
print(result.toarray())
# 输入：array数组或sparse矩阵，返回值：转换之前的数据格式
print(tfidf.inverse_transform(result))
```

## 特征预处理

### 归一化

- 最值归一化(Normalization)

将说有数据归一化到0～1的分布中
$$
x_{scale} =\frac{x-x_{min}}{x_{max}-x_{min}}
$$
适用于有明显边界的情况， 不适合有极端值的情况

```python
# processing.py
import numpy as np


class MinMaxScaler:

    def __init__(self):
        self.min_ = None
        self.max_ = None

    def fit(self, X):
        """根据训练数据集X获得数据的最大值最小值"""
        assert X.ndim == 2, "The dimension of X must be 2"

        self.min_ = np.array([np.min(X[:,i]) for i in range(X.shape[1])])
        self.max_ = np.array([np.max(X[:,i]) for i in range(X.shape[1])])

        return self

    def transform(self, X):
        """将X根据这个MinMaxScaler进行最值归一化处理"""
        assert X.ndim == 2, "The dimension of X must be 2"
        assert self.min_ is not None and self.min_ is not None, \
               "must fit before transform!"
        assert self.max_ is not None and self.max_ is not None, \
               "must fit before transform!"

        resX = np.empty(shape=X.shape, dtype=float)
        for col in range(X.shape[1]):
            resX[:,col] = (X[:,col] - self.min_[col]) / (self.max_[col]-self.min_[col])
        return resX
```

- 均值方差归一化(Standardization)

将所有数据归一化到均值为0，方差为1的分布中
$$
x_{scale} = \frac{x-x_{mean}}{S}
$$
适用与没有明显边界，有可能存在极端值

```python
# processing.py
import numpy as np


class StandardScaler:

    def __init__(self):
        self.mean_ = None
        self.scale_ = None

    def fit(self, X):
        """根据训练数据集X获得数据的均值和方差"""
        assert X.ndim == 2, "The dimension of X must be 2"

        self.mean_ = np.array([np.mean(X[:,i]) for i in range(X.shape[1])])
        self.scale_ = np.array([np.std(X[:,i]) for i in range(X.shape[1])])

        return self

    def transform(self, X):
        """将X根据这个StandardScaler进行均值方差归一化处理"""
        assert X.ndim == 2, "The dimension of X must be 2"
        assert self.mean_ is not None and self.scale_ is not None, \
               "must fit before transform!"
        assert X.shape[1] == len(self.mean_), \
               "the feature number of X must be equal to mean_ and std_"

        resX = np.empty(shape=X.shape, dtype=float)
        for col in range(X.shape[1]):
            resX[:,col] = (X[:,col] - self.mean_[col]) / self.scale_[col]
        return resX

```

### 缺失值

对缺失值进行补充，可使用均值、插值等方法

### 特征选择

主要方法：

```
Filter:VarianceThreshold

Embedded:正则化、决策树

Wrapper

```

函数

```
# 类
sklearn.feature_selection.VarianceThreshold
# 实例化
VarianceThreshold(threshold = 0.0)
删除所有低方差特征

Variance.fit_transform(X,y)       
X:numpy array格式的数据[n_samples,n_features]
返回值：训练集差异低于threshold的特征将被删除。
默认值是保留所有非零方差特征，即删除所有样本
中具有相同值的特征。
```

实现

```python
from sklearn.feature_selection import VarianceThreshold

data = [[0, 2, 0, 3],
        [0, 1, 4, 3],
        [0, 1, 1, 3]]
        
# 删除所有低方差特征，默认0.0
vt = VarianceThreshold()
# 输入值：numpy array格式数据
# 返回值：训练集差异低于threshold的特征将被删除
result = vt.fit_transform(data)
print(result)
print(result.shape)
```

## 特征降维

分类：

```
主成成分分析(principalcomponent analysis,PCA)

因子分析(Factor Analysis)

独立成分分析(Independent Component Analysis，ICA)
```

本质：PCA是一种分析、简化数据集的技术。在PCA中，数据从原来的坐标系转换到新的坐标系。

目的：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。

函数

```python
# 类
sklearn.decomposition.PCA
# 实例化
PCA(n_components=None)
将数据分解为较低维数空间

PCA.fit_transform(X)       
X:numpy array格式的数据[n_samples,n_features]
返回值：转换后指定维度的array
```

实现

```python
from sklearn.decomposition import PCA

data = [[2, 8, 4, 5],
        [6, 3, 0, 8],
        [5, 4, 9, 1]]

# 将数据分解为较低维数空间
pca = PCA(n_components=3)
# 输入：numpy array格式的数据，返回值：转换后制定维度的array
result = pca.fit_transform(data)
# pca降维之后，新的数据具体意义就丧失了，主要信息保留
print(result)
```

**特征选择/降维**

相同点：

特征选择和降维都是降低数据维度

不同点：

特征选择筛选掉的特征不会对模型的训练产生任何影响

降维做了数据的映射，保留主要成分，所有的特征对模型训练有影响

## 使用流程

流程

```
1.导入需要的库
2.导入数据集
3.处理丢失数据
4.解析分类数据
5.拆分数据集为训练集合和测试集合
6.特征量化
```



data

| Country | Age  | Salary | Purchased |
| ------- | ---- | ------ | --------- |
| France  | 44   | 72000  | No        |
| Spain   | 27   | 48000  | Yes       |
| Germany | 30   | 54000  | No        |
| Spain   | 38   | 61000  | No        |
| Germany | 40   |        | Yes       |
| France  | 35   | 58000  | Yes       |
| Spain   |      | 52000  | No        |
| France  | 48   | 79000  | Yes       |
| Germany | 50   | 83000  | No        |
| France  | 37   | 67000  | Yes       |

code

```python
#Step 1: Importing the libraries
import numpy as np
import pandas as pd

#Step 2: Importing dataset
dataset = pd.read_csv('../datasets/Data.csv')
X = dataset.iloc[ : , :-1].values
Y = dataset.iloc[ : , 3].values
print("Step 2: Importing dataset")
print("X")
print(X)
print("Y")
print(Y)

#Step 3: Handling the missing data
from sklearn.preprocessing import Imputer
imputer = Imputer(missing_values = "NaN", strategy = "mean", axis = 0)
imputer = imputer.fit(X[ : , 1:3])
X[ : , 1:3] = imputer.transform(X[ : , 1:3])
print("---------------------")
print("Step 3: Handling the missing data")
print("step2")
print("X")
print(X)

#Step 4: Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X = LabelEncoder()
X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])
#Creating a dummy variable
onehotencoder = OneHotEncoder(categorical_features = [0])
X = onehotencoder.fit_transform(X).toarray()
labelencoder_Y = LabelEncoder()
Y =  labelencoder_Y.fit_transform(Y)
print("---------------------")
print("Step 4: Encoding categorical data")
print("X")
print(X)
print("Y")
print(Y)

#Step 5: Splitting the datasets into training sets and Test sets
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)
print("---------------------")
print("Step 5: Splitting the datasets into training sets and Test sets")
print("X_train")
print(X_train)
print("X_test")
print(X_test)
print("Y_train")
print(Y_train)
print("Y_test")
print(Y_test)

#Step 6: Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
print("---------------------")
print("Step 6: Feature Scaling")
print("X_train")
print(X_train)
print("X_test")
print(X_test)
```

