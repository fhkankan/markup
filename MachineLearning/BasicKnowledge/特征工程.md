# 特征工程

数据预处理涉及的策略和技术非常广泛，主要技术是属性选择技术和主成分分析技术。

属性选择

```
指从数据集中选择最具代表性的属性子集，删除冗余或不相关的属性，从而提高数据处理的效率，使模型更容易理解
```

主成分分析

```
利用降维的思想，把给定的一组相关属性通过线性变换转换成另一组不相关的属性，这些新属性按照方差依次递减的顺序进行排列
主成分分析将很多个复杂属性归结为少数几个主成分，将复杂问题简单化，便于分析和处理
```

离散化

```
将连续的数值型数据切分为若干个称为分箱(bin)的小段，是数据分析中常用的手段
离散化的实质是将无限空间中有限的个体映射到有限的空间中去，其作用是提高算法的时空效率，有时也为了能够使用特定的机器学习算法而将数据离散化
```

## 样本不均衡

对于样本不均衡问题，可采取以下方法处理

```
1. 增加或减少样本
2. 调整权重惩罚
3. 使用其他舍弃的特征产生更好的特征
4. 尝试多种模型算法,如随机森林、逻辑回归之外的模型
5. 使用集成学习，采用多个模型来提高准确预测
6. 调整模型的超参数
```

### 数据采样

对于样本不均衡状况，可以采用下采样或过采样进行处理

- 下采样

按照样本较少的类别，将样本较多的类别中选取同样的样本数

缺点：由于样本过少，易发生过拟合

- 过采样

对于样本较少的类别，采用一定的算法，自动生成与样本较多类别个数的样本数据数据

SMOTE算法

```
1. 对于少数类中的每一个样本x，以欧式距离为标准计算它到少数类样本集中所有样本的距离，得到其k近邻
2. 根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本x，从其k近邻中随机选择若干个样本，假设选择的近邻为x(n)
3. 对于每一个随机选出的近邻x(n)，分别与原样本按照如下公式构建新的样本
```

$$
x_{new} = x + rand(0, 1)*(\bar{x}-x)
$$

使用

```python
import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

credit_cards = pd.read_csv('creditcard.csv')
columns = credit_cards.columns
features_columns = columns.delete(len(columns) - 1)
features = credit_cards[features_columns]
labels = credit_cards['Class']
features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2, random_state=0)

oversample = SMOTE(random_state=0)
os_features, os_labels = oversample.fit_sample(features_train, lables_train)

print(len(os_labels[os_labels==1]))
os_features = pd.DataFrame(os_features)
os_labels = pd.DataFrame(os_labels)
```

### 权重调整

通过对较多数据样本的权重降低，较少的样本的权重提高，来平衡样本不均衡

```python
from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import cross_val_predict, KFold

# 模型训练
lr = LogisticRegression()
cols = loans.columns
train_cols = cols.drop("loan_status")
features = loans[train_cols]
target = loans["loan_status"]
lr.fit(features, target)
predictions = lr.predict(features)

# 未调整权重
# lr = LogisticRegression()
# 调整权重
# 采用默认调整权重
lr = LogisticRegression(clas_weight="balanced")
# 自定义调整权重
lr = LogisticRegression(clas_weight={0:10, 1:1})
kf = KFold(features.shape[0], random_state=1)
predictions = cross_val_predict(lr, features, target, cv=kf)
predictions = pd.Series(predictions)

# 评价指标
fp_filter = (predictions == 1) & (loans["loan_status"] == 0)
fp = len(predictions[fp_filter])
tp_filter = (predictions == 1) & (loans["loan_status"] == 1)
tp = len(predictions[tp_filter])
fn_filter = (predictions == 0) & (loans["loan_status"] == 1)
fn = len(predictions[fn_filter])
tn_filter = (predictions == 0) & (loans["loan_status"] == 0)
tn = len(predictions[tn_filter])

fpr = fp / float((fp + tn))
tpr = tp / float((tp + fn))
print(fpr, tpr)
```

## 特征抽取

### 分类特征

- DictVectorizer

```python
# 字典向量化
from sklearn.feature_extraction import DictVectorizer

# 实例化
vec = DictVectorizer(sparse=True,…)

# 方法
vec.fit_transform(X)       
# 参数X:字典或者包含字典的迭代器
# 返回值：返回sparse矩阵

# 等价于vec.fit_transform(X)
vec.fit(X)
vec.transform(X)

vec.inverse_transform(X)
# 参数X:array数组或者sparse矩阵
# 返回值:转换之前数据格式

vec.get_feature_names()  # 返回类别名称
```

实现

```python
from sklearn.feature_extraction import DictVectorizer

data = [{'city': '北京', 'temperature': 100},
        {'city': '上海', 'temperature': 60},
        {'city': '深圳', 'temperature': 30}]

# 字典数据特征抽取，默认稀疏矩阵
vec = DictVectorizer(sparse=False)
# 输入是字典或者包含字典的迭代器，返回值是sparse矩阵
result = vec.fit_transform(data)
# 输入是array数组后者sparse矩阵，返回值是转换之前数据格式
origin = vec.inverse_transform(result)

print(vec.get_feature_names())  # 返回了表的名称
# ['city=上海', 'city=北京', 'city=深圳', 'temperature']
print(result)
# [[  0.   1.   0. 100.]
#  [  1.   0.   0.  60.]
#  [  0.   0.   1.  30.]]
print(origin)
# [{'city=北京': 1.0, 'temperature': 100.0},
#  {'city=上海': 1.0, 'temperature': 60.0}, 
#  {'city=深圳': 1.0, 'temperature': 30.0}]

vec = DictVectorizer(sparse=True)  # 采用稀疏矩阵
res = vec.fit_transform(data)
print(res)
#   (0, 1)	1.0
#   (0, 3)	100.0
#   (1, 0)	1.0
#   (1, 3)	60.0
#   (2, 2)	1.0
#   (2, 3)	30.0
```

- one-hot

```
3种方式：
1. DictVectorizer(注意，数字不会进行转换)
2. OneHotEncoder(Numpy)
3. pd.get_dummies(Pandas)
```

实现

```python
import pandas as pd
from sklearn.feature_extraction import DictVectorizer
from sklearn.preprocessing import OneHotEncoder


df = pd.read_csv("./one_hot_test.csv")
# print(df)

# 1.DictVecotrizer
# df.gender = df.gender.map({0: "female", 1: "male", })
# print(type(df))
# print(df["gender"])  # Series对象
# print(df[["gender"]]) # DataFrame对象
# print(list(df[["gender"]].T.to_dict().values()))
# data = list(df[["gender"]].T.to_dict().values())
# dv = DictVectorizer(sparse=False)
# result = dv.fit_transform(data)
# print(dv.get_feature_names())
# print(result)

# 2.OneHotEncoder
encoder = OneHotEncoder(sparse=False)
result = encoder.fit_transform(df[["gender"]])
print(result)

# 3.pd.get_dummies转换之后不需要在做合并
data = pd.get_dummies(data=df, columns=["gender"])
print(data)
```

### 文本特征

- 单词统计

```python
from sklearn.feature_extraction.text import CountVectorizer

# 实例化
vec = CountVectorizer()

vec.fit_transform(X)       
# 参数X:文本或者包含文本字符串的可迭代对象
# 返回：词频矩阵
# fit_transform = fit + transform

vev.get_feature_names()
# 返回值:单词列表
```

实现

```python
from sklearn.feature_extraction.text import CountVectorizer

data = ["life is short, i like python", "lisfe is too long, i disliake python"]

# 特征抽取，抽取词频矩阵
vec = CountVectorizer()
# fit提取特征名
name = vec.fit(data)
# transform根据提取出来的特征词，统计个数
result = vec.transform(data)
# data是文本或包含文本字符串的可迭代对象，返回词频矩阵
# result = vec.fit_transform(data)  # fit_transform = fit + transform
print(vec.get_feature_names())  # 返回单词列表
# ['disliake', 'is', 'life', 'like', 'lisfe', 'long', 'python', 'short', 'too']
print(result)  # 稀疏矩阵
#  (0, 1)	1
#   (0, 2)	1
#   (0, 3)	1
#   (0, 6)	1
#   (0, 7)	1
#   (1, 0)	1
#   (1, 1)	1
#   (1, 4)	1
#   (1, 5)	1
#   (1, 6)	1
#   (1, 8)	1

print(result.toarray())  # sparse矩阵转换为array数组
# [[0 1 1 1 0 0 1 1 0]
#  [1 1 0 0 1 1 1 0 1]]

```

中文文本

```python
import jieba
from sklearn.feature_extraction.text import CountVectorizer

data = "生活很短，我喜欢python, 生活太久了，我不喜欢python"

# 分词,返回值是generator
cut_ge = jieba.cut(data)
# 方法一：生成器转列表
# content = []
# for word in cut_ge:
#     content.append(word)
# data = [" ".join(content)]
# 方法二，join(可迭代)
data = " ".join(cut_ge)
cv = CountVectorizer()
result = cv.fit_transform(data)
print(cv.get_feature_names())
print(result)
print(result.toarray())
```

- TF-IDF

原始的单词统计会让一些常用词聚集过高的权重，不利于分类算法。

TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

```python
from sklearn.feature_extraction.text import TfidfVectorizer
# 返回词的权重矩阵

vec = TfidfVectorizer(stop_words=None,…)

vec.fit_transform(X,y)       
# 参数X:文本或者包含文本字符串的可迭代对象
# 返回值：返回sparse矩阵

vec.inverse_transform(X)
# 参数X:array数组或者sparse矩阵
# 返回值:转换之前数据格式

vec.get_feature_names()
# 返回值:单词列表
```

实现

```python
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

data = ["life is short, i like python", "lisfe is too long, i disliake python"]

vec = TfidfVectorizer()
X = vec.fit_transform(data)
print(X)
#   (0, 6)	0.35520008546852583
#   (0, 3)	0.4992213265230509
#   (0, 7)	0.4992213265230509
#   (0, 1)	0.35520008546852583
#   (0, 2)	0.4992213265230509
#   (1, 0)	0.4466561618018052
#   (1, 5)	0.4466561618018052
#   (1, 8)	0.4466561618018052
#   (1, 4)	0.4466561618018052
#   (1, 6)	0.31779953783628945
#   (1, 1)	0.31779953783628945
print(X.toarray())
# [[0.         0.35520009 0.49922133 0.49922133 0.         0.         0.35520009 0.49922133 0.]
#  [0.44665616 0.31779954 0.         0.         0.44665616 0.44665616 0.31779954 0.         0.44665616]]
res = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())
print(res)
#    disliake      is      life      like  ...      long  python     short       too
# 0  0.000000  0.3552  0.499221  0.499221  ...  0.000000  0.3552  0.499221  0.000000
# 1  0.446656  0.3178  0.000000  0.000000  ...  0.446656  0.3178  0.000000  0.446656

```

中文

```python
import jieba 
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

def cut_words():
    s1 = "今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。"
    s2 = "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。"
    s3 = "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"   
    s1_ge = jieba.cut(s1)
    s2_ge = jieba.cut(s2)
    s3_ge = jieba.cut(s3)
    return " ".join(s1_ge), " ".join(s2_ge), " ".join(s3_ge)

words1, words2, words3 = cut_words()
# 使用TFIDF特征抽取
tfidf = TfidfVectorizer(stop_words=["一种", "每个"])
# 输入：文本或包含文本字符创的可迭代对象，返回值：saprse矩阵
result = tfidf.fit_transform([words1, words2, words3])
# 返回值：单词列表
print(tfidf.get_feature_names())
print(result.toarray())
# 输入：array数组或sparse矩阵，返回值：转换之前的数据格式
print(tfidf.inverse_transform(result))
```

### 图像特征

方法一：使用像素

## 特征预处理

### 归一化

- 最值归一化(Normalization)

将说有数据归一化到0～1的分布中
$$
x_{scale} =\frac{x-x_{min}}{x_{max}-x_{min}}
$$
适用于有明显边界的情况， 不适合有极端值的情况

```python
# processing.py
import numpy as np


class MinMaxScaler:

    def __init__(self):
        self.min_ = None
        self.max_ = None

    def fit(self, X):
        """根据训练数据集X获得数据的最大值最小值"""
        assert X.ndim == 2, "The dimension of X must be 2"

        self.min_ = np.array([np.min(X[:,i]) for i in range(X.shape[1])])
        self.max_ = np.array([np.max(X[:,i]) for i in range(X.shape[1])])

        return self

    def transform(self, X):
        """将X根据这个MinMaxScaler进行最值归一化处理"""
        assert X.ndim == 2, "The dimension of X must be 2"
        assert self.min_ is not None and self.min_ is not None, \
               "must fit before transform!"
        assert self.max_ is not None and self.max_ is not None, \
               "must fit before transform!"

        resX = np.empty(shape=X.shape, dtype=float)
        for col in range(X.shape[1]):
            resX[:,col] = (X[:,col] - self.min_[col]) / (self.max_[col]-self.min_[col])
        return resX
```

sklearn实现

```python
from sklearn.preprocessing import MinMaxScaler

# numpy array格式的数据[n_samples,n_features]
data = ...
# 实例化,每个特征缩放到给定范围(默认[0,1])
mm = MinMaxScaler(feature_range=(0,1)…)
# 归一化，返回转换后的形状相同的array
result = mm.fit_transform(data)  
# 原始数据中每列特征的最小最大值
print(mm.data_min_)
print(mm.data_max_)
```

- 均值方差归一化(Standardization)

将所有数据归一化到均值为0，方差为1的分布中
$$
x_{scale} = \frac{x-x_{mean}}{S}
$$
适用与没有明显边界，有可能存在极端值

```python
# processing.py
import numpy as np


class StandardScaler:

    def __init__(self):
        self.mean_ = None
        self.scale_ = None

    def fit(self, X):
        """根据训练数据集X获得数据的均值和方差"""
        assert X.ndim == 2, "The dimension of X must be 2"

        self.mean_ = np.array([np.mean(X[:,i]) for i in range(X.shape[1])])
        self.scale_ = np.array([np.std(X[:,i]) for i in range(X.shape[1])])

        return self

    def transform(self, X):
        """将X根据这个StandardScaler进行均值方差归一化处理"""
        assert X.ndim == 2, "The dimension of X must be 2"
        assert self.mean_ is not None and self.scale_ is not None, \
               "must fit before transform!"
        assert X.shape[1] == len(self.mean_), \
               "the feature number of X must be equal to mean_ and std_"

        resX = np.empty(shape=X.shape, dtype=float)
        for col in range(X.shape[1]):
            resX[:,col] = (X[:,col] - self.mean_[col]) / self.scale_[col]
        return resX

```

sklearn实现

```python
from sklearn.preprocessing import StandardScaler

data = ...
# 处理之后每列的数据都聚集在均值0，方差1附近
ss = StandardScaler()
# 输入：ndarray，输出:转换后形状相同的array
result = ss.fit_transform(data)
# 原始数据中每列特征的平均值
print(ss.mean_)
```

### 缺失值

对缺失值进行补充，可使用均值、插值等方法

### 特征选择

主要方法：

```
Filter:VarianceThreshold

Embedded:正则化、决策树

Wrapper
```

函数

```python
# 类
sklearn.feature_selection.VarianceThreshold
# 实例化
VarianceThreshold(threshold = 0.0)
删除所有低方差特征

Variance.fit_transform(X,y)       
X:numpy array格式的数据[n_samples,n_features]
返回值：训练集差异低于threshold的特征将被删除。
默认值是保留所有非零方差特征，即删除所有样本
中具有相同值的特征。
```

实现

```python
from sklearn.feature_selection import VarianceThreshold

data = [[0, 2, 0, 3],
        [0, 1, 4, 3],
        [0, 1, 1, 3]]
        
# 删除所有低方差特征，默认0.0
vt = VarianceThreshold()
# 输入值：numpy array格式数据
# 返回值：训练集差异低于threshold的特征将被删除
result = vt.fit_transform(data)
print(result)
print(result.shape)
```

## 特征重要性

将某个特征对应的样本值加入干扰值之后获取error，与原本特征的样本获取的error，进行对比，若相差较大，则说明此特征比较重要

```python
import numpy as np
from sklearn.feature_selection import SelectKBest, f_classif
import matplotlib.pyplot as plt

predictors = ["Pclass", "Sex", "Age", "Fare"]

# 特征选择
selector = SelectKBest(f_classif, k=5)
selector.fit(titanic[predictors], titanic["Survived"])
# 获取每个特征对应的p-value，然后将其转换为score
scores = -np.log10(selector.pvalues_)

# 画图
plt.bar(range(len(predictors)), scores)
plt.xticks(range(len(predictors)), predictos, totation="vertical")
plt.show()
```

## 特征降维

分类：

```
主成成分分析(principalcomponent analysis,PCA)

因子分析(Factor Analysis)

独立成分分析(Independent Component Analysis，ICA)
```

本质：PCA是一种分析、简化数据集的技术。在PCA中，数据从原来的坐标系转换到新的坐标系。

目的：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。

函数

```python
# 类
sklearn.decomposition.PCA
# 实例化
PCA(n_components=None)
将数据分解为较低维数空间

PCA.fit_transform(X)       
X:numpy array格式的数据[n_samples,n_features]
返回值：转换后指定维度的array
```

实现

```python
from sklearn.decomposition import PCA

data = [[2, 8, 4, 5],
        [6, 3, 0, 8],
        [5, 4, 9, 1]]

# 将数据分解为较低维数空间
pca = PCA(n_components=3)
# 输入：numpy array格式的数据，返回值：转换后制定维度的array
result = pca.fit_transform(data)
# pca降维之后，新的数据具体意义就丧失了，主要信息保留
print(result)
```

**特征选择/降维**

相同点：

特征选择和降维都是降低数据维度

不同点：

特征选择筛选掉的特征不会对模型的训练产生任何影响

降维做了数据的映射，保留主要成分，所有的特征对模型训练有影响

## 使用流程

流程

```
1.导入需要的库
2.导入数据集
3.处理丢失数据
4.解析分类数据
5.拆分数据集为训练集合和测试集合
6.特征量化
```

data

| Country | Age  | Salary | Purchased |
| ------- | ---- | ------ | --------- |
| France  | 44   | 72000  | No        |
| Spain   | 27   | 48000  | Yes       |
| Germany | 30   | 54000  | No        |
| Spain   | 38   | 61000  | No        |
| Germany | 40   |        | Yes       |
| France  | 35   | 58000  | Yes       |
| Spain   |      | 52000  | No        |
| France  | 48   | 79000  | Yes       |
| Germany | 50   | 83000  | No        |
| France  | 37   | 67000  | Yes       |

code

```python
#Step 1: Importing the libraries
import numpy as np
import pandas as pd

#Step 2: Importing dataset
dataset = pd.read_csv('../datasets/Data.csv')
X = dataset.iloc[ : , :-1].values
Y = dataset.iloc[ : , 3].values
print("Step 2: Importing dataset")
print("X")
print(X)
print("Y")
print(Y)

#Step 3: Handling the missing data
from sklearn.preprocessing import Imputer
imputer = Imputer(missing_values = "NaN", strategy = "mean", axis = 0)
imputer = imputer.fit(X[ : , 1:3])
X[ : , 1:3] = imputer.transform(X[ : , 1:3])
print("---------------------")
print("Step 3: Handling the missing data")
print("step2")
print("X")
print(X)

#Step 4: Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X = LabelEncoder()
X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])
#Creating a dummy variable
onehotencoder = OneHotEncoder(categorical_features = [0])
X = onehotencoder.fit_transform(X).toarray()
labelencoder_Y = LabelEncoder()
Y =  labelencoder_Y.fit_transform(Y)
print("---------------------")
print("Step 4: Encoding categorical data")
print("X")
print(X)
print("Y")
print(Y)

#Step 5: Splitting the datasets into training sets and Test sets
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)
print("---------------------")
print("Step 5: Splitting the datasets into training sets and Test sets")
print("X_train")
print(X_train)
print("X_test")
print(X_test)
print("Y_train")
print(Y_train)
print("Y_test")
print(Y_test)

#Step 6: Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
print("---------------------")
print("Step 6: Feature Scaling")
print("X_train")
print(X_train)
print("X_test")
print(X_test)
```

